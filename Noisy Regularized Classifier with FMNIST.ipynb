{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noisy Regularized Classifier with FMNIST\n",
    "\n",
    "------------------------------------------------------\n",
    "*Basic example of the behaviour of stochastic regularization layers in FMNIST classification*\n",
    "\n",
    "\n",
    "------------------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember in our methodology, there are 4 main stages:\n",
    "    1. Pre-training of the base architecture.\n",
    "    2. Training GMVAE with some hidden vector within the base model.\n",
    "    3. Integration of GMVAE in the base architecture.\n",
    "    4. Finetuning layers and parameters above the place where the stochastic layer is placed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pre-training of the base architecture of the classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we present the pre-training of the FMNIST as the initial step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we build two classes for the definition of the classification neural network:\n",
    " - Classifier: The raw version of the NN that is the original architecture without the regularizer. It is needed to do the pretraining and is described in the following cell. It is composed of a function, *obtain_input_to_regularize_lower*, in order to get the hidden vectors considered the input to the GMVAE. They are the vectors after the first layer, fc1.\n",
    " - NoRClassifier: This version is defined later in the notebook and it includes the GMVAE within the structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 700)\n",
    "        self.fc2 = nn.Linear(700, 600)\n",
    "        self.fc3 = nn.Linear(600, 512)\n",
    "        self.fc4 = nn.Linear(512, 256)\n",
    "        self.fc5 = nn.Linear(256, 128)\n",
    "        self.fc6 = nn.Linear(128, 64)\n",
    "        self.fc7 = nn.Linear(64, 32)\n",
    "        self.fc8 = nn.Linear(32, 16)\n",
    "        self.fc9 = nn.Linear(16, 10)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = F.relu(self.fc6(x))\n",
    "        x = F.relu(self.fc7(x))\n",
    "        x = self.dropout(F.relu(self.fc8(x)))\n",
    "        x = F.log_softmax(self.fc9(x), dim=1)\n",
    "        return x\n",
    "\n",
    "    def obtain_input_to_regularize_lower(self, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return x.detach()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = 0\n",
    "epochs = 200\n",
    "lr = 0.01\n",
    "load_path = \"200epochs_dp0_lr001\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download dataset and transform images:\n",
    "\n",
    "We normalize the images and build training, validation and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, ), (0.5,),)])\n",
    "\n",
    "trainset = datasets.FashionMNIST('~/.python/F_MNIST_data', download=True, train=True, transform=transform)\n",
    "testset = datasets.FashionMNIST('~/.python/F_MNIST_data', download=True, train=False, transform=transform)\n",
    "\n",
    "indices = list(range(len(trainset)))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(0.2 * len(trainset)))\n",
    "train_sample = SubsetRandomSampler(indices[:split])\n",
    "valid_sample = SubsetRandomSampler(indices[split:])\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, sampler=train_sample, batch_size=64)\n",
    "validloader = torch.utils.data.DataLoader(trainset, sampler=valid_sample, batch_size=64)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the Classifier model and define the criterion and optimizer used in the training step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Classifier()\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 2.314347 \tValidation Loss: 2.310117\n",
      "validation loss decreased(inf -->2.310117). Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 2.309818 \tValidation Loss: 2.307090\n",
      "validation loss decreased(2.310117 -->2.307090). Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 2.307009 \tValidation Loss: 2.305363\n",
      "validation loss decreased(2.307090 -->2.305363). Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 2.305267 \tValidation Loss: 2.304313\n",
      "validation loss decreased(2.305363 -->2.304313). Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 2.304359 \tValidation Loss: 2.303791\n",
      "validation loss decreased(2.304313 -->2.303791). Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 2.303360 \tValidation Loss: 2.303380\n",
      "validation loss decreased(2.303791 -->2.303380). Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 2.302857 \tValidation Loss: 2.303191\n",
      "validation loss decreased(2.303380 -->2.303191). Saving model ...\n",
      "Epoch: 8 \tTraining Loss: 2.302865 \tValidation Loss: 2.303206\n",
      "Epoch: 9 \tTraining Loss: 2.302440 \tValidation Loss: 2.303017\n",
      "validation loss decreased(2.303191 -->2.303017). Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 2.302087 \tValidation Loss: 2.302980\n",
      "validation loss decreased(2.303017 -->2.302980). Saving model ...\n",
      "Epoch: 11 \tTraining Loss: 2.302119 \tValidation Loss: 2.302957\n",
      "validation loss decreased(2.302980 -->2.302957). Saving model ...\n",
      "Epoch: 12 \tTraining Loss: 2.302267 \tValidation Loss: 2.302759\n",
      "validation loss decreased(2.302957 -->2.302759). Saving model ...\n",
      "Epoch: 13 \tTraining Loss: 2.301989 \tValidation Loss: 2.302784\n",
      "Epoch: 14 \tTraining Loss: 2.302108 \tValidation Loss: 2.302992\n",
      "Epoch: 15 \tTraining Loss: 2.302251 \tValidation Loss: 2.302797\n",
      "Epoch: 16 \tTraining Loss: 2.301729 \tValidation Loss: 2.302905\n",
      "Epoch: 17 \tTraining Loss: 2.301882 \tValidation Loss: 2.302752\n",
      "validation loss decreased(2.302759 -->2.302752). Saving model ...\n",
      "Epoch: 18 \tTraining Loss: 2.302013 \tValidation Loss: 2.302628\n",
      "validation loss decreased(2.302752 -->2.302628). Saving model ...\n",
      "Epoch: 19 \tTraining Loss: 2.301567 \tValidation Loss: 2.302558\n",
      "validation loss decreased(2.302628 -->2.302558). Saving model ...\n",
      "Epoch: 20 \tTraining Loss: 2.301434 \tValidation Loss: 2.302475\n",
      "validation loss decreased(2.302558 -->2.302475). Saving model ...\n",
      "Epoch: 21 \tTraining Loss: 2.301092 \tValidation Loss: 2.302297\n",
      "validation loss decreased(2.302475 -->2.302297). Saving model ...\n",
      "Epoch: 22 \tTraining Loss: 2.301330 \tValidation Loss: 2.302207\n",
      "validation loss decreased(2.302297 -->2.302207). Saving model ...\n",
      "Epoch: 23 \tTraining Loss: 2.301420 \tValidation Loss: 2.301906\n",
      "validation loss decreased(2.302207 -->2.301906). Saving model ...\n",
      "Epoch: 24 \tTraining Loss: 2.300852 \tValidation Loss: 2.301834\n",
      "validation loss decreased(2.301906 -->2.301834). Saving model ...\n",
      "Epoch: 25 \tTraining Loss: 2.300948 \tValidation Loss: 2.301719\n",
      "validation loss decreased(2.301834 -->2.301719). Saving model ...\n",
      "Epoch: 26 \tTraining Loss: 2.300886 \tValidation Loss: 2.301416\n",
      "validation loss decreased(2.301719 -->2.301416). Saving model ...\n",
      "Epoch: 27 \tTraining Loss: 2.300413 \tValidation Loss: 2.301233\n",
      "validation loss decreased(2.301416 -->2.301233). Saving model ...\n",
      "Epoch: 28 \tTraining Loss: 2.300095 \tValidation Loss: 2.300898\n",
      "validation loss decreased(2.301233 -->2.300898). Saving model ...\n",
      "Epoch: 29 \tTraining Loss: 2.299386 \tValidation Loss: 2.300365\n",
      "validation loss decreased(2.300898 -->2.300365). Saving model ...\n",
      "Epoch: 30 \tTraining Loss: 2.298933 \tValidation Loss: 2.299415\n",
      "validation loss decreased(2.300365 -->2.299415). Saving model ...\n",
      "Epoch: 31 \tTraining Loss: 2.297978 \tValidation Loss: 2.298441\n",
      "validation loss decreased(2.299415 -->2.298441). Saving model ...\n",
      "Epoch: 32 \tTraining Loss: 2.296639 \tValidation Loss: 2.296835\n",
      "validation loss decreased(2.298441 -->2.296835). Saving model ...\n",
      "Epoch: 33 \tTraining Loss: 2.294642 \tValidation Loss: 2.293996\n",
      "validation loss decreased(2.296835 -->2.293996). Saving model ...\n",
      "Epoch: 34 \tTraining Loss: 2.290295 \tValidation Loss: 2.287347\n",
      "validation loss decreased(2.293996 -->2.287347). Saving model ...\n",
      "Epoch: 35 \tTraining Loss: 2.279573 \tValidation Loss: 2.272559\n",
      "validation loss decreased(2.287347 -->2.272559). Saving model ...\n",
      "Epoch: 36 \tTraining Loss: 2.256698 \tValidation Loss: 2.237861\n",
      "validation loss decreased(2.272559 -->2.237861). Saving model ...\n",
      "Epoch: 37 \tTraining Loss: 2.184307 \tValidation Loss: 2.106079\n",
      "validation loss decreased(2.237861 -->2.106079). Saving model ...\n",
      "Epoch: 38 \tTraining Loss: 1.993693 \tValidation Loss: 1.925894\n",
      "validation loss decreased(2.106079 -->1.925894). Saving model ...\n",
      "Epoch: 39 \tTraining Loss: 1.872893 \tValidation Loss: 1.842547\n",
      "validation loss decreased(1.925894 -->1.842547). Saving model ...\n",
      "Epoch: 40 \tTraining Loss: 1.792191 \tValidation Loss: 1.769297\n",
      "validation loss decreased(1.842547 -->1.769297). Saving model ...\n",
      "Epoch: 41 \tTraining Loss: 1.737764 \tValidation Loss: 1.728911\n",
      "validation loss decreased(1.769297 -->1.728911). Saving model ...\n",
      "Epoch: 42 \tTraining Loss: 1.702781 \tValidation Loss: 1.692252\n",
      "validation loss decreased(1.728911 -->1.692252). Saving model ...\n",
      "Epoch: 43 \tTraining Loss: 1.653926 \tValidation Loss: 1.621481\n",
      "validation loss decreased(1.692252 -->1.621481). Saving model ...\n",
      "Epoch: 44 \tTraining Loss: 1.568897 \tValidation Loss: 1.505101\n",
      "validation loss decreased(1.621481 -->1.505101). Saving model ...\n",
      "Epoch: 45 \tTraining Loss: 1.472157 \tValidation Loss: 1.463477\n",
      "validation loss decreased(1.505101 -->1.463477). Saving model ...\n",
      "Epoch: 46 \tTraining Loss: 1.416152 \tValidation Loss: 1.709395\n",
      "Epoch: 47 \tTraining Loss: 1.378445 \tValidation Loss: 1.352202\n",
      "validation loss decreased(1.463477 -->1.352202). Saving model ...\n",
      "Epoch: 48 \tTraining Loss: 1.316810 \tValidation Loss: 1.271229\n",
      "validation loss decreased(1.352202 -->1.271229). Saving model ...\n",
      "Epoch: 49 \tTraining Loss: 1.285935 \tValidation Loss: 1.475737\n",
      "Epoch: 50 \tTraining Loss: 1.240233 \tValidation Loss: 1.207685\n",
      "validation loss decreased(1.271229 -->1.207685). Saving model ...\n",
      "Epoch: 51 \tTraining Loss: 1.208967 \tValidation Loss: 1.212319\n",
      "Epoch: 52 \tTraining Loss: 1.194681 \tValidation Loss: 1.163745\n",
      "validation loss decreased(1.207685 -->1.163745). Saving model ...\n",
      "Epoch: 53 \tTraining Loss: 1.154447 \tValidation Loss: 1.143130\n",
      "validation loss decreased(1.163745 -->1.143130). Saving model ...\n",
      "Epoch: 54 \tTraining Loss: 1.132024 \tValidation Loss: 1.196236\n",
      "Epoch: 55 \tTraining Loss: 1.100899 \tValidation Loss: 1.180642\n",
      "Epoch: 56 \tTraining Loss: 1.077294 \tValidation Loss: 1.119029\n",
      "validation loss decreased(1.143130 -->1.119029). Saving model ...\n",
      "Epoch: 57 \tTraining Loss: 1.052012 \tValidation Loss: 1.092435\n",
      "validation loss decreased(1.119029 -->1.092435). Saving model ...\n",
      "Epoch: 58 \tTraining Loss: 1.040157 \tValidation Loss: 1.072362\n",
      "validation loss decreased(1.092435 -->1.072362). Saving model ...\n",
      "Epoch: 59 \tTraining Loss: 1.012363 \tValidation Loss: 1.056337\n",
      "validation loss decreased(1.072362 -->1.056337). Saving model ...\n",
      "Epoch: 60 \tTraining Loss: 0.983166 \tValidation Loss: 1.042423\n",
      "validation loss decreased(1.056337 -->1.042423). Saving model ...\n",
      "Epoch: 61 \tTraining Loss: 0.981473 \tValidation Loss: 1.048100\n",
      "Epoch: 62 \tTraining Loss: 0.938479 \tValidation Loss: 1.084385\n",
      "Epoch: 63 \tTraining Loss: 0.921873 \tValidation Loss: 1.011861\n",
      "validation loss decreased(1.042423 -->1.011861). Saving model ...\n",
      "Epoch: 64 \tTraining Loss: 0.896386 \tValidation Loss: 0.897118\n",
      "validation loss decreased(1.011861 -->0.897118). Saving model ...\n",
      "Epoch: 65 \tTraining Loss: 0.866392 \tValidation Loss: 1.090335\n",
      "Epoch: 66 \tTraining Loss: 0.842276 \tValidation Loss: 1.073182\n",
      "Epoch: 67 \tTraining Loss: 0.815717 \tValidation Loss: 0.845281\n",
      "validation loss decreased(0.897118 -->0.845281). Saving model ...\n",
      "Epoch: 68 \tTraining Loss: 0.810570 \tValidation Loss: 0.860072\n",
      "Epoch: 69 \tTraining Loss: 0.770768 \tValidation Loss: 0.824005\n",
      "validation loss decreased(0.845281 -->0.824005). Saving model ...\n",
      "Epoch: 70 \tTraining Loss: 0.767898 \tValidation Loss: 0.832561\n",
      "Epoch: 71 \tTraining Loss: 0.753190 \tValidation Loss: 0.920599\n",
      "Epoch: 72 \tTraining Loss: 0.724255 \tValidation Loss: 0.799164\n",
      "validation loss decreased(0.824005 -->0.799164). Saving model ...\n",
      "Epoch: 73 \tTraining Loss: 0.698547 \tValidation Loss: 1.122756\n",
      "Epoch: 74 \tTraining Loss: 0.703830 \tValidation Loss: 0.804905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 75 \tTraining Loss: 0.698983 \tValidation Loss: 0.847473\n",
      "Epoch: 76 \tTraining Loss: 0.696908 \tValidation Loss: 0.756770\n",
      "validation loss decreased(0.799164 -->0.756770). Saving model ...\n",
      "Epoch: 77 \tTraining Loss: 0.662660 \tValidation Loss: 0.750899\n",
      "validation loss decreased(0.756770 -->0.750899). Saving model ...\n",
      "Epoch: 78 \tTraining Loss: 0.671168 \tValidation Loss: 0.810436\n",
      "Epoch: 79 \tTraining Loss: 0.620970 \tValidation Loss: 0.799917\n",
      "Epoch: 80 \tTraining Loss: 0.613795 \tValidation Loss: 0.897918\n",
      "Epoch: 81 \tTraining Loss: 0.608221 \tValidation Loss: 0.840996\n",
      "Epoch: 82 \tTraining Loss: 0.730733 \tValidation Loss: 0.744379\n",
      "validation loss decreased(0.750899 -->0.744379). Saving model ...\n",
      "Epoch: 83 \tTraining Loss: 0.609216 \tValidation Loss: 0.760167\n",
      "Epoch: 84 \tTraining Loss: 0.579142 \tValidation Loss: 0.781639\n",
      "Epoch: 85 \tTraining Loss: 0.679981 \tValidation Loss: 0.742264\n",
      "validation loss decreased(0.744379 -->0.742264). Saving model ...\n",
      "Epoch: 86 \tTraining Loss: 0.573556 \tValidation Loss: 0.830123\n",
      "Epoch: 87 \tTraining Loss: 0.559807 \tValidation Loss: 0.863379\n",
      "Epoch: 88 \tTraining Loss: 0.542846 \tValidation Loss: 1.053725\n",
      "Epoch: 89 \tTraining Loss: 0.652345 \tValidation Loss: 0.915579\n",
      "Epoch: 90 \tTraining Loss: 0.524501 \tValidation Loss: 0.714727\n",
      "validation loss decreased(0.742264 -->0.714727). Saving model ...\n",
      "Epoch: 91 \tTraining Loss: 0.530088 \tValidation Loss: 0.882464\n",
      "Epoch: 92 \tTraining Loss: 0.485012 \tValidation Loss: 0.782635\n",
      "Epoch: 93 \tTraining Loss: 0.519196 \tValidation Loss: 0.770091\n",
      "Epoch: 94 \tTraining Loss: 0.481208 \tValidation Loss: 0.788879\n",
      "Epoch: 95 \tTraining Loss: 0.475770 \tValidation Loss: 0.703697\n",
      "validation loss decreased(0.714727 -->0.703697). Saving model ...\n",
      "Epoch: 96 \tTraining Loss: 0.776210 \tValidation Loss: 0.747192\n",
      "Epoch: 97 \tTraining Loss: 0.501593 \tValidation Loss: 0.695734\n",
      "validation loss decreased(0.703697 -->0.695734). Saving model ...\n",
      "Epoch: 98 \tTraining Loss: 0.491264 \tValidation Loss: 0.752015\n",
      "Epoch: 99 \tTraining Loss: 0.466465 \tValidation Loss: 0.735639\n",
      "Epoch: 100 \tTraining Loss: 0.455768 \tValidation Loss: 0.753416\n",
      "Epoch: 101 \tTraining Loss: 0.477638 \tValidation Loss: 0.774127\n",
      "Epoch: 102 \tTraining Loss: 0.454796 \tValidation Loss: 0.759263\n",
      "Epoch: 103 \tTraining Loss: 0.416058 \tValidation Loss: 0.809118\n",
      "Epoch: 104 \tTraining Loss: 0.472381 \tValidation Loss: 0.755300\n",
      "Epoch: 105 \tTraining Loss: 0.421839 \tValidation Loss: 0.812819\n",
      "Epoch: 106 \tTraining Loss: 0.393023 \tValidation Loss: 0.792260\n",
      "Epoch: 107 \tTraining Loss: 0.388135 \tValidation Loss: 0.738767\n",
      "Epoch: 108 \tTraining Loss: 0.362587 \tValidation Loss: 3.620016\n",
      "Epoch: 109 \tTraining Loss: 0.772344 \tValidation Loss: 0.743960\n",
      "Epoch: 110 \tTraining Loss: 0.411310 \tValidation Loss: 0.780759\n",
      "Epoch: 111 \tTraining Loss: 0.401170 \tValidation Loss: 1.220149\n",
      "Epoch: 112 \tTraining Loss: 0.418710 \tValidation Loss: 0.827953\n",
      "Epoch: 113 \tTraining Loss: 0.366161 \tValidation Loss: 0.827598\n",
      "Epoch: 114 \tTraining Loss: 0.365885 \tValidation Loss: 0.747789\n",
      "Epoch: 115 \tTraining Loss: 0.463627 \tValidation Loss: 0.906415\n",
      "Epoch: 116 \tTraining Loss: 0.349863 \tValidation Loss: 0.824329\n",
      "Epoch: 117 \tTraining Loss: 0.339066 \tValidation Loss: 0.794097\n",
      "Epoch: 118 \tTraining Loss: 0.319006 \tValidation Loss: 0.880284\n",
      "Epoch: 119 \tTraining Loss: 0.331687 \tValidation Loss: 0.787791\n",
      "Epoch: 120 \tTraining Loss: 0.321463 \tValidation Loss: 0.871319\n",
      "Epoch: 121 \tTraining Loss: 0.451651 \tValidation Loss: 0.710327\n",
      "Epoch: 122 \tTraining Loss: 0.334620 \tValidation Loss: 0.796545\n",
      "Epoch: 123 \tTraining Loss: 0.334220 \tValidation Loss: 0.823457\n",
      "Epoch: 124 \tTraining Loss: 0.299953 \tValidation Loss: 0.809528\n",
      "Epoch: 125 \tTraining Loss: 0.291409 \tValidation Loss: 1.147812\n",
      "Epoch: 126 \tTraining Loss: 0.285402 \tValidation Loss: 1.156089\n",
      "Epoch: 127 \tTraining Loss: 0.354073 \tValidation Loss: 0.822902\n",
      "Epoch: 128 \tTraining Loss: 0.320753 \tValidation Loss: 0.859867\n",
      "Epoch: 129 \tTraining Loss: 0.304154 \tValidation Loss: 0.918243\n",
      "Epoch: 130 \tTraining Loss: 0.296384 \tValidation Loss: 0.838392\n",
      "Epoch: 131 \tTraining Loss: 0.419734 \tValidation Loss: 0.873827\n",
      "Epoch: 132 \tTraining Loss: 0.318869 \tValidation Loss: 0.914022\n",
      "Epoch: 133 \tTraining Loss: 0.254815 \tValidation Loss: 0.865003\n",
      "Epoch: 134 \tTraining Loss: 0.248674 \tValidation Loss: 1.169955\n",
      "Epoch: 135 \tTraining Loss: 0.246685 \tValidation Loss: 0.883289\n",
      "Epoch: 136 \tTraining Loss: 0.231439 \tValidation Loss: 0.944716\n",
      "Epoch: 137 \tTraining Loss: 0.268875 \tValidation Loss: 0.899800\n",
      "Epoch: 138 \tTraining Loss: 0.227423 \tValidation Loss: 0.979821\n",
      "Epoch: 139 \tTraining Loss: 0.571699 \tValidation Loss: 0.777041\n",
      "Epoch: 140 \tTraining Loss: 0.304243 \tValidation Loss: 0.857290\n",
      "Epoch: 141 \tTraining Loss: 0.237159 \tValidation Loss: 0.905445\n",
      "Epoch: 142 \tTraining Loss: 0.245929 \tValidation Loss: 0.978691\n",
      "Epoch: 143 \tTraining Loss: 0.214832 \tValidation Loss: 1.038763\n",
      "Epoch: 144 \tTraining Loss: 0.258055 \tValidation Loss: 1.108649\n",
      "Epoch: 145 \tTraining Loss: 0.885421 \tValidation Loss: 0.858790\n",
      "Epoch: 146 \tTraining Loss: 0.421200 \tValidation Loss: 0.796665\n",
      "Epoch: 147 \tTraining Loss: 0.301855 \tValidation Loss: 0.886864\n",
      "Epoch: 148 \tTraining Loss: 0.250851 \tValidation Loss: 0.980405\n",
      "Epoch: 149 \tTraining Loss: 0.270562 \tValidation Loss: 0.915363\n",
      "Epoch: 150 \tTraining Loss: 0.271016 \tValidation Loss: 0.952954\n",
      "Epoch: 151 \tTraining Loss: 0.220068 \tValidation Loss: 0.921550\n",
      "Epoch: 152 \tTraining Loss: 0.222935 \tValidation Loss: 1.105505\n",
      "Epoch: 153 \tTraining Loss: 0.290493 \tValidation Loss: 0.908544\n",
      "Epoch: 154 \tTraining Loss: 0.209505 \tValidation Loss: 0.966479\n",
      "Epoch: 155 \tTraining Loss: 0.207142 \tValidation Loss: 1.437631\n",
      "Epoch: 156 \tTraining Loss: 0.229143 \tValidation Loss: 1.159294\n",
      "Epoch: 157 \tTraining Loss: 0.206947 \tValidation Loss: 1.070442\n",
      "Epoch: 158 \tTraining Loss: 0.393090 \tValidation Loss: 0.856740\n",
      "Epoch: 159 \tTraining Loss: 0.240951 \tValidation Loss: 1.049248\n",
      "Epoch: 160 \tTraining Loss: 0.240407 \tValidation Loss: 0.890033\n",
      "Epoch: 161 \tTraining Loss: 0.188755 \tValidation Loss: 1.194522\n",
      "Epoch: 162 \tTraining Loss: 0.191833 \tValidation Loss: 1.133257\n",
      "Epoch: 163 \tTraining Loss: 0.185633 \tValidation Loss: 1.202746\n",
      "Epoch: 164 \tTraining Loss: 0.174739 \tValidation Loss: 1.117457\n",
      "Epoch: 165 \tTraining Loss: 0.168276 \tValidation Loss: 1.220010\n",
      "Epoch: 166 \tTraining Loss: 0.183513 \tValidation Loss: 2.244592\n",
      "Epoch: 167 \tTraining Loss: 0.427331 \tValidation Loss: 1.011770\n",
      "Epoch: 168 \tTraining Loss: 0.172983 \tValidation Loss: 1.111165\n",
      "Epoch: 169 \tTraining Loss: 0.182449 \tValidation Loss: 1.387481\n",
      "Epoch: 170 \tTraining Loss: 0.161125 \tValidation Loss: 1.267021\n",
      "Epoch: 171 \tTraining Loss: 0.202142 \tValidation Loss: 1.455166\n",
      "Epoch: 172 \tTraining Loss: 0.171583 \tValidation Loss: 1.213676\n",
      "Epoch: 173 \tTraining Loss: 0.266990 \tValidation Loss: 1.066598\n",
      "Epoch: 174 \tTraining Loss: 0.210079 \tValidation Loss: 1.263273\n",
      "Epoch: 175 \tTraining Loss: 0.169857 \tValidation Loss: 1.570927\n",
      "Epoch: 176 \tTraining Loss: 0.190001 \tValidation Loss: 1.809040\n",
      "Epoch: 177 \tTraining Loss: 0.196508 \tValidation Loss: 1.213409\n",
      "Epoch: 178 \tTraining Loss: 0.184801 \tValidation Loss: 1.258547\n",
      "Epoch: 179 \tTraining Loss: 0.152460 \tValidation Loss: 1.270809\n",
      "Epoch: 180 \tTraining Loss: 0.151510 \tValidation Loss: 1.860066\n",
      "Epoch: 181 \tTraining Loss: 0.176427 \tValidation Loss: 1.347180\n",
      "Epoch: 182 \tTraining Loss: 0.187050 \tValidation Loss: 1.521700\n",
      "Epoch: 183 \tTraining Loss: 0.177659 \tValidation Loss: 1.360776\n",
      "Epoch: 184 \tTraining Loss: 0.141147 \tValidation Loss: 1.312710\n",
      "Epoch: 185 \tTraining Loss: 0.114387 \tValidation Loss: 1.510024\n",
      "Epoch: 186 \tTraining Loss: 0.298330 \tValidation Loss: 1.191598\n",
      "Epoch: 187 \tTraining Loss: 0.153801 \tValidation Loss: 1.296480\n",
      "Epoch: 188 \tTraining Loss: 0.136949 \tValidation Loss: 1.314831\n",
      "Epoch: 189 \tTraining Loss: 0.128585 \tValidation Loss: 1.429195\n",
      "Epoch: 190 \tTraining Loss: 0.122657 \tValidation Loss: 1.566712\n",
      "Epoch: 191 \tTraining Loss: 0.134621 \tValidation Loss: 1.275288\n",
      "Epoch: 192 \tTraining Loss: 0.152800 \tValidation Loss: 1.487536\n",
      "Epoch: 193 \tTraining Loss: 0.160100 \tValidation Loss: 1.447903\n",
      "Epoch: 194 \tTraining Loss: 0.124160 \tValidation Loss: 1.513015\n",
      "Epoch: 195 \tTraining Loss: 0.313928 \tValidation Loss: 1.241984\n",
      "Epoch: 196 \tTraining Loss: 0.153169 \tValidation Loss: 1.278775\n",
      "Epoch: 197 \tTraining Loss: 0.117582 \tValidation Loss: 1.368841\n",
      "Epoch: 198 \tTraining Loss: 0.174413 \tValidation Loss: 1.506194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 199 \tTraining Loss: 1.069337 \tValidation Loss: 1.258136\n",
      "Epoch: 200 \tTraining Loss: 0.246911 \tValidation Loss: 1.069874\n"
     ]
    }
   ],
   "source": [
    "valid_loss_min = np.Inf\n",
    "steps = 0\n",
    "train_losses, valid_losses = [], []\n",
    "model.train()\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    valid_loss = 0\n",
    "    for images, labels in trainloader:\n",
    "        optimizer.zero_grad()\n",
    "        log_ps = model(images)\n",
    "        loss = criterion(log_ps, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()*images.size(0)\n",
    "\n",
    "    for images, labels in validloader:\n",
    "        log_ps = model(images)\n",
    "        loss = criterion(log_ps, labels)\n",
    "        valid_loss += loss.item()*images.size(0)\n",
    "\n",
    "    running_loss = running_loss/len(trainloader.sampler)\n",
    "    valid_loss = valid_loss/len(validloader.sampler)\n",
    "    train_losses.append(running_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(e+1, running_loss, valid_loss))\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('validation loss decreased({:.6f} -->{:.6f}). Saving model ...'.format(valid_loss_min, valid_loss))\n",
    "        torch.save(model.state_dict(), 'saved/{}/model_pretrain{}.pt'.format(load_path, dropout))\n",
    "        valid_loss_min = valid_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the training and validation losses in every epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(valid_losses, label='Valid Loss', linestyle='dashed')\n",
    "plt.xlabel('Pre-training epochs')\n",
    "plt.ylim((0, 2.5))\n",
    "plt.legend(fontsize=16)\n",
    "plt.grid(True)\n",
    "plt.savefig('saved/{}/loss_pretrain{}.pdf'.format(load_path, dropout))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot helps the reader visualize the overfitting effect of the model, due to the huge size of the neural network in relation to the number of samples and the lack of regularization mechanisms. However, we save the best model in the training loop, before the validation loss starts to increase around epoch 100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load best model trained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Classifier(\n",
       "  (fc1): Linear(in_features=784, out_features=700, bias=True)\n",
       "  (fc2): Linear(in_features=700, out_features=600, bias=True)\n",
       "  (fc3): Linear(in_features=600, out_features=512, bias=True)\n",
       "  (fc4): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (fc5): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (fc6): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (fc7): Linear(in_features=64, out_features=32, bias=True)\n",
       "  (fc8): Linear(in_features=32, out_features=16, bias=True)\n",
       "  (fc9): Linear(in_features=16, out_features=10, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('saved/{}/model_pretrain{}.pt'.format(load_path, dropout)))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print test results in order to visualize the images in the dataset and the samples which are more difficult to classify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1800x288 with 20 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "output = model(images)\n",
    "_, preds = torch.max(output, 1)\n",
    "images = images.numpy()\n",
    "\n",
    "fig = plt.figure(figsize=(25, 4))\n",
    "for idx in np.arange(20):\n",
    "    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\n",
    "    ax.imshow(np.squeeze(images[idx]), cmap='gray')\n",
    "    ax.set_title(\"{} ({})\".format(str(preds[idx].item()), str(labels[idx].item())), color=(\"green\" if preds[idx]==labels[idx] else \"red\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training GMVAE with some hidden vectors within the base model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We include some libraries and scripts with the GMVAE definition and with the suitable functions to train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_path = './GMVAE'\n",
    "sys.path.append(os.path.abspath(add_path))\n",
    "\n",
    "from GMVAE import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define all the paraemters for the GMVAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Loading parameters...\n",
      "cuda set to False\n",
      "====================================================================================================\n",
      "PARAMETERS AND CONFIGURATION\n",
      "- name_extension: \n",
      "- dataset_name: FMNIST_200epochs_dp0_lr001\n",
      "- dataroot: \n",
      "- train_dataroot: \n",
      "- test_dataroot: \n",
      "- cuda: 0\n",
      "- device: 0\n",
      "- epochs: 400\n",
      "- batch_size: 64\n",
      "- l_rate: 0.001\n",
      "- dropout: 0.3\n",
      "- weight_decay: 0\n",
      "- hidden_dim: 50\n",
      "- sigma: 0.0001\n",
      "- z_dim: 25\n",
      "- w_dim: 25\n",
      "- K: 10\n",
      "- layers: 3\n",
      "- remove: 1\n",
      "- checkpoint_dir: experiments/FMNIST_200epochs_dp0_lr001/checkpoints/GMVAE_1_00001_25_25_10_50_3_03_0001\n",
      "- result_dir: experiments/FMNIST_200epochs_dp0_lr001/results/GMVAE_1_00001_25_25_10_50_3_03_0001\n",
      "- board_dir: experiments/FMNIST_200epochs_dp0_lr001/summary/GMVAE_1_00001_25_25_10_50_3_03_0001\n",
      "- train: 1\n",
      "- summary: 1\n",
      "- plot: 0\n",
      "- restore: 0\n",
      "- results: 1\n",
      "- verbose: 1\n",
      "- extra: \n",
      "- step_restore: 400\n",
      "- checkpoint_step: 10\n",
      "- option: 1\n",
      "- model_name: GMVAE_1_00001_25_25_10_50_3_03_0001\n",
      "- input_dim: 700\n",
      "====================================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\n Loading parameters...')\n",
    "\n",
    "config = Bunch()\n",
    "\n",
    "config.name_extension = \"\"\n",
    "config.dataset_name = \"FMNIST_\" + load_path\n",
    "config.dataroot = \"\"\n",
    "config.train_dataroot = \"\"\n",
    "config.test_dataroot = \"\"\n",
    "config.cuda = 0\n",
    "config.device = 0\n",
    "config.epochs = 400\n",
    "config.batch_size = 64\n",
    "config.l_rate = 1e-3\n",
    "config.dropout = 0.3\n",
    "config.weight_decay = 0\n",
    "config.hidden_dim = 50\n",
    "config.sigma = 1e-4\n",
    "config.z_dim = 25\n",
    "config.w_dim = 25\n",
    "config.K = 10\n",
    "config.layers = 3\n",
    "config.remove = 1\n",
    "config.checkpoint_dir = \"checkpoints\"\n",
    "config.result_dir = \"results\"\n",
    "config.board_dir = \"summary\"\n",
    "config.train = 1\n",
    "config.summary = 1 \n",
    "config.plot = 0\n",
    "config.restore = 0\n",
    "config.results = 1\n",
    "config.verbose = 1\n",
    "config.extra = \"\"\n",
    "config.step_restore = 400\n",
    "config.checkpoint_step = 10\n",
    "config.option = 1\n",
    "\n",
    "\n",
    "# capture the config path from the input arguments\n",
    "config.model_name = get_model_name(config)\n",
    "\n",
    "if (config.extra is not ''):\n",
    "    config.model_name += '_' + config.extra\n",
    "\n",
    "config.board_dir = os.path.join(\"experiments/\" + config.dataset_name + '/' + config.board_dir + \"/\",\n",
    "                                config.model_name)\n",
    "config.checkpoint_dir = os.path.join(\"experiments/\" + config.dataset_name + '/' + config.checkpoint_dir + \"/\",\n",
    "                                     config.model_name)\n",
    "config.result_dir = os.path.join(\"experiments/\" + config.dataset_name + '/' + config.result_dir + \"/\",\n",
    "                                 config.model_name)\n",
    "\n",
    "flags = Bunch()\n",
    "flags.train = config['train']\n",
    "flags.summary = config['summary']\n",
    "flags.restore = config['restore']\n",
    "flags.verbose = config['verbose']\n",
    "flags.results = config['results']\n",
    "\n",
    "\n",
    "# Create the experiment directories\n",
    "create_dirs([config.board_dir, config.checkpoint_dir, config.result_dir])\n",
    "\n",
    "# Save configuration arguments in a txt\n",
    "save_args(config, config.board_dir)\n",
    "\n",
    "# Set GPU configuration\n",
    "if config.cuda and torch.cuda.is_available():\n",
    "    torch.cuda.set_device(config.device)\n",
    "else:\n",
    "    print('cuda set to False')\n",
    "    config.cuda = 0\n",
    "\n",
    "config.input_dim = 700\n",
    "\n",
    "\n",
    "print('='*100)\n",
    "print('PARAMETERS AND CONFIGURATION')\n",
    "for k, v in config.items(): print('- {}: {}'.format(k, v))\n",
    "print('='*100 + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the pre-trained model because we need it to compute the inputs to the GMVAE, that is, the hidden vectors after the first layer that we are going to regularize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Classifier(\n",
       "  (fc1): Linear(in_features=784, out_features=700, bias=True)\n",
       "  (fc2): Linear(in_features=700, out_features=600, bias=True)\n",
       "  (fc3): Linear(in_features=600, out_features=512, bias=True)\n",
       "  (fc4): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (fc5): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (fc6): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (fc7): Linear(in_features=64, out_features=32, bias=True)\n",
       "  (fc8): Linear(in_features=32, out_features=16, bias=True)\n",
       "  (fc9): Linear(in_features=16, out_features=10, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Classifier()\n",
    "\n",
    "model.load_state_dict(torch.load('saved/{}/model_pretrain0.pt'.format(load_path)))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the dataset and use the function *obtain_input_to_regularize_lower* from the Classifier to transform the original images to the hidden vector in target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_GMVAE = transforms.Compose([transforms.ToTensor(), model.obtain_input_to_regularize_lower])\n",
    "\n",
    "trainset_GMVAE = datasets.FashionMNIST('~/.python/F_MNIST_data', download=True, train=True, transform=transform_GMVAE)\n",
    "\n",
    "indices = list(range(len(trainset_GMVAE)))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(0.2 * len(trainset_GMVAE)))\n",
    "train_sample = SubsetRandomSampler(indices[:split])\n",
    "valid_sample = SubsetRandomSampler(indices[split:])\n",
    "\n",
    "trainloader_GMVAE = torch.utils.data.DataLoader(trainset_GMVAE, sampler=train_sample, batch_size=64)\n",
    "validloader_GMVAE = torch.utils.data.DataLoader(trainset_GMVAE, sampler=valid_sample, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the GMVAE model with the parameters defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Building computation graph...\n",
      "====================================================================================================\n",
      "Saving config to \"experiments/FMNIST_200epochs_dp0_lr001/checkpoints/GMVAE_1_00001_25_25_10_50_3_03_0001/config.pt\".\n",
      "====================================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\n Building computation graph...')\n",
    "GMVAE_model = GMVAE(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the GMVAE model the the cofiguration and the inputs defined before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 203260\n",
      "(Epoch 1 / 400)\n",
      "Train - Log-lik: -117049.85325;  CP: 25.42033;  KL_w: 0.11260;  KL_y: 0.04933;\n",
      "Valid - Log-lik: -54213.86672;  CP: 40.18926;  KL_w: 0.00376;  KL_y: 0.09635;\n",
      "Total Loss=Train: 117075.43420; Val: 54254.15625\n",
      "(Epoch 2 / 400)\n",
      "Train - Log-lik: -57868.09664;  CP: 47.76927;  KL_w: 0.02192;  KL_y: 0.19268;\n",
      "Valid - Log-lik: -42040.81509;  CP: 53.40542;  KL_w: 0.00118;  KL_y: 0.29125;\n",
      "Total Loss=Train: 57916.08024; Val: 42094.51169\n",
      "(Epoch 3 / 400)\n",
      "Train - Log-lik: -49487.34803;  CP: 57.61246;  KL_w: 0.01059;  KL_y: 0.46478;\n",
      "Valid - Log-lik: -36401.61922;  CP: 60.94118;  KL_w: 0.00047;  KL_y: 0.67526;\n",
      "Total Loss=Train: 49545.43573; Val: 36463.23568\n",
      "(Epoch 4 / 400)\n",
      "Train - Log-lik: -45126.34192;  CP: 64.41055;  KL_w: 0.00719;  KL_y: 0.72697;\n",
      "Valid - Log-lik: -33142.58237;  CP: 67.12546;  KL_w: 0.00010;  KL_y: 0.80403;\n",
      "Total Loss=Train: 45191.48689; Val: 33210.51187\n",
      "(Epoch 5 / 400)\n",
      "Train - Log-lik: -43648.70805;  CP: 69.90003;  KL_w: 0.00576;  KL_y: 0.88773;\n",
      "Valid - Log-lik: -31193.12264;  CP: 71.63201;  KL_w: 0.00008;  KL_y: 0.95375;\n",
      "Total Loss=Train: 43719.50145; Val: 31265.70841\n",
      "(Epoch 6 / 400)\n",
      "Train - Log-lik: -42305.82102;  CP: 73.90941;  KL_w: 0.00501;  KL_y: 0.98921;\n",
      "Valid - Log-lik: -30800.71664;  CP: 75.26462;  KL_w: 0.00005;  KL_y: 1.09288;\n",
      "Total Loss=Train: 42380.72411; Val: 30877.07414\n",
      "(Epoch 7 / 400)\n",
      "Train - Log-lik: -41443.52775;  CP: 77.03851;  KL_w: 0.00427;  KL_y: 1.07931;\n",
      "Valid - Log-lik: -30465.42109;  CP: 78.23577;  KL_w: 0.00011;  KL_y: 1.14792;\n",
      "Total Loss=Train: 41521.64952; Val: 30544.80473\n",
      "(Epoch 8 / 400)\n",
      "Train - Log-lik: -41026.69429;  CP: 79.69960;  KL_w: 0.00382;  KL_y: 1.14345;\n",
      "Valid - Log-lik: -30594.93673;  CP: 80.35831;  KL_w: 0.00009;  KL_y: 1.19166;\n",
      "Total Loss=Train: 41107.54118; Val: 30676.48669\n",
      "(Epoch 9 / 400)\n",
      "Train - Log-lik: -40588.52821;  CP: 81.89394;  KL_w: 0.00320;  KL_y: 1.18842;\n",
      "Valid - Log-lik: -30276.64818;  CP: 82.36433;  KL_w: 0.00003;  KL_y: 1.23952;\n",
      "Total Loss=Train: 40671.61446; Val: 30360.25201\n",
      "(Epoch 10 / 400)\n",
      "Train - Log-lik: -40216.12043;  CP: 83.58238;  KL_w: 0.00272;  KL_y: 1.22230;\n",
      "Valid - Log-lik: -29695.26454;  CP: 83.93422;  KL_w: 0.00002;  KL_y: 1.27258;\n",
      "Total Loss=Train: 40300.92875; Val: 29780.47135\n",
      "====================================================================================================\n",
      "Saving checkpoint to \"experiments/FMNIST_200epochs_dp0_lr001/checkpoints/GMVAE_1_00001_25_25_10_50_3_03_0001/step_10.pt\".\n",
      "====================================================================================================\n",
      "\n",
      "(Epoch 11 / 400)\n",
      "Train - Log-lik: -39749.30606;  CP: 85.03329;  KL_w: 0.00234;  KL_y: 1.24721;\n",
      "Valid - Log-lik: -29997.54177;  CP: 85.17073;  KL_w: 0.00002;  KL_y: 1.30118;\n",
      "Total Loss=Train: 39835.58982; Val: 30084.01368\n",
      "(Epoch 12 / 400)\n",
      "Train - Log-lik: -39785.06409;  CP: 85.99145;  KL_w: 0.00196;  KL_y: 1.27609;\n",
      "Valid - Log-lik: -30112.56552;  CP: 86.43665;  KL_w: 0.00004;  KL_y: 1.28061;\n",
      "Total Loss=Train: 39872.33339; Val: 30200.28278\n",
      "(Epoch 13 / 400)\n",
      "Train - Log-lik: -39489.90461;  CP: 87.23281;  KL_w: 0.00165;  KL_y: 1.30204;\n",
      "Valid - Log-lik: -30035.63170;  CP: 87.53401;  KL_w: 0.00004;  KL_y: 1.35598;\n",
      "Total Loss=Train: 39578.43991; Val: 30124.52170\n",
      "(Epoch 14 / 400)\n",
      "Train - Log-lik: -39416.41423;  CP: 87.99467;  KL_w: 0.00137;  KL_y: 1.32470;\n",
      "Valid - Log-lik: -29897.88638;  CP: 88.11812;  KL_w: 0.00004;  KL_y: 1.33510;\n",
      "Total Loss=Train: 39505.73361; Val: 29987.33958\n",
      "(Epoch 15 / 400)\n",
      "Train - Log-lik: -39008.67435;  CP: 89.12584;  KL_w: 0.00113;  KL_y: 1.33186;\n",
      "Valid - Log-lik: -29962.25240;  CP: 89.13270;  KL_w: 0.00001;  KL_y: 1.34382;\n",
      "Total Loss=Train: 39099.13198; Val: 30052.72898\n",
      "(Epoch 16 / 400)\n",
      "Train - Log-lik: -38982.51968;  CP: 89.72449;  KL_w: 0.00094;  KL_y: 1.34162;\n",
      "Valid - Log-lik: -30118.58899;  CP: 89.34714;  KL_w: 0.00004;  KL_y: 1.37279;\n",
      "Total Loss=Train: 39073.58565; Val: 30209.30887\n",
      "(Epoch 17 / 400)\n",
      "Train - Log-lik: -38845.73142;  CP: 89.87155;  KL_w: 0.00073;  KL_y: 1.35335;\n",
      "Valid - Log-lik: -30012.95801;  CP: 89.41846;  KL_w: 0.00001;  KL_y: 1.38517;\n",
      "Total Loss=Train: 38936.95630; Val: 30103.76167\n",
      "(Epoch 18 / 400)\n",
      "Train - Log-lik: -38775.65290;  CP: 90.11726;  KL_w: 0.00059;  KL_y: 1.36386;\n",
      "Valid - Log-lik: -29666.31608;  CP: 89.85277;  KL_w: 0.00002;  KL_y: 1.38175;\n",
      "Total Loss=Train: 38867.13391; Val: 29757.55059\n",
      "(Epoch 19 / 400)\n",
      "Train - Log-lik: -38890.91612;  CP: 90.15364;  KL_w: 0.00044;  KL_y: 1.36758;\n",
      "Valid - Log-lik: -29941.07702;  CP: 89.49248;  KL_w: 0.00001;  KL_y: 1.40096;\n",
      "Total Loss=Train: 38982.43744; Val: 30031.97054\n",
      "(Epoch 20 / 400)\n",
      "Train - Log-lik: -38577.51444;  CP: 90.25922;  KL_w: 0.00034;  KL_y: 1.37363;\n",
      "Valid - Log-lik: -30029.01422;  CP: 89.87448;  KL_w: 0.00001;  KL_y: 1.39368;\n",
      "Total Loss=Train: 38669.14737; Val: 30120.28237\n",
      "====================================================================================================\n",
      "Saving checkpoint to \"experiments/FMNIST_200epochs_dp0_lr001/checkpoints/GMVAE_1_00001_25_25_10_50_3_03_0001/step_20.pt\".\n",
      "====================================================================================================\n",
      "\n",
      "(Epoch 21 / 400)\n",
      "Train - Log-lik: -38489.48480;  CP: 90.27572;  KL_w: 0.00026;  KL_y: 1.38385;\n",
      "Valid - Log-lik: -29962.17140;  CP: 89.19364;  KL_w: 0.00001;  KL_y: 1.42307;\n",
      "Total Loss=Train: 38581.14428; Val: 30052.78813\n",
      "(Epoch 22 / 400)\n",
      "Train - Log-lik: -38380.70375;  CP: 89.66154;  KL_w: 0.00020;  KL_y: 1.39359;\n",
      "Valid - Log-lik: -29721.90519;  CP: 88.90130;  KL_w: 0.00000;  KL_y: 1.44639;\n",
      "Total Loss=Train: 38471.75891; Val: 29812.25292\n",
      "(Epoch 23 / 400)\n",
      "Train - Log-lik: -38338.30352;  CP: 89.64401;  KL_w: 0.00015;  KL_y: 1.40814;\n",
      "Valid - Log-lik: -29726.15841;  CP: 88.55690;  KL_w: 0.00000;  KL_y: 1.46522;\n",
      "Total Loss=Train: 38429.35570; Val: 29816.18049\n",
      "(Epoch 24 / 400)\n",
      "Train - Log-lik: -38414.54730;  CP: 89.70779;  KL_w: 0.00011;  KL_y: 1.40618;\n",
      "Valid - Log-lik: -30228.38048;  CP: 88.44448;  KL_w: 0.00000;  KL_y: 1.48134;\n",
      "Total Loss=Train: 38505.66118; Val: 30318.30631\n",
      "(Epoch 25 / 400)\n",
      "Train - Log-lik: -38249.29534;  CP: 89.50561;  KL_w: 0.00008;  KL_y: 1.41976;\n",
      "Valid - Log-lik: -29516.07597;  CP: 88.00122;  KL_w: 0.00000;  KL_y: 1.46551;\n",
      "Total Loss=Train: 38340.22068; Val: 29605.54267\n",
      "(Epoch 26 / 400)\n",
      "Train - Log-lik: -38282.98828;  CP: 88.49916;  KL_w: 0.00006;  KL_y: 1.42619;\n",
      "Valid - Log-lik: -29503.18277;  CP: 87.92722;  KL_w: 0.00000;  KL_y: 1.48585;\n",
      "Total Loss=Train: 38372.91345; Val: 29592.59584\n",
      "(Epoch 27 / 400)\n",
      "Train - Log-lik: -38132.65083;  CP: 88.51810;  KL_w: 0.00005;  KL_y: 1.43188;\n",
      "Valid - Log-lik: -30265.82283;  CP: 87.33326;  KL_w: 0.00001;  KL_y: 1.44857;\n",
      "Total Loss=Train: 38222.60084; Val: 30354.60466\n",
      "(Epoch 28 / 400)\n",
      "Train - Log-lik: -38103.92703;  CP: 88.52436;  KL_w: 0.00003;  KL_y: 1.44210;\n",
      "Valid - Log-lik: -29514.69034;  CP: 87.80432;  KL_w: 0.00000;  KL_y: 1.48901;\n",
      "Total Loss=Train: 38193.89343; Val: 29603.98366\n",
      "(Epoch 29 / 400)\n",
      "Train - Log-lik: -37768.89141;  CP: 88.36403;  KL_w: 0.00003;  KL_y: 1.44543;\n",
      "Valid - Log-lik: -29816.07384;  CP: 87.20504;  KL_w: 0.00001;  KL_y: 1.48516;\n",
      "Total Loss=Train: 37858.70079; Val: 29904.76406\n",
      "(Epoch 30 / 400)\n",
      "Train - Log-lik: -37642.89081;  CP: 87.89970;  KL_w: 0.00003;  KL_y: 1.45766;\n",
      "Valid - Log-lik: -29897.42983;  CP: 87.20694;  KL_w: 0.00002;  KL_y: 1.56510;\n",
      "Total Loss=Train: 37732.24819; Val: 29986.20187\n",
      "====================================================================================================\n",
      "Saving checkpoint to \"experiments/FMNIST_200epochs_dp0_lr001/checkpoints/GMVAE_1_00001_25_25_10_50_3_03_0001/step_30.pt\".\n",
      "====================================================================================================\n",
      "\n",
      "(Epoch 31 / 400)\n",
      "Train - Log-lik: -37581.57014;  CP: 87.59884;  KL_w: 0.00002;  KL_y: 1.47747;\n",
      "Valid - Log-lik: -29426.11142;  CP: 86.77528;  KL_w: 0.00001;  KL_y: 1.53430;\n",
      "Total Loss=Train: 37670.64658; Val: 29514.42101\n",
      "(Epoch 32 / 400)\n",
      "Train - Log-lik: -37635.05163;  CP: 87.41193;  KL_w: 0.00002;  KL_y: 1.48851;\n",
      "Valid - Log-lik: -29425.18021;  CP: 86.31921;  KL_w: 0.00001;  KL_y: 1.56342;\n",
      "Total Loss=Train: 37723.95215; Val: 29513.06280\n",
      "(Epoch 33 / 400)\n",
      "Train - Log-lik: -37496.94675;  CP: 87.13986;  KL_w: 0.00002;  KL_y: 1.49230;\n",
      "Valid - Log-lik: -29863.07386;  CP: 85.73330;  KL_w: 0.00001;  KL_y: 1.57172;\n",
      "Total Loss=Train: 37585.57892; Val: 29950.37885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 34 / 400)\n",
      "Train - Log-lik: -37323.81902;  CP: 86.36303;  KL_w: 0.00002;  KL_y: 1.51516;\n",
      "Valid - Log-lik: -29560.50840;  CP: 84.50493;  KL_w: 0.00001;  KL_y: 1.57970;\n",
      "Total Loss=Train: 37411.69733; Val: 29646.59300\n",
      "(Epoch 35 / 400)\n",
      "Train - Log-lik: -37145.16475;  CP: 85.52984;  KL_w: 0.00002;  KL_y: 1.51586;\n",
      "Valid - Log-lik: -29398.65110;  CP: 84.67166;  KL_w: 0.00001;  KL_y: 1.61050;\n",
      "Total Loss=Train: 37232.21039; Val: 29484.93328\n",
      "(Epoch 36 / 400)\n",
      "Train - Log-lik: -36989.84103;  CP: 86.14948;  KL_w: 0.00002;  KL_y: 1.52910;\n",
      "Valid - Log-lik: -29074.62070;  CP: 85.04011;  KL_w: 0.00001;  KL_y: 1.56140;\n",
      "Total Loss=Train: 37077.51951; Val: 29161.22219\n",
      "(Epoch 37 / 400)\n",
      "Train - Log-lik: -36899.51346;  CP: 86.35504;  KL_w: 0.00001;  KL_y: 1.52570;\n",
      "Valid - Log-lik: -28382.92288;  CP: 85.87227;  KL_w: 0.00001;  KL_y: 1.61016;\n",
      "Total Loss=Train: 36987.39434; Val: 28470.40530\n",
      "(Epoch 38 / 400)\n",
      "Train - Log-lik: -36804.37411;  CP: 86.83154;  KL_w: 0.00001;  KL_y: 1.53635;\n",
      "Valid - Log-lik: -28773.21959;  CP: 86.44230;  KL_w: 0.00001;  KL_y: 1.60787;\n",
      "Total Loss=Train: 36892.74200; Val: 28861.26978\n",
      "(Epoch 39 / 400)\n",
      "Train - Log-lik: -36535.12619;  CP: 87.63384;  KL_w: 0.00001;  KL_y: 1.53660;\n",
      "Valid - Log-lik: -29386.01489;  CP: 86.29330;  KL_w: 0.00001;  KL_y: 1.59110;\n",
      "Total Loss=Train: 36624.29658; Val: 29473.89925\n",
      "(Epoch 40 / 400)\n",
      "Train - Log-lik: -36476.98534;  CP: 87.20483;  KL_w: 0.00002;  KL_y: 1.53148;\n",
      "Valid - Log-lik: -27914.82550;  CP: 85.85195;  KL_w: 0.00001;  KL_y: 1.57424;\n",
      "Total Loss=Train: 36565.72156; Val: 28002.25172\n",
      "====================================================================================================\n",
      "Saving checkpoint to \"experiments/FMNIST_200epochs_dp0_lr001/checkpoints/GMVAE_1_00001_25_25_10_50_3_03_0001/step_40.pt\".\n",
      "====================================================================================================\n",
      "\n",
      "(Epoch 41 / 400)\n",
      "Train - Log-lik: -36327.27595;  CP: 86.90548;  KL_w: 0.00002;  KL_y: 1.53520;\n",
      "Valid - Log-lik: -27983.05948;  CP: 85.51125;  KL_w: 0.00001;  KL_y: 1.58247;\n",
      "Total Loss=Train: 36415.71662; Val: 28070.15322\n",
      "(Epoch 42 / 400)\n",
      "Train - Log-lik: -36375.19167;  CP: 86.51948;  KL_w: 0.00001;  KL_y: 1.54812;\n",
      "Valid - Log-lik: -28084.76200;  CP: 85.25315;  KL_w: 0.00001;  KL_y: 1.59682;\n",
      "Total Loss=Train: 36463.25924; Val: 28171.61195\n",
      "(Epoch 43 / 400)\n",
      "Train - Log-lik: -36438.79065;  CP: 85.94090;  KL_w: 0.00001;  KL_y: 1.55643;\n",
      "Valid - Log-lik: -28048.66541;  CP: 84.72097;  KL_w: 0.00001;  KL_y: 1.64405;\n",
      "Total Loss=Train: 36526.28813; Val: 28135.03044\n",
      "(Epoch 44 / 400)\n",
      "Train - Log-lik: -36139.53497;  CP: 86.37272;  KL_w: 0.00001;  KL_y: 1.57109;\n",
      "Valid - Log-lik: -27904.90765;  CP: 85.91628;  KL_w: 0.00002;  KL_y: 1.63329;\n",
      "Total Loss=Train: 36227.47878; Val: 27992.45726\n",
      "(Epoch 45 / 400)\n",
      "Train - Log-lik: -36069.36835;  CP: 88.10476;  KL_w: 0.00002;  KL_y: 1.57168;\n",
      "Valid - Log-lik: -28064.61700;  CP: 86.97982;  KL_w: 0.00001;  KL_y: 1.65035;\n",
      "Total Loss=Train: 36159.04477; Val: 28153.24716\n",
      "(Epoch 46 / 400)\n",
      "Train - Log-lik: -36179.91615;  CP: 88.15997;  KL_w: 0.00001;  KL_y: 1.57824;\n",
      "Valid - Log-lik: -27774.42014;  CP: 87.37781;  KL_w: 0.00000;  KL_y: 1.67015;\n",
      "Total Loss=Train: 36269.65433; Val: 27863.46804\n",
      "(Epoch 47 / 400)\n",
      "Train - Log-lik: -35784.74466;  CP: 87.19373;  KL_w: 0.00001;  KL_y: 1.57728;\n",
      "Valid - Log-lik: -27623.48261;  CP: 85.31579;  KL_w: 0.00001;  KL_y: 1.66513;\n",
      "Total Loss=Train: 35873.51566; Val: 27710.46352\n",
      "(Epoch 48 / 400)\n",
      "Train - Log-lik: -35902.45861;  CP: 86.64328;  KL_w: 0.00001;  KL_y: 1.57619;\n",
      "Valid - Log-lik: -27353.20911;  CP: 85.62557;  KL_w: 0.00001;  KL_y: 1.63486;\n",
      "Total Loss=Train: 35990.67809; Val: 27440.46951\n",
      "(Epoch 49 / 400)\n",
      "Train - Log-lik: -35709.92253;  CP: 86.74896;  KL_w: 0.00001;  KL_y: 1.57608;\n",
      "Valid - Log-lik: -27393.97550;  CP: 85.32602;  KL_w: 0.00001;  KL_y: 1.64478;\n",
      "Total Loss=Train: 35798.24740; Val: 27480.94630\n",
      "(Epoch 50 / 400)\n",
      "Train - Log-lik: -35711.96306;  CP: 87.69578;  KL_w: 0.00001;  KL_y: 1.57262;\n",
      "Valid - Log-lik: -27229.07557;  CP: 87.80786;  KL_w: 0.00000;  KL_y: 1.66130;\n",
      "Total Loss=Train: 35801.23155; Val: 27318.54472\n",
      "====================================================================================================\n",
      "Saving checkpoint to \"experiments/FMNIST_200epochs_dp0_lr001/checkpoints/GMVAE_1_00001_25_25_10_50_3_03_0001/step_50.pt\".\n",
      "====================================================================================================\n",
      "\n",
      "(Epoch 51 / 400)\n",
      "Train - Log-lik: -35456.77498;  CP: 89.91367;  KL_w: 0.00001;  KL_y: 1.57511;\n",
      "Valid - Log-lik: -26952.65254;  CP: 89.30649;  KL_w: 0.00001;  KL_y: 1.70193;\n",
      "Total Loss=Train: 35548.26382; Val: 27043.66097\n",
      "(Epoch 52 / 400)\n",
      "Train - Log-lik: -35333.39106;  CP: 89.47165;  KL_w: 0.00001;  KL_y: 1.57891;\n",
      "Valid - Log-lik: -26632.34914;  CP: 87.90388;  KL_w: 0.00001;  KL_y: 1.65590;\n",
      "Total Loss=Train: 35424.44170; Val: 26721.90891\n",
      "(Epoch 53 / 400)\n",
      "Train - Log-lik: -35290.35531;  CP: 89.93993;  KL_w: 0.00001;  KL_y: 1.58003;\n",
      "Valid - Log-lik: -26936.35932;  CP: 88.81933;  KL_w: 0.00001;  KL_y: 1.63468;\n",
      "Total Loss=Train: 35381.87509; Val: 27026.81334\n",
      "(Epoch 54 / 400)\n",
      "Train - Log-lik: -35197.01849;  CP: 91.02831;  KL_w: 0.00001;  KL_y: 1.58009;\n",
      "Valid - Log-lik: -26594.95664;  CP: 90.29053;  KL_w: 0.00001;  KL_y: 1.64455;\n",
      "Total Loss=Train: 35289.62710; Val: 26686.89170\n",
      "(Epoch 55 / 400)\n",
      "Train - Log-lik: -35053.52425;  CP: 90.56416;  KL_w: 0.00001;  KL_y: 1.57196;\n",
      "Valid - Log-lik: -26483.11042;  CP: 89.23866;  KL_w: 0.00001;  KL_y: 1.68538;\n",
      "Total Loss=Train: 35145.66053; Val: 26574.03444\n",
      "(Epoch 56 / 400)\n",
      "Train - Log-lik: -34944.75760;  CP: 90.43356;  KL_w: 0.00001;  KL_y: 1.56735;\n",
      "Valid - Log-lik: -26303.75236;  CP: 89.62898;  KL_w: 0.00001;  KL_y: 1.63829;\n",
      "Total Loss=Train: 35036.75832; Val: 26395.01961\n",
      "(Epoch 57 / 400)\n",
      "Train - Log-lik: -34974.46594;  CP: 90.36089;  KL_w: 0.00001;  KL_y: 1.57196;\n",
      "Valid - Log-lik: -26374.44328;  CP: 88.54981;  KL_w: 0.00000;  KL_y: 1.66126;\n",
      "Total Loss=Train: 35066.39874; Val: 26464.65432\n",
      "(Epoch 58 / 400)\n",
      "Train - Log-lik: -34767.58324;  CP: 89.92793;  KL_w: 0.00001;  KL_y: 1.56643;\n",
      "Valid - Log-lik: -26290.54612;  CP: 88.51650;  KL_w: 0.00000;  KL_y: 1.64354;\n",
      "Total Loss=Train: 34859.07755; Val: 26380.70618\n",
      "(Epoch 59 / 400)\n",
      "Train - Log-lik: -34790.02191;  CP: 89.27434;  KL_w: 0.00001;  KL_y: 1.55233;\n",
      "Valid - Log-lik: -26114.57600;  CP: 87.44349;  KL_w: 0.00000;  KL_y: 1.62316;\n",
      "Total Loss=Train: 34880.84848; Val: 26203.64266\n",
      "(Epoch 60 / 400)\n",
      "Train - Log-lik: -34661.29720;  CP: 91.12079;  KL_w: 0.00001;  KL_y: 1.55068;\n",
      "Valid - Log-lik: -25901.15625;  CP: 90.39133;  KL_w: 0.00001;  KL_y: 1.64840;\n",
      "Total Loss=Train: 34753.96875; Val: 25993.19595\n",
      "====================================================================================================\n",
      "Saving checkpoint to \"experiments/FMNIST_200epochs_dp0_lr001/checkpoints/GMVAE_1_00001_25_25_10_50_3_03_0001/step_60.pt\".\n",
      "====================================================================================================\n",
      "\n",
      "(Epoch 61 / 400)\n",
      "Train - Log-lik: -34634.98451;  CP: 90.73619;  KL_w: 0.00001;  KL_y: 1.55830;\n",
      "Valid - Log-lik: -25880.26924;  CP: 89.04828;  KL_w: 0.00001;  KL_y: 1.65070;\n",
      "Total Loss=Train: 34727.27895; Val: 25970.96822\n",
      "(Epoch 62 / 400)\n",
      "Train - Log-lik: -34536.11969;  CP: 91.26985;  KL_w: 0.00001;  KL_y: 1.56047;\n",
      "Valid - Log-lik: -26205.86840;  CP: 90.84015;  KL_w: 0.00001;  KL_y: 1.63485;\n",
      "Total Loss=Train: 34628.95005; Val: 26298.34338\n",
      "(Epoch 63 / 400)\n",
      "Train - Log-lik: -34638.59215;  CP: 90.92499;  KL_w: 0.00001;  KL_y: 1.55266;\n",
      "Valid - Log-lik: -25911.97912;  CP: 89.67765;  KL_w: 0.00001;  KL_y: 1.62869;\n",
      "Total Loss=Train: 34731.06983; Val: 26003.28545\n",
      "(Epoch 64 / 400)\n",
      "Train - Log-lik: -34532.65947;  CP: 90.79869;  KL_w: 0.00001;  KL_y: 1.55619;\n",
      "Valid - Log-lik: -25798.18753;  CP: 90.24474;  KL_w: 0.00001;  KL_y: 1.62335;\n",
      "Total Loss=Train: 34625.01427; Val: 25890.05560\n",
      "(Epoch 65 / 400)\n",
      "Train - Log-lik: -34361.80601;  CP: 90.97853;  KL_w: 0.00001;  KL_y: 1.54382;\n",
      "Valid - Log-lik: -25749.58665;  CP: 89.56003;  KL_w: 0.00000;  KL_y: 1.62651;\n",
      "Total Loss=Train: 34454.32840; Val: 25840.77321\n",
      "(Epoch 66 / 400)\n",
      "Train - Log-lik: -34261.58228;  CP: 90.36220;  KL_w: 0.00001;  KL_y: 1.54632;\n",
      "Valid - Log-lik: -26229.09723;  CP: 88.46855;  KL_w: 0.00000;  KL_y: 1.60182;\n",
      "Total Loss=Train: 34353.49070; Val: 26319.16761\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 67 / 400)\n",
      "Train - Log-lik: -34450.90625;  CP: 90.43086;  KL_w: 0.00001;  KL_y: 1.54177;\n",
      "Valid - Log-lik: -25703.45790;  CP: 90.33274;  KL_w: 0.00001;  KL_y: 1.64634;\n",
      "Total Loss=Train: 34542.87886; Val: 25795.43702\n",
      "(Epoch 68 / 400)\n",
      "Train - Log-lik: -34345.83070;  CP: 91.29616;  KL_w: 0.00001;  KL_y: 1.54001;\n",
      "Valid - Log-lik: -25751.61899;  CP: 86.93736;  KL_w: 0.00001;  KL_y: 1.60459;\n",
      "Total Loss=Train: 34438.66691; Val: 25840.16097\n",
      "(Epoch 69 / 400)\n",
      "Train - Log-lik: -34224.37874;  CP: 91.04556;  KL_w: 0.00001;  KL_y: 1.54104;\n",
      "Valid - Log-lik: -25543.14645;  CP: 90.25777;  KL_w: 0.00000;  KL_y: 1.62137;\n",
      "Total Loss=Train: 34316.96550; Val: 25635.02561\n",
      "(Epoch 70 / 400)\n",
      "Train - Log-lik: -34241.10112;  CP: 91.85769;  KL_w: 0.00001;  KL_y: 1.54102;\n",
      "Valid - Log-lik: -25714.44785;  CP: 90.60982;  KL_w: 0.00001;  KL_y: 1.62052;\n",
      "Total Loss=Train: 34334.49990; Val: 25806.67820\n",
      "====================================================================================================\n",
      "Saving checkpoint to \"experiments/FMNIST_200epochs_dp0_lr001/checkpoints/GMVAE_1_00001_25_25_10_50_3_03_0001/step_70.pt\".\n",
      "====================================================================================================\n",
      "\n",
      "(Epoch 71 / 400)\n",
      "Train - Log-lik: -34170.38333;  CP: 91.61830;  KL_w: 0.00001;  KL_y: 1.53799;\n",
      "Valid - Log-lik: -25578.20651;  CP: 91.76177;  KL_w: 0.00000;  KL_y: 1.59499;\n",
      "Total Loss=Train: 34263.53964; Val: 25671.56324\n",
      "(Epoch 72 / 400)\n",
      "Train - Log-lik: -34133.00552;  CP: 91.59837;  KL_w: 0.00001;  KL_y: 1.53046;\n",
      "Valid - Log-lik: -25631.53922;  CP: 89.88934;  KL_w: 0.00001;  KL_y: 1.61145;\n",
      "Total Loss=Train: 34226.13424; Val: 25723.04004\n",
      "(Epoch 73 / 400)\n",
      "Train - Log-lik: -34196.40581;  CP: 91.03938;  KL_w: 0.00001;  KL_y: 1.53281;\n",
      "Valid - Log-lik: -25796.49506;  CP: 91.24811;  KL_w: 0.00001;  KL_y: 1.65219;\n",
      "Total Loss=Train: 34288.97802; Val: 25889.39532\n",
      "(Epoch 74 / 400)\n",
      "Train - Log-lik: -34038.13234;  CP: 91.90129;  KL_w: 0.00001;  KL_y: 1.54193;\n",
      "Valid - Log-lik: -25434.85343;  CP: 89.64654;  KL_w: 0.00000;  KL_y: 1.67185;\n",
      "Total Loss=Train: 34131.57557; Val: 25526.17184\n",
      "(Epoch 75 / 400)\n",
      "Train - Log-lik: -34126.24765;  CP: 91.16793;  KL_w: 0.00001;  KL_y: 1.53250;\n",
      "Valid - Log-lik: -25667.39429;  CP: 90.95428;  KL_w: 0.00000;  KL_y: 1.61408;\n",
      "Total Loss=Train: 34218.94779; Val: 25759.96265\n",
      "(Epoch 76 / 400)\n",
      "Train - Log-lik: -33976.36931;  CP: 93.06443;  KL_w: 0.00001;  KL_y: 1.53627;\n",
      "Valid - Log-lik: -25596.44540;  CP: 91.10002;  KL_w: 0.00001;  KL_y: 1.61556;\n",
      "Total Loss=Train: 34070.97013; Val: 25689.16102\n",
      "(Epoch 77 / 400)\n",
      "Train - Log-lik: -33982.33315;  CP: 92.02514;  KL_w: 0.00001;  KL_y: 1.54277;\n",
      "Valid - Log-lik: -25571.16661;  CP: 92.14047;  KL_w: 0.00000;  KL_y: 1.63770;\n",
      "Total Loss=Train: 34075.90120; Val: 25664.94479\n",
      "(Epoch 78 / 400)\n",
      "Train - Log-lik: -33910.99545;  CP: 92.60456;  KL_w: 0.00001;  KL_y: 1.53956;\n",
      "Valid - Log-lik: -25236.28002;  CP: 90.00254;  KL_w: 0.00001;  KL_y: 1.62835;\n",
      "Total Loss=Train: 34005.13962; Val: 25327.91094\n",
      "(Epoch 79 / 400)\n",
      "Train - Log-lik: -33908.78023;  CP: 92.39141;  KL_w: 0.00001;  KL_y: 1.53221;\n",
      "Valid - Log-lik: -25580.92083;  CP: 91.55748;  KL_w: 0.00000;  KL_y: 1.65820;\n",
      "Total Loss=Train: 34002.70382; Val: 25674.13648\n",
      "(Epoch 80 / 400)\n",
      "Train - Log-lik: -33784.60648;  CP: 92.67986;  KL_w: 0.00001;  KL_y: 1.54115;\n",
      "Valid - Log-lik: -25193.82121;  CP: 91.87674;  KL_w: 0.00000;  KL_y: 1.58602;\n",
      "Total Loss=Train: 33878.82747; Val: 25287.28401\n",
      "====================================================================================================\n",
      "Saving checkpoint to \"experiments/FMNIST_200epochs_dp0_lr001/checkpoints/GMVAE_1_00001_25_25_10_50_3_03_0001/step_80.pt\".\n",
      "====================================================================================================\n",
      "\n",
      "(Epoch 81 / 400)\n",
      "Train - Log-lik: -33736.74192;  CP: 90.97202;  KL_w: 0.00001;  KL_y: 1.53523;\n",
      "Valid - Log-lik: -25189.82835;  CP: 90.83606;  KL_w: 0.00000;  KL_y: 1.62913;\n",
      "Total Loss=Train: 33829.24922; Val: 25282.29356\n",
      "(Epoch 82 / 400)\n",
      "Train - Log-lik: -33790.64945;  CP: 92.22391;  KL_w: 0.00001;  KL_y: 1.53321;\n",
      "Valid - Log-lik: -25098.34125;  CP: 90.82712;  KL_w: 0.00001;  KL_y: 1.61229;\n",
      "Total Loss=Train: 33884.40657; Val: 25190.78063\n",
      "(Epoch 83 / 400)\n",
      "Train - Log-lik: -33821.51544;  CP: 91.82693;  KL_w: 0.00001;  KL_y: 1.53295;\n",
      "Valid - Log-lik: -25556.90077;  CP: 90.99316;  KL_w: 0.00000;  KL_y: 1.64189;\n",
      "Total Loss=Train: 33914.87529; Val: 25649.53580\n",
      "(Epoch 84 / 400)\n",
      "Train - Log-lik: -33757.18973;  CP: 91.17947;  KL_w: 0.00000;  KL_y: 1.51846;\n",
      "Valid - Log-lik: -25518.99873;  CP: 90.27682;  KL_w: 0.00001;  KL_y: 1.60873;\n",
      "Total Loss=Train: 33849.88758; Val: 25610.88426\n",
      "(Epoch 85 / 400)\n",
      "Train - Log-lik: -33809.00338;  CP: 92.24744;  KL_w: 0.00001;  KL_y: 1.52846;\n",
      "Valid - Log-lik: -25512.08505;  CP: 91.68541;  KL_w: 0.00001;  KL_y: 1.61655;\n",
      "Total Loss=Train: 33902.77929; Val: 25605.38701\n",
      "(Epoch 86 / 400)\n",
      "Train - Log-lik: -33789.23763;  CP: 92.64512;  KL_w: 0.00001;  KL_y: 1.53407;\n",
      "Valid - Log-lik: -25090.93838;  CP: 91.95672;  KL_w: 0.00000;  KL_y: 1.64313;\n",
      "Total Loss=Train: 33883.41669; Val: 25184.53814\n",
      "(Epoch 87 / 400)\n",
      "Train - Log-lik: -33564.75897;  CP: 93.61461;  KL_w: 0.00001;  KL_y: 1.52555;\n",
      "Valid - Log-lik: -25281.00109;  CP: 93.00523;  KL_w: 0.00001;  KL_y: 1.64758;\n",
      "Total Loss=Train: 33659.89923; Val: 25375.65385\n",
      "(Epoch 88 / 400)\n",
      "Train - Log-lik: -33675.39274;  CP: 94.29149;  KL_w: 0.00001;  KL_y: 1.53038;\n",
      "Valid - Log-lik: -25119.27257;  CP: 91.23085;  KL_w: 0.00001;  KL_y: 1.57159;\n",
      "Total Loss=Train: 33771.21470; Val: 25212.07502\n",
      "(Epoch 89 / 400)\n",
      "Train - Log-lik: -33696.47511;  CP: 92.27553;  KL_w: 0.00001;  KL_y: 1.53272;\n",
      "Valid - Log-lik: -25642.86500;  CP: 92.06619;  KL_w: 0.00001;  KL_y: 1.60125;\n",
      "Total Loss=Train: 33790.28338; Val: 25736.53248\n",
      "(Epoch 90 / 400)\n",
      "Train - Log-lik: -33694.87843;  CP: 93.73463;  KL_w: 0.00001;  KL_y: 1.52971;\n",
      "Valid - Log-lik: -25376.12007;  CP: 92.93757;  KL_w: 0.00000;  KL_y: 1.61821;\n",
      "Total Loss=Train: 33790.14255; Val: 25470.67586\n",
      "====================================================================================================\n",
      "Saving checkpoint to \"experiments/FMNIST_200epochs_dp0_lr001/checkpoints/GMVAE_1_00001_25_25_10_50_3_03_0001/step_90.pt\".\n",
      "====================================================================================================\n",
      "\n",
      "(Epoch 91 / 400)\n",
      "Train - Log-lik: -33573.48787;  CP: 94.13778;  KL_w: 0.00001;  KL_y: 1.52327;\n",
      "Valid - Log-lik: -25312.94573;  CP: 92.94852;  KL_w: 0.00000;  KL_y: 1.59837;\n",
      "Total Loss=Train: 33669.14892; Val: 25407.49265\n",
      "(Epoch 92 / 400)\n",
      "Train - Log-lik: -33675.42263;  CP: 94.09170;  KL_w: 0.00001;  KL_y: 1.52617;\n",
      "Valid - Log-lik: -25212.37670;  CP: 91.76101;  KL_w: 0.00001;  KL_y: 1.63969;\n",
      "Total Loss=Train: 33771.04050; Val: 25305.77741\n",
      "(Epoch 93 / 400)\n",
      "Train - Log-lik: -33612.50052;  CP: 92.50332;  KL_w: 0.00001;  KL_y: 1.52382;\n",
      "Valid - Log-lik: -25420.57874;  CP: 91.98417;  KL_w: 0.00000;  KL_y: 1.63001;\n",
      "Total Loss=Train: 33706.52760; Val: 25514.19295\n",
      "(Epoch 94 / 400)\n",
      "Train - Log-lik: -33627.43039;  CP: 93.70468;  KL_w: 0.00001;  KL_y: 1.51350;\n",
      "Valid - Log-lik: -25149.83898;  CP: 91.76396;  KL_w: 0.00000;  KL_y: 1.58702;\n",
      "Total Loss=Train: 33722.64855; Val: 25243.18995\n",
      "(Epoch 95 / 400)\n",
      "Train - Log-lik: -33596.20410;  CP: 94.11186;  KL_w: 0.00001;  KL_y: 1.52164;\n",
      "Valid - Log-lik: -25458.65002;  CP: 92.87941;  KL_w: 0.00000;  KL_y: 1.61507;\n",
      "Total Loss=Train: 33691.83760; Val: 25553.14451\n",
      "(Epoch 96 / 400)\n",
      "Train - Log-lik: -33491.96678;  CP: 93.80213;  KL_w: 0.00001;  KL_y: 1.51370;\n",
      "Valid - Log-lik: -25110.42047;  CP: 93.75132;  KL_w: 0.00001;  KL_y: 1.60787;\n",
      "Total Loss=Train: 33587.28259; Val: 25205.77968\n",
      "(Epoch 97 / 400)\n",
      "Train - Log-lik: -33651.00339;  CP: 93.59490;  KL_w: 0.00001;  KL_y: 1.52481;\n",
      "Valid - Log-lik: -25540.24853;  CP: 91.02696;  KL_w: 0.00001;  KL_y: 1.62977;\n",
      "Total Loss=Train: 33746.12310; Val: 25632.90530\n",
      "(Epoch 98 / 400)\n",
      "Train - Log-lik: -33428.85242;  CP: 92.73185;  KL_w: 0.00001;  KL_y: 1.51314;\n",
      "Valid - Log-lik: -24880.50556;  CP: 91.56824;  KL_w: 0.00000;  KL_y: 1.62237;\n",
      "Total Loss=Train: 33523.09727; Val: 24973.69611\n",
      "(Epoch 99 / 400)\n",
      "Train - Log-lik: -33370.45434;  CP: 91.77963;  KL_w: 0.00001;  KL_y: 1.51101;\n",
      "Valid - Log-lik: -25165.70122;  CP: 92.34772;  KL_w: 0.00000;  KL_y: 1.58643;\n",
      "Total Loss=Train: 33463.74502; Val: 25259.63541\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 100 / 400)\n",
      "Train - Log-lik: -33497.78265;  CP: 93.55436;  KL_w: 0.00001;  KL_y: 1.50716;\n",
      "Valid - Log-lik: -24961.77035;  CP: 92.96707;  KL_w: 0.00001;  KL_y: 1.60655;\n",
      "Total Loss=Train: 33592.84423; Val: 25056.34396\n",
      "====================================================================================================\n",
      "Saving checkpoint to \"experiments/FMNIST_200epochs_dp0_lr001/checkpoints/GMVAE_1_00001_25_25_10_50_3_03_0001/step_100.pt\".\n",
      "====================================================================================================\n",
      "\n",
      "(Epoch 101 / 400)\n",
      "Train - Log-lik: -33384.50774;  CP: 93.01603;  KL_w: 0.00001;  KL_y: 1.50123;\n",
      "Valid - Log-lik: -25039.90318;  CP: 91.65249;  KL_w: 0.00001;  KL_y: 1.59144;\n",
      "Total Loss=Train: 33479.02501; Val: 25133.14706\n",
      "(Epoch 102 / 400)\n",
      "Train - Log-lik: -33402.39265;  CP: 92.71432;  KL_w: 0.00001;  KL_y: 1.50201;\n",
      "Valid - Log-lik: -25089.37771;  CP: 91.63320;  KL_w: 0.00001;  KL_y: 1.58979;\n",
      "Total Loss=Train: 33496.60885; Val: 25182.60069\n",
      "(Epoch 103 / 400)\n",
      "Train - Log-lik: -33433.01549;  CP: 93.08513;  KL_w: 0.00001;  KL_y: 1.50720;\n",
      "Valid - Log-lik: -25201.60916;  CP: 92.07523;  KL_w: 0.00001;  KL_y: 1.58142;\n",
      "Total Loss=Train: 33527.60780; Val: 25295.26579\n",
      "(Epoch 104 / 400)\n",
      "Train - Log-lik: -33570.78009;  CP: 94.27688;  KL_w: 0.00001;  KL_y: 1.50293;\n",
      "Valid - Log-lik: -24965.48297;  CP: 93.99777;  KL_w: 0.00000;  KL_y: 1.58129;\n",
      "Total Loss=Train: 33666.55981; Val: 25061.06198\n",
      "(Epoch 105 / 400)\n",
      "Train - Log-lik: -33366.74590;  CP: 94.23315;  KL_w: 0.00001;  KL_y: 1.49929;\n",
      "Valid - Log-lik: -25441.93121;  CP: 92.05519;  KL_w: 0.00000;  KL_y: 1.58666;\n",
      "Total Loss=Train: 33462.47826; Val: 25535.57307\n",
      "(Epoch 106 / 400)\n",
      "Train - Log-lik: -33258.90349;  CP: 93.76611;  KL_w: 0.00000;  KL_y: 1.49518;\n",
      "Valid - Log-lik: -25098.10679;  CP: 90.18294;  KL_w: 0.00001;  KL_y: 1.61128;\n",
      "Total Loss=Train: 33354.16471; Val: 25189.90103\n",
      "(Epoch 107 / 400)\n",
      "Train - Log-lik: -33385.16152;  CP: 92.91352;  KL_w: 0.00001;  KL_y: 1.49394;\n",
      "Valid - Log-lik: -25379.15053;  CP: 91.59197;  KL_w: 0.00000;  KL_y: 1.57847;\n",
      "Total Loss=Train: 33479.56894; Val: 25472.32097\n",
      "(Epoch 108 / 400)\n",
      "Train - Log-lik: -33418.80509;  CP: 92.20667;  KL_w: 0.00000;  KL_y: 1.49857;\n",
      "Valid - Log-lik: -25382.85353;  CP: 92.64978;  KL_w: 0.00001;  KL_y: 1.58810;\n",
      "Total Loss=Train: 33512.51024; Val: 25477.09139\n",
      "(Epoch 109 / 400)\n",
      "Train - Log-lik: -33444.20688;  CP: 94.44148;  KL_w: 0.00000;  KL_y: 1.49579;\n",
      "Valid - Log-lik: -25271.81785;  CP: 92.63767;  KL_w: 0.00000;  KL_y: 1.55349;\n",
      "Total Loss=Train: 33540.14423; Val: 25366.00904\n",
      "(Epoch 110 / 400)\n",
      "Train - Log-lik: -33316.57135;  CP: 93.57143;  KL_w: 0.00000;  KL_y: 1.49990;\n",
      "Valid - Log-lik: -25184.11070;  CP: 92.41586;  KL_w: 0.00000;  KL_y: 1.53916;\n",
      "Total Loss=Train: 33411.64263; Val: 25278.06568\n",
      "====================================================================================================\n",
      "Saving checkpoint to \"experiments/FMNIST_200epochs_dp0_lr001/checkpoints/GMVAE_1_00001_25_25_10_50_3_03_0001/step_110.pt\".\n",
      "====================================================================================================\n",
      "\n",
      "(Epoch 111 / 400)\n",
      "Train - Log-lik: -33287.40894;  CP: 93.47850;  KL_w: 0.00000;  KL_y: 1.49600;\n",
      "Valid - Log-lik: -25040.66965;  CP: 91.08414;  KL_w: 0.00000;  KL_y: 1.56077;\n",
      "Total Loss=Train: 33382.38344; Val: 25133.31458\n",
      "(Epoch 112 / 400)\n",
      "Train - Log-lik: -33290.93053;  CP: 92.60775;  KL_w: 0.00001;  KL_y: 1.49539;\n",
      "Valid - Log-lik: -24761.67495;  CP: 90.53375;  KL_w: 0.00000;  KL_y: 1.57659;\n",
      "Total Loss=Train: 33385.03361; Val: 24853.78525\n",
      "(Epoch 113 / 400)\n",
      "Train - Log-lik: -33317.08404;  CP: 93.15567;  KL_w: 0.00001;  KL_y: 1.48662;\n",
      "Valid - Log-lik: -24912.65234;  CP: 92.06451;  KL_w: 0.00000;  KL_y: 1.57445;\n",
      "Total Loss=Train: 33411.72624; Val: 25006.29130\n",
      "(Epoch 114 / 400)\n",
      "Train - Log-lik: -33407.67826;  CP: 95.05504;  KL_w: 0.00001;  KL_y: 1.50203;\n",
      "Valid - Log-lik: -25163.11731;  CP: 93.76613;  KL_w: 0.00001;  KL_y: 1.56175;\n",
      "Total Loss=Train: 33504.23551; Val: 25258.44522\n",
      "(Epoch 115 / 400)\n",
      "Train - Log-lik: -33307.60172;  CP: 93.67067;  KL_w: 0.00001;  KL_y: 1.49511;\n",
      "Valid - Log-lik: -24880.49436;  CP: 90.80513;  KL_w: 0.00001;  KL_y: 1.58166;\n",
      "Total Loss=Train: 33402.76765; Val: 24972.88121\n",
      "(Epoch 116 / 400)\n",
      "Train - Log-lik: -33458.86483;  CP: 92.58803;  KL_w: 0.00001;  KL_y: 1.48817;\n",
      "Valid - Log-lik: -25026.29617;  CP: 90.77708;  KL_w: 0.00000;  KL_y: 1.53498;\n",
      "Total Loss=Train: 33552.94103; Val: 25118.60825\n",
      "(Epoch 117 / 400)\n",
      "Train - Log-lik: -33281.22403;  CP: 93.12237;  KL_w: 0.00001;  KL_y: 1.48155;\n",
      "Valid - Log-lik: -25132.15386;  CP: 92.63511;  KL_w: 0.00001;  KL_y: 1.59664;\n",
      "Total Loss=Train: 33375.82797; Val: 25226.38560\n",
      "(Epoch 118 / 400)\n",
      "Train - Log-lik: -33265.45177;  CP: 94.65130;  KL_w: 0.00001;  KL_y: 1.48926;\n",
      "Valid - Log-lik: -24822.90013;  CP: 93.54072;  KL_w: 0.00000;  KL_y: 1.53683;\n",
      "Total Loss=Train: 33361.59247; Val: 24917.97767\n",
      "(Epoch 119 / 400)\n",
      "Train - Log-lik: -33295.79021;  CP: 94.13323;  KL_w: 0.00000;  KL_y: 1.47804;\n",
      "Valid - Log-lik: -25030.08083;  CP: 93.11276;  KL_w: 0.00000;  KL_y: 1.57169;\n",
      "Total Loss=Train: 33391.40154; Val: 25124.76527\n",
      "(Epoch 120 / 400)\n",
      "Train - Log-lik: -33164.18235;  CP: 93.99754;  KL_w: 0.00001;  KL_y: 1.48018;\n",
      "Valid - Log-lik: -25186.18002;  CP: 92.11324;  KL_w: 0.00000;  KL_y: 1.57858;\n",
      "Total Loss=Train: 33259.65994; Val: 25279.87187\n",
      "====================================================================================================\n",
      "Saving checkpoint to \"experiments/FMNIST_200epochs_dp0_lr001/checkpoints/GMVAE_1_00001_25_25_10_50_3_03_0001/step_120.pt\".\n",
      "====================================================================================================\n",
      "\n",
      "(Epoch 121 / 400)\n",
      "Train - Log-lik: -33141.18705;  CP: 94.22273;  KL_w: 0.00000;  KL_y: 1.48308;\n",
      "Valid - Log-lik: -25188.34797;  CP: 93.31017;  KL_w: 0.00000;  KL_y: 1.52895;\n",
      "Total Loss=Train: 33236.89302; Val: 25283.18708\n",
      "(Epoch 122 / 400)\n",
      "Train - Log-lik: -33276.21018;  CP: 94.25460;  KL_w: 0.00001;  KL_y: 1.47534;\n",
      "Valid - Log-lik: -25433.54644;  CP: 92.83562;  KL_w: 0.00001;  KL_y: 1.54705;\n",
      "Total Loss=Train: 33371.94006; Val: 25527.92914\n",
      "(Epoch 123 / 400)\n",
      "Train - Log-lik: -33238.26520;  CP: 93.11564;  KL_w: 0.00001;  KL_y: 1.47141;\n",
      "Valid - Log-lik: -24906.17632;  CP: 90.64493;  KL_w: 0.00001;  KL_y: 1.54027;\n",
      "Total Loss=Train: 33332.85231; Val: 24998.36153\n",
      "(Epoch 124 / 400)\n",
      "Train - Log-lik: -33074.98834;  CP: 92.73904;  KL_w: 0.00000;  KL_y: 1.47779;\n",
      "Valid - Log-lik: -24540.41268;  CP: 92.00887;  KL_w: 0.00001;  KL_y: 1.52951;\n",
      "Total Loss=Train: 33169.20518; Val: 24633.95110\n",
      "(Epoch 125 / 400)\n",
      "Train - Log-lik: -33074.94561;  CP: 95.26924;  KL_w: 0.00000;  KL_y: 1.47994;\n",
      "Valid - Log-lik: -24718.08973;  CP: 93.49446;  KL_w: 0.00001;  KL_y: 1.54038;\n",
      "Total Loss=Train: 33171.69473; Val: 24813.12462\n",
      "(Epoch 126 / 400)\n",
      "Train - Log-lik: -33157.12505;  CP: 92.92604;  KL_w: 0.00001;  KL_y: 1.47520;\n",
      "Valid - Log-lik: -24927.88449;  CP: 92.94489;  KL_w: 0.00001;  KL_y: 1.57524;\n",
      "Total Loss=Train: 33251.52638; Val: 25022.40459\n",
      "(Epoch 127 / 400)\n",
      "Train - Log-lik: -33145.11664;  CP: 93.84767;  KL_w: 0.00001;  KL_y: 1.47216;\n",
      "Valid - Log-lik: -24889.00014;  CP: 91.96417;  KL_w: 0.00000;  KL_y: 1.54462;\n",
      "Total Loss=Train: 33240.43635; Val: 24982.50891\n",
      "(Epoch 128 / 400)\n",
      "Train - Log-lik: -33159.61120;  CP: 94.43854;  KL_w: 0.00001;  KL_y: 1.47991;\n",
      "Valid - Log-lik: -24768.98180;  CP: 92.27239;  KL_w: 0.00000;  KL_y: 1.57122;\n",
      "Total Loss=Train: 33255.52958; Val: 24862.82541\n",
      "(Epoch 129 / 400)\n",
      "Train - Log-lik: -33128.23917;  CP: 93.88576;  KL_w: 0.00000;  KL_y: 1.48894;\n",
      "Valid - Log-lik: -24897.76503;  CP: 93.47361;  KL_w: 0.00000;  KL_y: 1.53618;\n",
      "Total Loss=Train: 33223.61371; Val: 24992.77479\n",
      "(Epoch 130 / 400)\n",
      "Train - Log-lik: -32946.17469;  CP: 93.13472;  KL_w: 0.00000;  KL_y: 1.47621;\n",
      "Valid - Log-lik: -24904.50285;  CP: 92.85032;  KL_w: 0.00000;  KL_y: 1.54546;\n",
      "Total Loss=Train: 33040.78551; Val: 24998.89863\n",
      "====================================================================================================\n",
      "Saving checkpoint to \"experiments/FMNIST_200epochs_dp0_lr001/checkpoints/GMVAE_1_00001_25_25_10_50_3_03_0001/step_130.pt\".\n",
      "====================================================================================================\n",
      "\n",
      "(Epoch 131 / 400)\n",
      "Train - Log-lik: -33159.57105;  CP: 93.68376;  KL_w: 0.00001;  KL_y: 1.46706;\n",
      "Valid - Log-lik: -25045.34241;  CP: 91.62107;  KL_w: 0.00001;  KL_y: 1.55210;\n",
      "Total Loss=Train: 33254.72173; Val: 25138.51559\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 132 / 400)\n",
      "Train - Log-lik: -33021.25396;  CP: 93.02869;  KL_w: 0.00001;  KL_y: 1.47806;\n",
      "Valid - Log-lik: -25205.62085;  CP: 92.58450;  KL_w: 0.00000;  KL_y: 1.55342;\n",
      "Total Loss=Train: 33115.76081; Val: 25299.75878\n",
      "(Epoch 133 / 400)\n",
      "Train - Log-lik: -33116.86527;  CP: 93.09646;  KL_w: 0.00000;  KL_y: 1.47627;\n",
      "Valid - Log-lik: -25285.88113;  CP: 93.49774;  KL_w: 0.00000;  KL_y: 1.59318;\n",
      "Total Loss=Train: 33211.43793; Val: 25380.97206\n",
      "(Epoch 134 / 400)\n",
      "Train - Log-lik: -33112.59377;  CP: 94.65851;  KL_w: 0.00000;  KL_y: 1.48564;\n",
      "Valid - Log-lik: -24652.85361;  CP: 93.47083;  KL_w: 0.00000;  KL_y: 1.57299;\n",
      "Total Loss=Train: 33208.73798; Val: 24747.89746\n",
      "(Epoch 135 / 400)\n",
      "Train - Log-lik: -33168.32809;  CP: 94.11413;  KL_w: 0.00001;  KL_y: 1.49264;\n",
      "Valid - Log-lik: -24761.27516;  CP: 93.14023;  KL_w: 0.00000;  KL_y: 1.58075;\n",
      "Total Loss=Train: 33263.93492; Val: 24855.99611\n",
      "(Epoch 136 / 400)\n",
      "Train - Log-lik: -33143.99752;  CP: 94.14265;  KL_w: 0.00000;  KL_y: 1.48774;\n",
      "Valid - Log-lik: -24845.47364;  CP: 92.40729;  KL_w: 0.00000;  KL_y: 1.56029;\n",
      "Total Loss=Train: 33239.62775; Val: 24939.44121\n",
      "(Epoch 137 / 400)\n",
      "Train - Log-lik: -32999.14722;  CP: 92.65286;  KL_w: 0.00000;  KL_y: 1.47152;\n",
      "Valid - Log-lik: -25329.99527;  CP: 92.24515;  KL_w: 0.00000;  KL_y: 1.55588;\n",
      "Total Loss=Train: 33093.27155; Val: 25423.79632\n",
      "(Epoch 138 / 400)\n",
      "Train - Log-lik: -33139.79138;  CP: 93.06197;  KL_w: 0.00000;  KL_y: 1.48437;\n",
      "Valid - Log-lik: -25118.57870;  CP: 92.23227;  KL_w: 0.00001;  KL_y: 1.61068;\n",
      "Total Loss=Train: 33234.33762; Val: 25212.42167\n",
      "(Epoch 139 / 400)\n",
      "Train - Log-lik: -33034.32623;  CP: 93.43639;  KL_w: 0.00001;  KL_y: 1.48906;\n",
      "Valid - Log-lik: -24662.74525;  CP: 91.67356;  KL_w: 0.00001;  KL_y: 1.57491;\n",
      "Total Loss=Train: 33129.25155; Val: 24755.99371\n",
      "(Epoch 140 / 400)\n",
      "Train - Log-lik: -32948.67282;  CP: 92.78528;  KL_w: 0.00001;  KL_y: 1.49018;\n",
      "Valid - Log-lik: -24981.46880;  CP: 91.69360;  KL_w: 0.00001;  KL_y: 1.57960;\n",
      "Total Loss=Train: 33042.94818; Val: 25074.74199\n",
      "====================================================================================================\n",
      "Saving checkpoint to \"experiments/FMNIST_200epochs_dp0_lr001/checkpoints/GMVAE_1_00001_25_25_10_50_3_03_0001/step_140.pt\".\n",
      "====================================================================================================\n",
      "\n",
      "(Epoch 141 / 400)\n",
      "Train - Log-lik: -33001.04503;  CP: 93.20862;  KL_w: 0.00001;  KL_y: 1.48952;\n",
      "Valid - Log-lik: -24833.97904;  CP: 92.77800;  KL_w: 0.00000;  KL_y: 1.60001;\n",
      "Total Loss=Train: 33095.74317; Val: 24928.35699\n",
      "(Epoch 142 / 400)\n",
      "Train - Log-lik: -33005.61719;  CP: 92.96234;  KL_w: 0.00000;  KL_y: 1.49994;\n",
      "Valid - Log-lik: -24763.77007;  CP: 91.97382;  KL_w: 0.00001;  KL_y: 1.61458;\n",
      "Total Loss=Train: 33100.07940; Val: 24857.35847\n",
      "(Epoch 143 / 400)\n",
      "Train - Log-lik: -32899.40751;  CP: 93.76848;  KL_w: 0.00001;  KL_y: 1.50452;\n",
      "Valid - Log-lik: -24964.07499;  CP: 92.54257;  KL_w: 0.00000;  KL_y: 1.57022;\n",
      "Total Loss=Train: 32994.68067; Val: 25058.18775\n",
      "(Epoch 144 / 400)\n",
      "Train - Log-lik: -32995.88833;  CP: 93.46829;  KL_w: 0.00000;  KL_y: 1.49692;\n",
      "Valid - Log-lik: -24930.29319;  CP: 93.00618;  KL_w: 0.00000;  KL_y: 1.57049;\n",
      "Total Loss=Train: 33090.85348; Val: 25024.86984\n",
      "(Epoch 145 / 400)\n",
      "Train - Log-lik: -32947.18752;  CP: 93.54782;  KL_w: 0.00001;  KL_y: 1.50616;\n",
      "Valid - Log-lik: -25109.52979;  CP: 93.22373;  KL_w: 0.00001;  KL_y: 1.60243;\n",
      "Total Loss=Train: 33042.24144; Val: 25204.35597\n",
      "(Epoch 146 / 400)\n",
      "Train - Log-lik: -32843.81797;  CP: 92.74425;  KL_w: 0.00001;  KL_y: 1.49993;\n",
      "Valid - Log-lik: -24623.73789;  CP: 90.51005;  KL_w: 0.00001;  KL_y: 1.63070;\n",
      "Total Loss=Train: 32938.06208; Val: 24715.87860\n",
      "(Epoch 147 / 400)\n",
      "Train - Log-lik: -32909.77710;  CP: 92.53087;  KL_w: 0.00001;  KL_y: 1.51652;\n",
      "Valid - Log-lik: -24751.74801;  CP: 93.07463;  KL_w: 0.00001;  KL_y: 1.62923;\n",
      "Total Loss=Train: 33003.82448; Val: 24846.45183\n",
      "(Epoch 148 / 400)\n",
      "Train - Log-lik: -32986.21662;  CP: 93.66031;  KL_w: 0.00001;  KL_y: 1.50019;\n",
      "Valid - Log-lik: -24767.06223;  CP: 92.13541;  KL_w: 0.00001;  KL_y: 1.58652;\n",
      "Total Loss=Train: 33081.37711; Val: 24860.78413\n",
      "(Epoch 149 / 400)\n",
      "Train - Log-lik: -32997.95535;  CP: 93.02933;  KL_w: 0.00001;  KL_y: 1.50287;\n",
      "Valid - Log-lik: -24733.22499;  CP: 93.08741;  KL_w: 0.00000;  KL_y: 1.57139;\n",
      "Total Loss=Train: 33092.48767; Val: 24827.88381\n",
      "(Epoch 150 / 400)\n",
      "Train - Log-lik: -32854.00989;  CP: 93.25615;  KL_w: 0.00001;  KL_y: 1.48998;\n",
      "Valid - Log-lik: -25319.37850;  CP: 93.47661;  KL_w: 0.00001;  KL_y: 1.55965;\n",
      "Total Loss=Train: 32948.75598; Val: 25414.41471\n",
      "====================================================================================================\n",
      "Saving checkpoint to \"experiments/FMNIST_200epochs_dp0_lr001/checkpoints/GMVAE_1_00001_25_25_10_50_3_03_0001/step_150.pt\".\n",
      "====================================================================================================\n",
      "\n",
      "(Epoch 151 / 400)\n",
      "Train - Log-lik: -32948.39563;  CP: 92.82028;  KL_w: 0.00000;  KL_y: 1.49614;\n",
      "Valid - Log-lik: -24809.74874;  CP: 92.05296;  KL_w: 0.00001;  KL_y: 1.56270;\n",
      "Total Loss=Train: 33042.71216; Val: 24903.36444\n",
      "(Epoch 152 / 400)\n",
      "Train - Log-lik: -33034.61257;  CP: 94.01983;  KL_w: 0.00000;  KL_y: 1.50019;\n",
      "Valid - Log-lik: -24818.63646;  CP: 92.56610;  KL_w: 0.00001;  KL_y: 1.57191;\n",
      "Total Loss=Train: 33130.13247; Val: 24912.77446\n",
      "(Epoch 153 / 400)\n",
      "Train - Log-lik: -33020.54773;  CP: 92.97243;  KL_w: 0.00001;  KL_y: 1.49384;\n",
      "Valid - Log-lik: -24967.73441;  CP: 92.39945;  KL_w: 0.00000;  KL_y: 1.55878;\n",
      "Total Loss=Train: 33115.01395; Val: 25061.69268\n",
      "(Epoch 154 / 400)\n",
      "Train - Log-lik: -32906.40594;  CP: 93.50504;  KL_w: 0.00000;  KL_y: 1.49842;\n",
      "Valid - Log-lik: -25201.65209;  CP: 92.10006;  KL_w: 0.00000;  KL_y: 1.61850;\n",
      "Total Loss=Train: 33001.40931; Val: 25295.37066\n",
      "(Epoch 155 / 400)\n",
      "Train - Log-lik: -32929.79146;  CP: 93.74047;  KL_w: 0.00001;  KL_y: 1.51210;\n",
      "Valid - Log-lik: -24965.56493;  CP: 93.43737;  KL_w: 0.00000;  KL_y: 1.58482;\n",
      "Total Loss=Train: 33025.04429; Val: 25060.58712\n",
      "(Epoch 156 / 400)\n",
      "Train - Log-lik: -32912.65595;  CP: 94.14590;  KL_w: 0.00001;  KL_y: 1.49584;\n",
      "Valid - Log-lik: -25001.07535;  CP: 92.77280;  KL_w: 0.00001;  KL_y: 1.60292;\n",
      "Total Loss=Train: 33008.29757; Val: 25095.45108\n",
      "(Epoch 157 / 400)\n",
      "Train - Log-lik: -32916.41963;  CP: 94.68727;  KL_w: 0.00001;  KL_y: 1.50643;\n",
      "Valid - Log-lik: -24735.69209;  CP: 93.54990;  KL_w: 0.00000;  KL_y: 1.53901;\n",
      "Total Loss=Train: 33012.61337; Val: 24830.78104\n",
      "(Epoch 158 / 400)\n",
      "Train - Log-lik: -32964.42725;  CP: 92.89027;  KL_w: 0.00001;  KL_y: 1.50097;\n",
      "Valid - Log-lik: -25292.30276;  CP: 92.17256;  KL_w: 0.00000;  KL_y: 1.57522;\n",
      "Total Loss=Train: 33058.81844; Val: 25386.05054\n",
      "(Epoch 159 / 400)\n",
      "Train - Log-lik: -32932.85521;  CP: 93.76638;  KL_w: 0.00000;  KL_y: 1.49767;\n",
      "Valid - Log-lik: -24755.52694;  CP: 93.98019;  KL_w: 0.00001;  KL_y: 1.60163;\n",
      "Total Loss=Train: 33028.11948; Val: 24851.10881\n",
      "(Epoch 160 / 400)\n",
      "Train - Log-lik: -32858.26997;  CP: 94.61037;  KL_w: 0.00001;  KL_y: 1.51096;\n",
      "Valid - Log-lik: -24983.97100;  CP: 92.48952;  KL_w: 0.00000;  KL_y: 1.61425;\n",
      "Total Loss=Train: 32954.39140; Val: 25078.07475\n",
      "====================================================================================================\n",
      "Saving checkpoint to \"experiments/FMNIST_200epochs_dp0_lr001/checkpoints/GMVAE_1_00001_25_25_10_50_3_03_0001/step_160.pt\".\n",
      "====================================================================================================\n",
      "\n",
      "(Epoch 161 / 400)\n",
      "Train - Log-lik: -32798.72854;  CP: 91.86282;  KL_w: 0.00001;  KL_y: 1.50591;\n",
      "Valid - Log-lik: -24683.18592;  CP: 91.02541;  KL_w: 0.00000;  KL_y: 1.63123;\n",
      "Total Loss=Train: 32892.09718; Val: 24775.84255\n",
      "(Epoch 162 / 400)\n",
      "Train - Log-lik: -32941.68583;  CP: 93.61351;  KL_w: 0.00000;  KL_y: 1.50441;\n",
      "Valid - Log-lik: -24585.90354;  CP: 93.93565;  KL_w: 0.00000;  KL_y: 1.56008;\n",
      "Total Loss=Train: 33036.80374; Val: 24681.39922\n",
      "(Epoch 163 / 400)\n",
      "Train - Log-lik: -32864.92351;  CP: 93.45575;  KL_w: 0.00001;  KL_y: 1.50629;\n",
      "Valid - Log-lik: -24424.32104;  CP: 91.85452;  KL_w: 0.00001;  KL_y: 1.58884;\n",
      "Total Loss=Train: 32959.88542; Val: 24517.76445\n",
      "(Epoch 164 / 400)\n",
      "Train - Log-lik: -32875.35848;  CP: 92.50549;  KL_w: 0.00001;  KL_y: 1.50582;\n",
      "Valid - Log-lik: -24905.53567;  CP: 91.04558;  KL_w: 0.00000;  KL_y: 1.58278;\n",
      "Total Loss=Train: 32969.36993; Val: 24998.16401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 165 / 400)\n",
      "Train - Log-lik: -32841.28226;  CP: 93.14159;  KL_w: 0.00001;  KL_y: 1.50137;\n",
      "Valid - Log-lik: -24942.05412;  CP: 92.60593;  KL_w: 0.00000;  KL_y: 1.59157;\n",
      "Total Loss=Train: 32935.92524; Val: 25036.25161\n",
      "(Epoch 166 / 400)\n",
      "Train - Log-lik: -32779.50814;  CP: 93.59482;  KL_w: 0.00001;  KL_y: 1.50037;\n",
      "Valid - Log-lik: -24436.83729;  CP: 93.23100;  KL_w: 0.00001;  KL_y: 1.57080;\n",
      "Total Loss=Train: 32874.60335; Val: 24531.63907\n",
      "(Epoch 167 / 400)\n",
      "Train - Log-lik: -32894.78044;  CP: 93.13429;  KL_w: 0.00001;  KL_y: 1.49251;\n",
      "Valid - Log-lik: -24722.93390;  CP: 93.26069;  KL_w: 0.00000;  KL_y: 1.57723;\n",
      "Total Loss=Train: 32989.40720; Val: 24817.77178\n",
      "(Epoch 168 / 400)\n",
      "Train - Log-lik: -32973.20958;  CP: 92.68083;  KL_w: 0.00001;  KL_y: 1.49365;\n",
      "Valid - Log-lik: -24769.61151;  CP: 90.55009;  KL_w: 0.00000;  KL_y: 1.59388;\n",
      "Total Loss=Train: 33067.38393; Val: 24861.75551\n",
      "(Epoch 169 / 400)\n",
      "Train - Log-lik: -32893.84934;  CP: 92.69590;  KL_w: 0.00000;  KL_y: 1.49825;\n",
      "Valid - Log-lik: -24737.99911;  CP: 91.80653;  KL_w: 0.00000;  KL_y: 1.56705;\n",
      "Total Loss=Train: 32988.04341; Val: 24831.37265\n",
      "(Epoch 170 / 400)\n",
      "Train - Log-lik: -32870.04275;  CP: 94.09372;  KL_w: 0.00000;  KL_y: 1.48458;\n",
      "Valid - Log-lik: -24767.78685;  CP: 93.28601;  KL_w: 0.00000;  KL_y: 1.54184;\n",
      "Total Loss=Train: 32965.62116; Val: 24862.61468\n",
      "====================================================================================================\n",
      "Saving checkpoint to \"experiments/FMNIST_200epochs_dp0_lr001/checkpoints/GMVAE_1_00001_25_25_10_50_3_03_0001/step_170.pt\".\n",
      "====================================================================================================\n",
      "\n",
      "(Epoch 171 / 400)\n",
      "Train - Log-lik: -32800.14740;  CP: 93.94153;  KL_w: 0.00000;  KL_y: 1.48411;\n",
      "Valid - Log-lik: -24643.25487;  CP: 93.50903;  KL_w: 0.00000;  KL_y: 1.57829;\n",
      "Total Loss=Train: 32895.57325; Val: 24738.34221\n",
      "(Epoch 172 / 400)\n",
      "Train - Log-lik: -32965.82952;  CP: 93.99508;  KL_w: 0.00001;  KL_y: 1.49432;\n",
      "Valid - Log-lik: -24781.53471;  CP: 93.55105;  KL_w: 0.00001;  KL_y: 1.58037;\n",
      "Total Loss=Train: 33061.31874; Val: 24876.66611\n",
      "(Epoch 173 / 400)\n",
      "Train - Log-lik: -32825.33750;  CP: 93.47310;  KL_w: 0.00001;  KL_y: 1.48609;\n",
      "Valid - Log-lik: -24580.24665;  CP: 92.36883;  KL_w: 0.00000;  KL_y: 1.61335;\n",
      "Total Loss=Train: 32920.29673; Val: 24674.22884\n",
      "(Epoch 174 / 400)\n",
      "Train - Log-lik: -32882.48810;  CP: 93.64954;  KL_w: 0.00001;  KL_y: 1.49917;\n",
      "Valid - Log-lik: -24802.74817;  CP: 93.30632;  KL_w: 0.00001;  KL_y: 1.58436;\n",
      "Total Loss=Train: 32977.63679; Val: 24897.63886\n",
      "(Epoch 175 / 400)\n",
      "Train - Log-lik: -32847.49115;  CP: 93.99724;  KL_w: 0.00000;  KL_y: 1.49350;\n",
      "Valid - Log-lik: -24882.63855;  CP: 92.07881;  KL_w: 0.00000;  KL_y: 1.56197;\n",
      "Total Loss=Train: 32942.98198; Val: 24976.27934\n",
      "(Epoch 176 / 400)\n",
      "Train - Log-lik: -32854.22337;  CP: 93.05349;  KL_w: 0.00001;  KL_y: 1.49181;\n",
      "Valid - Log-lik: -24733.08392;  CP: 91.36296;  KL_w: 0.00000;  KL_y: 1.55564;\n",
      "Total Loss=Train: 32948.76855; Val: 24826.00252\n",
      "(Epoch 177 / 400)\n",
      "Train - Log-lik: -32998.26140;  CP: 91.57097;  KL_w: 0.00001;  KL_y: 1.49008;\n",
      "Valid - Log-lik: -25067.14084;  CP: 90.46667;  KL_w: 0.00001;  KL_y: 1.56035;\n",
      "Total Loss=Train: 33091.32240; Val: 25159.16785\n",
      "(Epoch 178 / 400)\n",
      "Train - Log-lik: -32853.87415;  CP: 91.36885;  KL_w: 0.00001;  KL_y: 1.49320;\n",
      "Valid - Log-lik: -24751.25185;  CP: 91.45295;  KL_w: 0.00001;  KL_y: 1.58027;\n",
      "Total Loss=Train: 32946.73612; Val: 24844.28506\n",
      "(Epoch 179 / 400)\n",
      "Train - Log-lik: -33026.97894;  CP: 93.17042;  KL_w: 0.00000;  KL_y: 1.50474;\n",
      "Valid - Log-lik: -24498.09661;  CP: 92.75734;  KL_w: 0.00000;  KL_y: 1.58877;\n",
      "Total Loss=Train: 33121.65394; Val: 24592.44270\n",
      "(Epoch 180 / 400)\n",
      "Train - Log-lik: -32938.71738;  CP: 93.94713;  KL_w: 0.00000;  KL_y: 1.49669;\n",
      "Valid - Log-lik: -24444.77874;  CP: 92.84637;  KL_w: 0.00000;  KL_y: 1.61901;\n",
      "Total Loss=Train: 33034.16130; Val: 24539.24411\n",
      "====================================================================================================\n",
      "Saving checkpoint to \"experiments/FMNIST_200epochs_dp0_lr001/checkpoints/GMVAE_1_00001_25_25_10_50_3_03_0001/step_180.pt\".\n",
      "====================================================================================================\n",
      "\n",
      "(Epoch 181 / 400)\n",
      "Train - Log-lik: -32963.14030;  CP: 93.47423;  KL_w: 0.00001;  KL_y: 1.51173;\n",
      "Valid - Log-lik: -24964.43993;  CP: 92.21517;  KL_w: 0.00000;  KL_y: 1.63500;\n",
      "Total Loss=Train: 33058.12628; Val: 25058.29008\n",
      "(Epoch 182 / 400)\n",
      "Train - Log-lik: -32833.62793;  CP: 93.90595;  KL_w: 0.00001;  KL_y: 1.49426;\n",
      "Valid - Log-lik: -24430.56016;  CP: 92.53925;  KL_w: 0.00000;  KL_y: 1.54208;\n",
      "Total Loss=Train: 32929.02812; Val: 24524.64146\n",
      "(Epoch 183 / 400)\n",
      "Train - Log-lik: -32891.02798;  CP: 93.41943;  KL_w: 0.00000;  KL_y: 1.49631;\n",
      "Valid - Log-lik: -24631.39152;  CP: 91.50325;  KL_w: 0.00000;  KL_y: 1.54135;\n",
      "Total Loss=Train: 32985.94364; Val: 24724.43614\n",
      "(Epoch 184 / 400)\n",
      "Train - Log-lik: -32695.12056;  CP: 92.46427;  KL_w: 0.00000;  KL_y: 1.48129;\n",
      "Valid - Log-lik: -24441.28112;  CP: 92.47456;  KL_w: 0.00000;  KL_y: 1.56056;\n",
      "Total Loss=Train: 32789.06599; Val: 24535.31624\n",
      "(Epoch 185 / 400)\n",
      "Train - Log-lik: -32822.24590;  CP: 92.02463;  KL_w: 0.00000;  KL_y: 1.49125;\n",
      "Valid - Log-lik: -24779.43585;  CP: 91.67910;  KL_w: 0.00000;  KL_y: 1.58934;\n",
      "Total Loss=Train: 32915.76170; Val: 24872.70429\n",
      "(Epoch 186 / 400)\n",
      "Train - Log-lik: -32875.92754;  CP: 92.91935;  KL_w: 0.00000;  KL_y: 1.48388;\n",
      "Valid - Log-lik: -24548.77242;  CP: 92.68610;  KL_w: 0.00000;  KL_y: 1.55065;\n",
      "Total Loss=Train: 32970.33087; Val: 24643.00917\n",
      "(Epoch 187 / 400)\n",
      "Train - Log-lik: -32867.57427;  CP: 94.38856;  KL_w: 0.00001;  KL_y: 1.49630;\n",
      "Valid - Log-lik: -24588.03599;  CP: 92.12305;  KL_w: 0.00001;  KL_y: 1.59954;\n",
      "Total Loss=Train: 32963.45919; Val: 24681.75860\n",
      "(Epoch 188 / 400)\n",
      "Train - Log-lik: -32920.07574;  CP: 93.34744;  KL_w: 0.00001;  KL_y: 1.49393;\n",
      "Valid - Log-lik: -24831.98567;  CP: 92.53782;  KL_w: 0.00000;  KL_y: 1.57731;\n",
      "Total Loss=Train: 33014.91699; Val: 24926.10080\n",
      "(Epoch 189 / 400)\n",
      "Train - Log-lik: -32847.03553;  CP: 93.57735;  KL_w: 0.00000;  KL_y: 1.49723;\n",
      "Valid - Log-lik: -24844.83176;  CP: 93.35770;  KL_w: 0.00000;  KL_y: 1.56617;\n",
      "Total Loss=Train: 32942.11012; Val: 24939.75561\n",
      "(Epoch 190 / 400)\n",
      "Train - Log-lik: -32857.94898;  CP: 93.32502;  KL_w: 0.00000;  KL_y: 1.49272;\n",
      "Valid - Log-lik: -24848.67378;  CP: 91.75670;  KL_w: 0.00001;  KL_y: 1.53164;\n",
      "Total Loss=Train: 32952.76663; Val: 24941.96211\n",
      "====================================================================================================\n",
      "Saving checkpoint to \"experiments/FMNIST_200epochs_dp0_lr001/checkpoints/GMVAE_1_00001_25_25_10_50_3_03_0001/step_190.pt\".\n",
      "====================================================================================================\n",
      "\n",
      "(Epoch 191 / 400)\n",
      "Train - Log-lik: -32716.57549;  CP: 92.14205;  KL_w: 0.00001;  KL_y: 1.48351;\n",
      "Valid - Log-lik: -25049.96734;  CP: 90.63846;  KL_w: 0.00000;  KL_y: 1.55924;\n",
      "Total Loss=Train: 32810.20110; Val: 25142.16502\n",
      "(Epoch 192 / 400)\n",
      "Train - Log-lik: -33051.72116;  CP: 93.13199;  KL_w: 0.00000;  KL_y: 1.49208;\n",
      "Valid - Log-lik: -24821.45072;  CP: 93.52050;  KL_w: 0.00001;  KL_y: 1.57358;\n",
      "Total Loss=Train: 33146.34517; Val: 24916.54483\n",
      "(Epoch 193 / 400)\n",
      "Train - Log-lik: -32725.18566;  CP: 93.73168;  KL_w: 0.00001;  KL_y: 1.48208;\n",
      "Valid - Log-lik: -24794.42892;  CP: 91.97892;  KL_w: 0.00000;  KL_y: 1.55776;\n",
      "Total Loss=Train: 32820.39943; Val: 24887.96561\n",
      "(Epoch 194 / 400)\n",
      "Train - Log-lik: -32851.57711;  CP: 92.91531;  KL_w: 0.00001;  KL_y: 1.48375;\n",
      "Valid - Log-lik: -25233.93773;  CP: 93.06987;  KL_w: 0.00000;  KL_y: 1.57022;\n",
      "Total Loss=Train: 32945.97626; Val: 25328.57786\n",
      "(Epoch 195 / 400)\n",
      "Train - Log-lik: -32815.73735;  CP: 92.97528;  KL_w: 0.00000;  KL_y: 1.48384;\n",
      "Valid - Log-lik: -24847.58492;  CP: 91.34719;  KL_w: 0.00001;  KL_y: 1.60559;\n",
      "Total Loss=Train: 32910.19640; Val: 24940.53771\n",
      "(Epoch 196 / 400)\n",
      "Train - Log-lik: -32775.25591;  CP: 91.77627;  KL_w: 0.00001;  KL_y: 1.47976;\n",
      "Valid - Log-lik: -24533.95407;  CP: 91.18277;  KL_w: 0.00000;  KL_y: 1.57000;\n",
      "Total Loss=Train: 32868.51196; Val: 24626.70680\n",
      "(Epoch 197 / 400)\n",
      "Train - Log-lik: -32783.87578;  CP: 93.00340;  KL_w: 0.00000;  KL_y: 1.48479;\n",
      "Valid - Log-lik: -24526.52380;  CP: 92.06907;  KL_w: 0.00000;  KL_y: 1.56471;\n",
      "Total Loss=Train: 32878.36403; Val: 24620.15761\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 198 / 400)\n",
      "Train - Log-lik: -32732.09106;  CP: 92.08517;  KL_w: 0.00000;  KL_y: 1.48219;\n",
      "Valid - Log-lik: -24644.08048;  CP: 91.97928;  KL_w: 0.00000;  KL_y: 1.55369;\n",
      "Total Loss=Train: 32825.65861; Val: 24737.61344\n",
      "(Epoch 199 / 400)\n",
      "Train - Log-lik: -32779.07157;  CP: 91.59223;  KL_w: 0.00000;  KL_y: 1.48240;\n",
      "Valid - Log-lik: -24767.23165;  CP: 91.15565;  KL_w: 0.00000;  KL_y: 1.56467;\n",
      "Total Loss=Train: 32872.14627; Val: 24859.95198\n",
      "(Epoch 200 / 400)\n",
      "Train - Log-lik: -32804.92834;  CP: 92.78247;  KL_w: 0.00000;  KL_y: 1.48488;\n",
      "Valid - Log-lik: -24509.78018;  CP: 91.32973;  KL_w: 0.00000;  KL_y: 1.58488;\n",
      "Total Loss=Train: 32899.19569; Val: 24602.69479\n",
      "====================================================================================================\n",
      "Saving checkpoint to \"experiments/FMNIST_200epochs_dp0_lr001/checkpoints/GMVAE_1_00001_25_25_10_50_3_03_0001/step_200.pt\".\n",
      "====================================================================================================\n",
      "\n",
      "(Epoch 201 / 400)\n",
      "Train - Log-lik: -32778.68689;  CP: 93.00811;  KL_w: 0.00001;  KL_y: 1.48447;\n",
      "Valid - Log-lik: -24832.08087;  CP: 92.70443;  KL_w: 0.00000;  KL_y: 1.53387;\n",
      "Total Loss=Train: 32873.17956; Val: 24926.31921\n",
      "(Epoch 202 / 400)\n",
      "Train - Log-lik: -32803.48527;  CP: 92.84553;  KL_w: 0.00000;  KL_y: 1.47632;\n",
      "Valid - Log-lik: -24403.30785;  CP: 91.77119;  KL_w: 0.00000;  KL_y: 1.56573;\n",
      "Total Loss=Train: 32897.80713; Val: 24496.64478\n",
      "(Epoch 203 / 400)\n",
      "Train - Log-lik: -32729.64698;  CP: 93.65455;  KL_w: 0.00001;  KL_y: 1.47513;\n",
      "Valid - Log-lik: -24758.42309;  CP: 92.00916;  KL_w: 0.00000;  KL_y: 1.52308;\n",
      "Total Loss=Train: 32824.77667; Val: 24851.95534\n",
      "(Epoch 204 / 400)\n",
      "Train - Log-lik: -32762.25789;  CP: 92.65826;  KL_w: 0.00001;  KL_y: 1.48031;\n",
      "Valid - Log-lik: -24420.50073;  CP: 92.27037;  KL_w: 0.00001;  KL_y: 1.56419;\n",
      "Total Loss=Train: 32856.39643; Val: 24514.33533\n",
      "(Epoch 205 / 400)\n",
      "Train - Log-lik: -32806.17443;  CP: 94.34771;  KL_w: 0.00001;  KL_y: 1.46920;\n",
      "Valid - Log-lik: -24779.98882;  CP: 93.97204;  KL_w: 0.00000;  KL_y: 1.53089;\n",
      "Total Loss=Train: 32901.99147; Val: 24875.49178\n",
      "(Epoch 206 / 400)\n",
      "Train - Log-lik: -32804.10427;  CP: 94.45091;  KL_w: 0.00000;  KL_y: 1.46744;\n",
      "Valid - Log-lik: -24409.16817;  CP: 92.02023;  KL_w: 0.00000;  KL_y: 1.53412;\n",
      "Total Loss=Train: 32900.02273; Val: 24502.72258\n",
      "(Epoch 207 / 400)\n",
      "Train - Log-lik: -32773.20349;  CP: 93.36704;  KL_w: 0.00001;  KL_y: 1.46807;\n",
      "Valid - Log-lik: -24358.97774;  CP: 91.75299;  KL_w: 0.00001;  KL_y: 1.53432;\n",
      "Total Loss=Train: 32868.03877; Val: 24452.26503\n",
      "(Epoch 208 / 400)\n",
      "Train - Log-lik: -32709.49201;  CP: 92.98048;  KL_w: 0.00000;  KL_y: 1.47395;\n",
      "Valid - Log-lik: -24585.28892;  CP: 91.83781;  KL_w: 0.00000;  KL_y: 1.55705;\n",
      "Total Loss=Train: 32803.94648; Val: 24678.68383\n",
      "(Epoch 209 / 400)\n",
      "Train - Log-lik: -32753.82293;  CP: 92.57894;  KL_w: 0.00000;  KL_y: 1.46918;\n",
      "Valid - Log-lik: -24632.05511;  CP: 91.73010;  KL_w: 0.00000;  KL_y: 1.55001;\n",
      "Total Loss=Train: 32847.87105; Val: 24725.33526\n",
      "(Epoch 210 / 400)\n",
      "Train - Log-lik: -32695.62209;  CP: 92.41845;  KL_w: 0.00000;  KL_y: 1.46178;\n",
      "Valid - Log-lik: -24460.89462;  CP: 91.14012;  KL_w: 0.00000;  KL_y: 1.56002;\n",
      "Total Loss=Train: 32789.50237; Val: 24553.59477\n",
      "====================================================================================================\n",
      "Saving checkpoint to \"experiments/FMNIST_200epochs_dp0_lr001/checkpoints/GMVAE_1_00001_25_25_10_50_3_03_0001/step_210.pt\".\n",
      "====================================================================================================\n",
      "\n",
      "(Epoch 211 / 400)\n",
      "Train - Log-lik: -32706.12257;  CP: 92.33369;  KL_w: 0.00000;  KL_y: 1.46331;\n",
      "Valid - Log-lik: -24640.00175;  CP: 91.04270;  KL_w: 0.00000;  KL_y: 1.53686;\n",
      "Total Loss=Train: 32799.91964; Val: 24732.58130\n",
      "(Epoch 212 / 400)\n",
      "Train - Log-lik: -32748.96869;  CP: 92.52860;  KL_w: 0.00000;  KL_y: 1.45865;\n",
      "Valid - Log-lik: -24476.82746;  CP: 90.83021;  KL_w: 0.00000;  KL_y: 1.51043;\n",
      "Total Loss=Train: 32842.95597; Val: 24569.16807\n",
      "(Epoch 213 / 400)\n",
      "Train - Log-lik: -32675.64977;  CP: 90.53403;  KL_w: 0.00000;  KL_y: 1.46050;\n",
      "Valid - Log-lik: -24557.09896;  CP: 90.69322;  KL_w: 0.00000;  KL_y: 1.51799;\n",
      "Total Loss=Train: 32767.64439; Val: 24649.31017\n",
      "(Epoch 214 / 400)\n",
      "Train - Log-lik: -32770.75252;  CP: 92.62344;  KL_w: 0.00000;  KL_y: 1.46779;\n",
      "Valid - Log-lik: -24402.92703;  CP: 91.38332;  KL_w: 0.00000;  KL_y: 1.49596;\n",
      "Total Loss=Train: 32864.84393; Val: 24495.80636\n",
      "(Epoch 215 / 400)\n",
      "Train - Log-lik: -32755.37519;  CP: 92.31031;  KL_w: 0.00000;  KL_y: 1.45675;\n",
      "Valid - Log-lik: -24590.62324;  CP: 92.42996;  KL_w: 0.00000;  KL_y: 1.55205;\n",
      "Total Loss=Train: 32849.14228; Val: 24684.60519\n",
      "(Epoch 216 / 400)\n",
      "Train - Log-lik: -32651.29638;  CP: 93.63043;  KL_w: 0.00000;  KL_y: 1.46447;\n",
      "Valid - Log-lik: -24573.39256;  CP: 92.03974;  KL_w: 0.00001;  KL_y: 1.48343;\n",
      "Total Loss=Train: 32746.39125; Val: 24666.91572\n",
      "(Epoch 217 / 400)\n",
      "Train - Log-lik: -32665.78997;  CP: 92.84170;  KL_w: 0.00000;  KL_y: 1.46546;\n",
      "Valid - Log-lik: -24316.73614;  CP: 92.23860;  KL_w: 0.00000;  KL_y: 1.52896;\n",
      "Total Loss=Train: 32760.09716; Val: 24410.50373\n",
      "(Epoch 218 / 400)\n",
      "Train - Log-lik: -32759.98382;  CP: 93.74695;  KL_w: 0.00000;  KL_y: 1.46705;\n",
      "Valid - Log-lik: -24763.05188;  CP: 92.66447;  KL_w: 0.00000;  KL_y: 1.52287;\n",
      "Total Loss=Train: 32855.19794; Val: 24857.23930\n",
      "(Epoch 219 / 400)\n",
      "Train - Log-lik: -32820.30312;  CP: 93.18166;  KL_w: 0.00000;  KL_y: 1.47161;\n",
      "Valid - Log-lik: -24332.35854;  CP: 92.28315;  KL_w: 0.00000;  KL_y: 1.54933;\n",
      "Total Loss=Train: 32914.95652; Val: 24426.19099\n",
      "(Epoch 220 / 400)\n",
      "Train - Log-lik: -32739.46896;  CP: 92.63402;  KL_w: 0.00000;  KL_y: 1.45752;\n",
      "Valid - Log-lik: -24485.45899;  CP: 90.03785;  KL_w: 0.00000;  KL_y: 1.53308;\n",
      "Total Loss=Train: 32833.56041; Val: 24577.02995\n",
      "====================================================================================================\n",
      "Saving checkpoint to \"experiments/FMNIST_200epochs_dp0_lr001/checkpoints/GMVAE_1_00001_25_25_10_50_3_03_0001/step_220.pt\".\n",
      "====================================================================================================\n",
      "\n",
      "(Epoch 221 / 400)\n",
      "Train - Log-lik: -32674.85951;  CP: 91.74977;  KL_w: 0.00000;  KL_y: 1.45570;\n",
      "Valid - Log-lik: -24677.23654;  CP: 90.97595;  KL_w: 0.00000;  KL_y: 1.53426;\n",
      "Total Loss=Train: 32768.06501; Val: 24769.74677\n",
      "(Epoch 222 / 400)\n",
      "Train - Log-lik: -32671.03519;  CP: 91.95042;  KL_w: 0.00000;  KL_y: 1.45462;\n",
      "Valid - Log-lik: -24524.78797;  CP: 91.67635;  KL_w: 0.00001;  KL_y: 1.52164;\n",
      "Total Loss=Train: 32764.44025; Val: 24617.98596\n",
      "(Epoch 223 / 400)\n",
      "Train - Log-lik: -32539.46361;  CP: 92.01801;  KL_w: 0.00000;  KL_y: 1.45670;\n",
      "Valid - Log-lik: -24178.63306;  CP: 90.58894;  KL_w: 0.00001;  KL_y: 1.52600;\n",
      "Total Loss=Train: 32632.93832; Val: 24270.74794\n",
      "(Epoch 224 / 400)\n",
      "Train - Log-lik: -32661.67572;  CP: 91.87683;  KL_w: 0.00000;  KL_y: 1.46058;\n",
      "Valid - Log-lik: -24524.24134;  CP: 90.69691;  KL_w: 0.00000;  KL_y: 1.52870;\n",
      "Total Loss=Train: 32755.01305; Val: 24616.46697\n",
      "(Epoch 225 / 400)\n",
      "Train - Log-lik: -32575.59959;  CP: 91.96060;  KL_w: 0.00000;  KL_y: 1.46059;\n",
      "Valid - Log-lik: -24612.88684;  CP: 90.91537;  KL_w: 0.00000;  KL_y: 1.57550;\n",
      "Total Loss=Train: 32669.02083; Val: 24705.37770\n",
      "(Epoch 226 / 400)\n",
      "Train - Log-lik: -32630.43652;  CP: 91.86738;  KL_w: 0.00000;  KL_y: 1.46754;\n",
      "Valid - Log-lik: -24092.71865;  CP: 91.05406;  KL_w: 0.00000;  KL_y: 1.52169;\n",
      "Total Loss=Train: 32723.77134; Val: 24185.29436\n",
      "(Epoch 227 / 400)\n",
      "Train - Log-lik: -32709.93397;  CP: 92.23518;  KL_w: 0.00000;  KL_y: 1.46367;\n",
      "Valid - Log-lik: -24236.34395;  CP: 91.74041;  KL_w: 0.00000;  KL_y: 1.53183;\n",
      "Total Loss=Train: 32803.63282; Val: 24329.61618\n",
      "(Epoch 228 / 400)\n",
      "Train - Log-lik: -32749.99034;  CP: 92.22387;  KL_w: 0.00000;  KL_y: 1.45734;\n",
      "Valid - Log-lik: -24444.11631;  CP: 91.70277;  KL_w: 0.00001;  KL_y: 1.56913;\n",
      "Total Loss=Train: 32843.67157; Val: 24537.38817\n",
      "(Epoch 229 / 400)\n",
      "Train - Log-lik: -32642.55428;  CP: 92.80912;  KL_w: 0.00000;  KL_y: 1.45729;\n",
      "Valid - Log-lik: -24116.74994;  CP: 92.08341;  KL_w: 0.00000;  KL_y: 1.52706;\n",
      "Total Loss=Train: 32736.82092; Val: 24210.36041\n",
      "(Epoch 230 / 400)\n",
      "Train - Log-lik: -32771.03588;  CP: 93.56198;  KL_w: 0.00000;  KL_y: 1.45383;\n",
      "Valid - Log-lik: -24494.63156;  CP: 93.35646;  KL_w: 0.00000;  KL_y: 1.53453;\n",
      "Total Loss=Train: 32866.05176; Val: 24589.52256\n",
      "====================================================================================================\n",
      "Saving checkpoint to \"experiments/FMNIST_200epochs_dp0_lr001/checkpoints/GMVAE_1_00001_25_25_10_50_3_03_0001/step_230.pt\".\n",
      "====================================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 231 / 400)\n",
      "Train - Log-lik: -32663.49583;  CP: 93.32321;  KL_w: 0.00000;  KL_y: 1.46530;\n",
      "Valid - Log-lik: -24288.86647;  CP: 92.35180;  KL_w: 0.00000;  KL_y: 1.51695;\n",
      "Total Loss=Train: 32758.28428; Val: 24382.73523\n",
      "(Epoch 232 / 400)\n",
      "Train - Log-lik: -32724.75325;  CP: 93.12778;  KL_w: 0.00000;  KL_y: 1.46015;\n",
      "Valid - Log-lik: -24327.14299;  CP: 90.67425;  KL_w: 0.00000;  KL_y: 1.53000;\n",
      "Total Loss=Train: 32819.34112; Val: 24419.34724\n",
      "(Epoch 233 / 400)\n",
      "Train - Log-lik: -32628.98856;  CP: 92.03568;  KL_w: 0.00000;  KL_y: 1.46716;\n",
      "Valid - Log-lik: -24451.79136;  CP: 91.71408;  KL_w: 0.00000;  KL_y: 1.51255;\n",
      "Total Loss=Train: 32722.49151; Val: 24545.01796\n",
      "(Epoch 234 / 400)\n",
      "Train - Log-lik: -32593.95067;  CP: 92.35972;  KL_w: 0.00000;  KL_y: 1.46871;\n",
      "Valid - Log-lik: -24252.72423;  CP: 90.92870;  KL_w: 0.00000;  KL_y: 1.53403;\n",
      "Total Loss=Train: 32687.77905; Val: 24345.18696\n",
      "(Epoch 235 / 400)\n",
      "Train - Log-lik: -32585.60004;  CP: 91.43274;  KL_w: 0.00000;  KL_y: 1.47014;\n",
      "Valid - Log-lik: -24286.53655;  CP: 89.73142;  KL_w: 0.00001;  KL_y: 1.56023;\n",
      "Total Loss=Train: 32678.50284; Val: 24377.82822\n",
      "(Epoch 236 / 400)\n",
      "Train - Log-lik: -32663.35776;  CP: 90.65235;  KL_w: 0.00000;  KL_y: 1.47520;\n",
      "Valid - Log-lik: -24451.71147;  CP: 89.14686;  KL_w: 0.00001;  KL_y: 1.52435;\n",
      "Total Loss=Train: 32755.48531; Val: 24542.38268\n",
      "(Epoch 237 / 400)\n",
      "Train - Log-lik: -32614.62949;  CP: 91.22712;  KL_w: 0.00001;  KL_y: 1.48188;\n",
      "Valid - Log-lik: -24588.42379;  CP: 90.87805;  KL_w: 0.00000;  KL_y: 1.56912;\n",
      "Total Loss=Train: 32707.33858; Val: 24680.87091\n",
      "(Epoch 238 / 400)\n",
      "Train - Log-lik: -32619.72264;  CP: 91.69866;  KL_w: 0.00000;  KL_y: 1.47922;\n",
      "Valid - Log-lik: -24305.80896;  CP: 90.13747;  KL_w: 0.00000;  KL_y: 1.54387;\n",
      "Total Loss=Train: 32712.90055; Val: 24397.49025\n",
      "(Epoch 239 / 400)\n",
      "Train - Log-lik: -32607.94505;  CP: 89.86824;  KL_w: 0.00000;  KL_y: 1.48274;\n",
      "Valid - Log-lik: -24236.48971;  CP: 88.88985;  KL_w: 0.00001;  KL_y: 1.57131;\n",
      "Total Loss=Train: 32699.29598; Val: 24326.95082\n",
      "(Epoch 240 / 400)\n",
      "Train - Log-lik: -32619.11181;  CP: 91.35035;  KL_w: 0.00000;  KL_y: 1.48030;\n",
      "Valid - Log-lik: -24245.50545;  CP: 88.12355;  KL_w: 0.00000;  KL_y: 1.57333;\n",
      "Total Loss=Train: 32711.94232; Val: 24335.20233\n",
      "====================================================================================================\n",
      "Saving checkpoint to \"experiments/FMNIST_200epochs_dp0_lr001/checkpoints/GMVAE_1_00001_25_25_10_50_3_03_0001/step_240.pt\".\n",
      "====================================================================================================\n",
      "\n",
      "(Epoch 241 / 400)\n",
      "Train - Log-lik: -32618.46185;  CP: 90.89083;  KL_w: 0.00000;  KL_y: 1.48675;\n",
      "Valid - Log-lik: -24447.46546;  CP: 89.59146;  KL_w: 0.00000;  KL_y: 1.54638;\n",
      "Total Loss=Train: 32710.83925; Val: 24538.60326\n",
      "(Epoch 242 / 400)\n",
      "Train - Log-lik: -32612.52273;  CP: 90.14769;  KL_w: 0.00000;  KL_y: 1.48322;\n",
      "Valid - Log-lik: -24480.42368;  CP: 89.55960;  KL_w: 0.00000;  KL_y: 1.54604;\n",
      "Total Loss=Train: 32704.15347; Val: 24571.52926\n",
      "(Epoch 243 / 400)\n",
      "Train - Log-lik: -32575.34110;  CP: 92.31703;  KL_w: 0.00000;  KL_y: 1.48520;\n",
      "Valid - Log-lik: -24225.79771;  CP: 91.71386;  KL_w: 0.00000;  KL_y: 1.58551;\n",
      "Total Loss=Train: 32669.14353; Val: 24319.09704\n",
      "(Epoch 244 / 400)\n",
      "Train - Log-lik: -32639.88096;  CP: 90.78769;  KL_w: 0.00000;  KL_y: 1.47994;\n",
      "Valid - Log-lik: -24315.80607;  CP: 90.64740;  KL_w: 0.00000;  KL_y: 1.57228;\n",
      "Total Loss=Train: 32732.14872; Val: 24408.02574\n",
      "(Epoch 245 / 400)\n",
      "Train - Log-lik: -32672.69895;  CP: 91.25509;  KL_w: 0.00000;  KL_y: 1.47551;\n",
      "Valid - Log-lik: -24264.33741;  CP: 89.60510;  KL_w: 0.00000;  KL_y: 1.53880;\n",
      "Total Loss=Train: 32765.42950; Val: 24355.48132\n",
      "(Epoch 246 / 400)\n",
      "Train - Log-lik: -32557.22757;  CP: 91.26058;  KL_w: 0.00000;  KL_y: 1.48088;\n",
      "Valid - Log-lik: -24225.20094;  CP: 90.33607;  KL_w: 0.00000;  KL_y: 1.49533;\n",
      "Total Loss=Train: 32649.96923; Val: 24317.03236\n",
      "(Epoch 247 / 400)\n",
      "Train - Log-lik: -32569.71150;  CP: 89.90997;  KL_w: 0.00001;  KL_y: 1.48202;\n",
      "Valid - Log-lik: -24508.86217;  CP: 89.47247;  KL_w: 0.00000;  KL_y: 1.57097;\n",
      "Total Loss=Train: 32661.10365; Val: 24599.90562\n",
      "(Epoch 248 / 400)\n",
      "Train - Log-lik: -32733.23484;  CP: 90.95229;  KL_w: 0.00001;  KL_y: 1.48427;\n",
      "Valid - Log-lik: -24600.21025;  CP: 90.50997;  KL_w: 0.00000;  KL_y: 1.55548;\n",
      "Total Loss=Train: 32825.67136; Val: 24692.27568\n",
      "(Epoch 249 / 400)\n",
      "Train - Log-lik: -32499.76373;  CP: 90.62139;  KL_w: 0.00000;  KL_y: 1.48079;\n",
      "Valid - Log-lik: -24304.79272;  CP: 90.34532;  KL_w: 0.00000;  KL_y: 1.52192;\n",
      "Total Loss=Train: 32591.86587; Val: 24396.66001\n",
      "(Epoch 250 / 400)\n",
      "Train - Log-lik: -32512.75014;  CP: 92.32017;  KL_w: 0.00000;  KL_y: 1.47657;\n",
      "Valid - Log-lik: -24083.19920;  CP: 92.27255;  KL_w: 0.00000;  KL_y: 1.60714;\n",
      "Total Loss=Train: 32606.54691; Val: 24177.07889\n",
      "====================================================================================================\n",
      "Saving checkpoint to \"experiments/FMNIST_200epochs_dp0_lr001/checkpoints/GMVAE_1_00001_25_25_10_50_3_03_0001/step_250.pt\".\n",
      "====================================================================================================\n",
      "\n",
      "(Epoch 251 / 400)\n",
      "Train - Log-lik: -32628.56404;  CP: 93.31030;  KL_w: 0.00000;  KL_y: 1.49026;\n",
      "Valid - Log-lik: -24097.97229;  CP: 91.50923;  KL_w: 0.00000;  KL_y: 1.53857;\n",
      "Total Loss=Train: 32723.36459; Val: 24191.02002\n",
      "(Epoch 252 / 400)\n",
      "Train - Log-lik: -32622.17446;  CP: 91.23617;  KL_w: 0.00000;  KL_y: 1.48376;\n",
      "Valid - Log-lik: -24106.75917;  CP: 89.62966;  KL_w: 0.00000;  KL_y: 1.53373;\n",
      "Total Loss=Train: 32714.89440; Val: 24197.92256\n",
      "(Epoch 253 / 400)\n",
      "Train - Log-lik: -32633.04757;  CP: 91.24869;  KL_w: 0.00000;  KL_y: 1.48437;\n",
      "Valid - Log-lik: -24504.66808;  CP: 90.95696;  KL_w: 0.00000;  KL_y: 1.56153;\n",
      "Total Loss=Train: 32725.78056; Val: 24597.18652\n",
      "(Epoch 254 / 400)\n",
      "Train - Log-lik: -32540.82199;  CP: 92.34461;  KL_w: 0.00001;  KL_y: 1.47438;\n",
      "Valid - Log-lik: -24416.48477;  CP: 90.11224;  KL_w: 0.00000;  KL_y: 1.54088;\n",
      "Total Loss=Train: 32634.64106; Val: 24508.13787\n",
      "(Epoch 255 / 400)\n",
      "Train - Log-lik: -32599.29192;  CP: 92.88387;  KL_w: 0.00000;  KL_y: 1.47374;\n",
      "Valid - Log-lik: -24504.81764;  CP: 92.43023;  KL_w: 0.00000;  KL_y: 1.59013;\n",
      "Total Loss=Train: 32693.64968; Val: 24598.83803\n",
      "(Epoch 256 / 400)\n",
      "Train - Log-lik: -32538.18745;  CP: 94.04263;  KL_w: 0.00000;  KL_y: 1.47761;\n",
      "Valid - Log-lik: -24118.87639;  CP: 93.10285;  KL_w: 0.00000;  KL_y: 1.57267;\n",
      "Total Loss=Train: 32633.70766; Val: 24213.55191\n",
      "(Epoch 257 / 400)\n",
      "Train - Log-lik: -32544.65644;  CP: 93.22379;  KL_w: 0.00000;  KL_y: 1.48032;\n",
      "Valid - Log-lik: -24073.43029;  CP: 91.47601;  KL_w: 0.00000;  KL_y: 1.53607;\n",
      "Total Loss=Train: 32639.36046; Val: 24166.44238\n",
      "(Epoch 258 / 400)\n",
      "Train - Log-lik: -32408.43934;  CP: 91.86129;  KL_w: 0.00000;  KL_y: 1.48133;\n",
      "Valid - Log-lik: -24079.61179;  CP: 88.90171;  KL_w: 0.00000;  KL_y: 1.54604;\n",
      "Total Loss=Train: 32501.78180; Val: 24170.05954\n",
      "(Epoch 259 / 400)\n",
      "Train - Log-lik: -32608.37242;  CP: 92.75033;  KL_w: 0.00000;  KL_y: 1.47329;\n",
      "Valid - Log-lik: -24345.30695;  CP: 91.16841;  KL_w: 0.00000;  KL_y: 1.55733;\n",
      "Total Loss=Train: 32702.59606; Val: 24438.03273\n",
      "(Epoch 260 / 400)\n",
      "Train - Log-lik: -32545.47686;  CP: 92.31187;  KL_w: 0.00001;  KL_y: 1.47437;\n",
      "Valid - Log-lik: -24128.33855;  CP: 90.89214;  KL_w: 0.00000;  KL_y: 1.58710;\n",
      "Total Loss=Train: 32639.26304; Val: 24220.81778\n",
      "====================================================================================================\n",
      "Saving checkpoint to \"experiments/FMNIST_200epochs_dp0_lr001/checkpoints/GMVAE_1_00001_25_25_10_50_3_03_0001/step_260.pt\".\n",
      "====================================================================================================\n",
      "\n",
      "(Epoch 261 / 400)\n",
      "Train - Log-lik: -32513.42053;  CP: 91.00777;  KL_w: 0.00000;  KL_y: 1.47923;\n",
      "Valid - Log-lik: -23833.82533;  CP: 90.55710;  KL_w: 0.00000;  KL_y: 1.53154;\n",
      "Total Loss=Train: 32605.90754; Val: 23925.91397\n",
      "(Epoch 262 / 400)\n",
      "Train - Log-lik: -32528.09754;  CP: 92.69632;  KL_w: 0.00000;  KL_y: 1.48104;\n",
      "Valid - Log-lik: -24506.75141;  CP: 91.17281;  KL_w: 0.00000;  KL_y: 1.57132;\n",
      "Total Loss=Train: 32622.27489; Val: 24599.49549\n",
      "(Epoch 263 / 400)\n",
      "Train - Log-lik: -32552.22222;  CP: 91.66651;  KL_w: 0.00000;  KL_y: 1.47506;\n",
      "Valid - Log-lik: -24187.83744;  CP: 90.17931;  KL_w: 0.00000;  KL_y: 1.52820;\n",
      "Total Loss=Train: 32645.36354; Val: 24279.54496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 264 / 400)\n",
      "Train - Log-lik: -32581.86433;  CP: 91.42769;  KL_w: 0.00000;  KL_y: 1.48131;\n",
      "Valid - Log-lik: -24103.12324;  CP: 90.04703;  KL_w: 0.00001;  KL_y: 1.55333;\n",
      "Total Loss=Train: 32674.77332; Val: 24194.72359\n",
      "(Epoch 265 / 400)\n",
      "Train - Log-lik: -32380.25465;  CP: 91.73862;  KL_w: 0.00000;  KL_y: 1.48257;\n",
      "Valid - Log-lik: -24468.65859;  CP: 91.29764;  KL_w: 0.00001;  KL_y: 1.57679;\n",
      "Total Loss=Train: 32473.47578; Val: 24561.53302\n",
      "(Epoch 266 / 400)\n",
      "Train - Log-lik: -32491.09347;  CP: 92.02151;  KL_w: 0.00001;  KL_y: 1.47908;\n",
      "Valid - Log-lik: -23937.44168;  CP: 91.67632;  KL_w: 0.00000;  KL_y: 1.52080;\n",
      "Total Loss=Train: 32584.59408; Val: 24030.63879\n",
      "(Epoch 267 / 400)\n",
      "Train - Log-lik: -32388.77664;  CP: 91.52096;  KL_w: 0.00000;  KL_y: 1.48664;\n",
      "Valid - Log-lik: -24532.00914;  CP: 89.93377;  KL_w: 0.00000;  KL_y: 1.54123;\n",
      "Total Loss=Train: 32481.78425; Val: 24623.48412\n",
      "(Epoch 268 / 400)\n",
      "Train - Log-lik: -32495.34584;  CP: 91.19221;  KL_w: 0.00000;  KL_y: 1.48257;\n",
      "Valid - Log-lik: -24101.46823;  CP: 91.42313;  KL_w: 0.00000;  KL_y: 1.55117;\n",
      "Total Loss=Train: 32588.02036; Val: 24194.44249\n",
      "(Epoch 269 / 400)\n",
      "Train - Log-lik: -32520.05862;  CP: 91.85030;  KL_w: 0.00000;  KL_y: 1.48131;\n",
      "Valid - Log-lik: -24082.28185;  CP: 91.40208;  KL_w: 0.00000;  KL_y: 1.57458;\n",
      "Total Loss=Train: 32613.39025; Val: 24175.25855\n",
      "(Epoch 270 / 400)\n",
      "Train - Log-lik: -32649.09080;  CP: 91.87901;  KL_w: 0.00000;  KL_y: 1.47412;\n",
      "Valid - Log-lik: -24239.17124;  CP: 89.35868;  KL_w: 0.00000;  KL_y: 1.51612;\n",
      "Total Loss=Train: 32742.44399; Val: 24330.04603\n",
      "====================================================================================================\n",
      "Saving checkpoint to \"experiments/FMNIST_200epochs_dp0_lr001/checkpoints/GMVAE_1_00001_25_25_10_50_3_03_0001/step_270.pt\".\n",
      "====================================================================================================\n",
      "\n",
      "(Epoch 271 / 400)\n",
      "Train - Log-lik: -32552.26998;  CP: 90.58697;  KL_w: 0.00000;  KL_y: 1.46696;\n",
      "Valid - Log-lik: -23945.80663;  CP: 90.57511;  KL_w: 0.00000;  KL_y: 1.54156;\n",
      "Total Loss=Train: 32644.32401; Val: 24037.92331\n",
      "(Epoch 272 / 400)\n",
      "Train - Log-lik: -32576.86493;  CP: 90.33130;  KL_w: 0.00000;  KL_y: 1.47534;\n",
      "Valid - Log-lik: -24447.35240;  CP: 89.49297;  KL_w: 0.00000;  KL_y: 1.54731;\n",
      "Total Loss=Train: 32668.67153; Val: 24538.39263\n",
      "(Epoch 273 / 400)\n",
      "Train - Log-lik: -32464.29843;  CP: 91.44447;  KL_w: 0.00000;  KL_y: 1.47022;\n",
      "Valid - Log-lik: -24047.40795;  CP: 91.36093;  KL_w: 0.00000;  KL_y: 1.51354;\n",
      "Total Loss=Train: 32557.21313; Val: 24140.28246\n",
      "(Epoch 274 / 400)\n",
      "Train - Log-lik: -32456.47419;  CP: 92.07735;  KL_w: 0.00000;  KL_y: 1.45893;\n",
      "Valid - Log-lik: -23978.02456;  CP: 91.08917;  KL_w: 0.00000;  KL_y: 1.47776;\n",
      "Total Loss=Train: 32550.01043; Val: 24070.59151\n",
      "(Epoch 275 / 400)\n",
      "Train - Log-lik: -32574.92895;  CP: 90.81131;  KL_w: 0.00000;  KL_y: 1.46483;\n",
      "Valid - Log-lik: -23941.37544;  CP: 90.46269;  KL_w: 0.00000;  KL_y: 1.49969;\n",
      "Total Loss=Train: 32667.20512; Val: 24033.33784\n",
      "(Epoch 276 / 400)\n",
      "Train - Log-lik: -32479.05447;  CP: 90.98434;  KL_w: 0.00000;  KL_y: 1.47652;\n",
      "Valid - Log-lik: -24475.68434;  CP: 90.32475;  KL_w: 0.00000;  KL_y: 1.53121;\n",
      "Total Loss=Train: 32571.51542; Val: 24567.54028\n",
      "(Epoch 277 / 400)\n",
      "Train - Log-lik: -32504.34969;  CP: 91.14903;  KL_w: 0.00000;  KL_y: 1.47338;\n",
      "Valid - Log-lik: -24175.71200;  CP: 90.71659;  KL_w: 0.00001;  KL_y: 1.51396;\n",
      "Total Loss=Train: 32596.97204; Val: 24267.94260\n",
      "(Epoch 278 / 400)\n",
      "Train - Log-lik: -32500.78975;  CP: 92.36283;  KL_w: 0.00000;  KL_y: 1.46433;\n",
      "Valid - Log-lik: -24039.67589;  CP: 91.27874;  KL_w: 0.00000;  KL_y: 1.48787;\n",
      "Total Loss=Train: 32594.61701; Val: 24132.44249\n",
      "(Epoch 279 / 400)\n",
      "Train - Log-lik: -32440.29429;  CP: 91.38499;  KL_w: 0.00000;  KL_y: 1.46621;\n",
      "Valid - Log-lik: -24601.43377;  CP: 90.35132;  KL_w: 0.00000;  KL_y: 1.53723;\n",
      "Total Loss=Train: 32533.14549; Val: 24693.32235\n",
      "(Epoch 280 / 400)\n",
      "Train - Log-lik: -32594.68825;  CP: 91.10598;  KL_w: 0.00000;  KL_y: 1.46537;\n",
      "Valid - Log-lik: -23980.73043;  CP: 90.14826;  KL_w: 0.00000;  KL_y: 1.54650;\n",
      "Total Loss=Train: 32687.25961; Val: 24072.42515\n",
      "====================================================================================================\n",
      "Saving checkpoint to \"experiments/FMNIST_200epochs_dp0_lr001/checkpoints/GMVAE_1_00001_25_25_10_50_3_03_0001/step_280.pt\".\n",
      "====================================================================================================\n",
      "\n",
      "(Epoch 281 / 400)\n",
      "Train - Log-lik: -32412.88384;  CP: 91.45359;  KL_w: 0.00000;  KL_y: 1.47209;\n",
      "Valid - Log-lik: -24120.03329;  CP: 90.59699;  KL_w: 0.00000;  KL_y: 1.51789;\n",
      "Total Loss=Train: 32505.80965; Val: 24212.14818\n",
      "(Epoch 282 / 400)\n",
      "Train - Log-lik: -32475.63323;  CP: 90.91661;  KL_w: 0.00000;  KL_y: 1.47102;\n",
      "Valid - Log-lik: -23832.51063;  CP: 87.88662;  KL_w: 0.00000;  KL_y: 1.51291;\n",
      "Total Loss=Train: 32568.02079; Val: 23921.91016\n",
      "(Epoch 283 / 400)\n",
      "Train - Log-lik: -32544.72189;  CP: 89.45096;  KL_w: 0.00001;  KL_y: 1.47341;\n",
      "Valid - Log-lik: -24291.53684;  CP: 89.94443;  KL_w: 0.00000;  KL_y: 1.52277;\n",
      "Total Loss=Train: 32635.64616; Val: 24383.00400\n",
      "(Epoch 284 / 400)\n",
      "Train - Log-lik: -32571.84173;  CP: 91.03219;  KL_w: 0.00000;  KL_y: 1.47905;\n",
      "Valid - Log-lik: -23930.62825;  CP: 88.89544;  KL_w: 0.00000;  KL_y: 1.55013;\n",
      "Total Loss=Train: 32664.35293; Val: 24021.07382\n",
      "(Epoch 285 / 400)\n",
      "Train - Log-lik: -32598.24101;  CP: 90.48343;  KL_w: 0.00000;  KL_y: 1.48520;\n",
      "Valid - Log-lik: -24439.44317;  CP: 90.18925;  KL_w: 0.00000;  KL_y: 1.58805;\n",
      "Total Loss=Train: 32690.20974; Val: 24531.22047\n",
      "(Epoch 286 / 400)\n",
      "Train - Log-lik: -32370.37219;  CP: 91.67108;  KL_w: 0.00000;  KL_y: 1.48451;\n",
      "Valid - Log-lik: -24175.65012;  CP: 90.77437;  KL_w: 0.00000;  KL_y: 1.52653;\n",
      "Total Loss=Train: 32463.52781; Val: 24267.95105\n",
      "(Epoch 287 / 400)\n",
      "Train - Log-lik: -32599.57027;  CP: 91.73833;  KL_w: 0.00000;  KL_y: 1.48688;\n",
      "Valid - Log-lik: -24267.63005;  CP: 90.01686;  KL_w: 0.00000;  KL_y: 1.48069;\n",
      "Total Loss=Train: 32692.79545; Val: 24359.12758\n",
      "(Epoch 288 / 400)\n",
      "Train - Log-lik: -32548.26403;  CP: 90.56075;  KL_w: 0.00000;  KL_y: 1.46902;\n",
      "Valid - Log-lik: -24053.84318;  CP: 91.25598;  KL_w: 0.00000;  KL_y: 1.52082;\n",
      "Total Loss=Train: 32640.29369; Val: 24146.62000\n",
      "(Epoch 289 / 400)\n",
      "Train - Log-lik: -32460.55103;  CP: 90.58997;  KL_w: 0.00000;  KL_y: 1.46753;\n",
      "Valid - Log-lik: -23859.78242;  CP: 88.62547;  KL_w: 0.00000;  KL_y: 1.54173;\n",
      "Total Loss=Train: 32552.60864; Val: 23949.94965\n",
      "(Epoch 290 / 400)\n",
      "Train - Log-lik: -32473.11170;  CP: 89.60949;  KL_w: 0.00000;  KL_y: 1.47283;\n",
      "Valid - Log-lik: -24479.59190;  CP: 87.97799;  KL_w: 0.00000;  KL_y: 1.50041;\n",
      "Total Loss=Train: 32564.19415; Val: 24569.07028\n",
      "====================================================================================================\n",
      "Saving checkpoint to \"experiments/FMNIST_200epochs_dp0_lr001/checkpoints/GMVAE_1_00001_25_25_10_50_3_03_0001/step_290.pt\".\n",
      "====================================================================================================\n",
      "\n",
      "(Epoch 291 / 400)\n",
      "Train - Log-lik: -32550.06598;  CP: 90.20629;  KL_w: 0.00001;  KL_y: 1.47736;\n",
      "Valid - Log-lik: -23869.57633;  CP: 89.60394;  KL_w: 0.00000;  KL_y: 1.52946;\n",
      "Total Loss=Train: 32641.74968; Val: 23960.70971\n",
      "(Epoch 292 / 400)\n",
      "Train - Log-lik: -32449.12951;  CP: 90.39602;  KL_w: 0.00000;  KL_y: 1.47624;\n",
      "Valid - Log-lik: -24135.19510;  CP: 89.06736;  KL_w: 0.00000;  KL_y: 1.54360;\n",
      "Total Loss=Train: 32541.00179; Val: 24225.80609\n",
      "(Epoch 293 / 400)\n",
      "Train - Log-lik: -32444.56959;  CP: 89.84982;  KL_w: 0.00000;  KL_y: 1.48757;\n",
      "Valid - Log-lik: -24218.35664;  CP: 89.41612;  KL_w: 0.00000;  KL_y: 1.56692;\n",
      "Total Loss=Train: 32535.90700; Val: 24309.33972\n",
      "(Epoch 294 / 400)\n",
      "Train - Log-lik: -32415.34619;  CP: 89.92874;  KL_w: 0.00000;  KL_y: 1.48038;\n",
      "Valid - Log-lik: -24021.83441;  CP: 87.91123;  KL_w: 0.00001;  KL_y: 1.55845;\n",
      "Total Loss=Train: 32506.75525; Val: 24111.30408\n",
      "(Epoch 295 / 400)\n",
      "Train - Log-lik: -32468.13722;  CP: 89.57069;  KL_w: 0.00000;  KL_y: 1.48054;\n",
      "Valid - Log-lik: -24058.11627;  CP: 89.85509;  KL_w: 0.00000;  KL_y: 1.55067;\n",
      "Total Loss=Train: 32559.18848; Val: 24149.52202\n",
      "(Epoch 296 / 400)\n",
      "Train - Log-lik: -32497.60294;  CP: 91.24871;  KL_w: 0.00000;  KL_y: 1.48014;\n",
      "Valid - Log-lik: -24165.11220;  CP: 90.13438;  KL_w: 0.00001;  KL_y: 1.52768;\n",
      "Total Loss=Train: 32590.33180; Val: 24256.77427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 297 / 400)\n",
      "Train - Log-lik: -32428.54060;  CP: 91.10134;  KL_w: 0.00000;  KL_y: 1.47995;\n",
      "Valid - Log-lik: -24350.32854;  CP: 89.95817;  KL_w: 0.00000;  KL_y: 1.54160;\n",
      "Total Loss=Train: 32521.12183; Val: 24441.82831\n",
      "(Epoch 298 / 400)\n",
      "Train - Log-lik: -32441.98989;  CP: 90.14399;  KL_w: 0.00000;  KL_y: 1.47445;\n",
      "Valid - Log-lik: -23876.16511;  CP: 88.75777;  KL_w: 0.00000;  KL_y: 1.49656;\n",
      "Total Loss=Train: 32533.60834; Val: 23966.41939\n",
      "(Epoch 299 / 400)\n",
      "Train - Log-lik: -32482.52911;  CP: 88.97452;  KL_w: 0.00000;  KL_y: 1.47091;\n",
      "Valid - Log-lik: -24321.13517;  CP: 88.13527;  KL_w: 0.00000;  KL_y: 1.53299;\n",
      "Total Loss=Train: 32572.97454; Val: 24410.80346\n",
      "(Epoch 300 / 400)\n",
      "Train - Log-lik: -32493.31243;  CP: 88.71012;  KL_w: 0.00000;  KL_y: 1.47733;\n",
      "Valid - Log-lik: -24045.25073;  CP: 87.64849;  KL_w: 0.00000;  KL_y: 1.55521;\n",
      "Total Loss=Train: 32583.49999; Val: 24134.45435\n",
      "====================================================================================================\n",
      "Saving checkpoint to \"experiments/FMNIST_200epochs_dp0_lr001/checkpoints/GMVAE_1_00001_25_25_10_50_3_03_0001/step_300.pt\".\n",
      "====================================================================================================\n",
      "\n",
      "(Epoch 301 / 400)\n",
      "Train - Log-lik: -32541.93544;  CP: 88.86515;  KL_w: 0.00000;  KL_y: 1.48416;\n",
      "Valid - Log-lik: -24356.06508;  CP: 87.82596;  KL_w: 0.00000;  KL_y: 1.53770;\n",
      "Total Loss=Train: 32632.28477; Val: 24445.42873\n",
      "(Epoch 302 / 400)\n",
      "Train - Log-lik: -32564.55289;  CP: 88.14279;  KL_w: 0.00000;  KL_y: 1.47933;\n",
      "Valid - Log-lik: -24189.75695;  CP: 86.00499;  KL_w: 0.00000;  KL_y: 1.51677;\n",
      "Total Loss=Train: 32654.17503; Val: 24277.27871\n",
      "(Epoch 303 / 400)\n",
      "Train - Log-lik: -32431.02242;  CP: 88.49422;  KL_w: 0.00000;  KL_y: 1.47894;\n",
      "Valid - Log-lik: -24090.93158;  CP: 87.99598;  KL_w: 0.00000;  KL_y: 1.57774;\n",
      "Total Loss=Train: 32520.99547; Val: 24180.50531\n",
      "(Epoch 304 / 400)\n",
      "Train - Log-lik: -32454.29145;  CP: 89.17914;  KL_w: 0.00000;  KL_y: 1.47816;\n",
      "Valid - Log-lik: -23941.71576;  CP: 88.04107;  KL_w: 0.00000;  KL_y: 1.52069;\n",
      "Total Loss=Train: 32544.94876; Val: 24031.27751\n",
      "(Epoch 305 / 400)\n",
      "Train - Log-lik: -32425.54325;  CP: 89.97668;  KL_w: 0.00000;  KL_y: 1.47490;\n",
      "Valid - Log-lik: -24058.39343;  CP: 90.27098;  KL_w: 0.00000;  KL_y: 1.56345;\n",
      "Total Loss=Train: 32516.99487; Val: 24150.22788\n",
      "(Epoch 306 / 400)\n",
      "Train - Log-lik: -32425.21874;  CP: 90.05374;  KL_w: 0.00000;  KL_y: 1.48677;\n",
      "Valid - Log-lik: -24068.44443;  CP: 88.83632;  KL_w: 0.00001;  KL_y: 1.52648;\n",
      "Total Loss=Train: 32516.75932; Val: 24158.80720\n",
      "(Epoch 307 / 400)\n",
      "Train - Log-lik: -32332.93626;  CP: 89.84251;  KL_w: 0.00000;  KL_y: 1.47954;\n",
      "Valid - Log-lik: -24088.42629;  CP: 88.77845;  KL_w: 0.00000;  KL_y: 1.58254;\n",
      "Total Loss=Train: 32424.25824; Val: 24178.78728\n",
      "(Epoch 308 / 400)\n",
      "Train - Log-lik: -32399.69947;  CP: 90.43455;  KL_w: 0.00000;  KL_y: 1.48339;\n",
      "Valid - Log-lik: -24372.47428;  CP: 89.62931;  KL_w: 0.00000;  KL_y: 1.52907;\n",
      "Total Loss=Train: 32491.61737; Val: 24463.63268\n",
      "(Epoch 309 / 400)\n",
      "Train - Log-lik: -32459.56164;  CP: 89.13798;  KL_w: 0.00000;  KL_y: 1.48424;\n",
      "Valid - Log-lik: -24472.08882;  CP: 88.47362;  KL_w: 0.00000;  KL_y: 1.55166;\n",
      "Total Loss=Train: 32550.18372; Val: 24562.11412\n",
      "(Epoch 310 / 400)\n",
      "Train - Log-lik: -32269.32952;  CP: 87.00883;  KL_w: 0.00000;  KL_y: 1.48609;\n",
      "Valid - Log-lik: -24003.27606;  CP: 87.89258;  KL_w: 0.00001;  KL_y: 1.55309;\n",
      "Total Loss=Train: 32357.82437; Val: 24092.72175\n",
      "====================================================================================================\n",
      "Saving checkpoint to \"experiments/FMNIST_200epochs_dp0_lr001/checkpoints/GMVAE_1_00001_25_25_10_50_3_03_0001/step_310.pt\".\n",
      "====================================================================================================\n",
      "\n",
      "(Epoch 311 / 400)\n",
      "Train - Log-lik: -32462.84445;  CP: 88.03217;  KL_w: 0.00000;  KL_y: 1.48416;\n",
      "Valid - Log-lik: -24266.23925;  CP: 86.17085;  KL_w: 0.00001;  KL_y: 1.53319;\n",
      "Total Loss=Train: 32552.36068; Val: 24353.94329\n",
      "(Epoch 312 / 400)\n",
      "Train - Log-lik: -32410.93304;  CP: 87.16740;  KL_w: 0.00000;  KL_y: 1.48157;\n",
      "Valid - Log-lik: -24077.45017;  CP: 88.14815;  KL_w: 0.00000;  KL_y: 1.54141;\n",
      "Total Loss=Train: 32499.58201; Val: 24167.13974\n",
      "(Epoch 313 / 400)\n",
      "Train - Log-lik: -32398.03910;  CP: 89.06606;  KL_w: 0.00000;  KL_y: 1.48961;\n",
      "Valid - Log-lik: -24295.17561;  CP: 88.87183;  KL_w: 0.00000;  KL_y: 1.54061;\n",
      "Total Loss=Train: 32488.59480; Val: 24385.58803\n",
      "(Epoch 314 / 400)\n",
      "Train - Log-lik: -32449.71371;  CP: 89.10333;  KL_w: 0.00000;  KL_y: 1.48696;\n",
      "Valid - Log-lik: -24236.20374;  CP: 88.49457;  KL_w: 0.00001;  KL_y: 1.59248;\n",
      "Total Loss=Train: 32540.30410; Val: 24326.29079\n",
      "(Epoch 315 / 400)\n",
      "Train - Log-lik: -32329.62241;  CP: 88.40584;  KL_w: 0.00000;  KL_y: 1.48781;\n",
      "Valid - Log-lik: -24174.24693;  CP: 88.18706;  KL_w: 0.00000;  KL_y: 1.52748;\n",
      "Total Loss=Train: 32419.51613; Val: 24263.96143\n",
      "(Epoch 316 / 400)\n",
      "Train - Log-lik: -32469.89433;  CP: 89.12979;  KL_w: 0.00000;  KL_y: 1.48629;\n",
      "Valid - Log-lik: -24054.77846;  CP: 89.15762;  KL_w: 0.00000;  KL_y: 1.55392;\n",
      "Total Loss=Train: 32560.51040; Val: 24145.49003\n",
      "(Epoch 317 / 400)\n",
      "Train - Log-lik: -32275.84572;  CP: 89.89363;  KL_w: 0.00000;  KL_y: 1.48239;\n",
      "Valid - Log-lik: -24263.95335;  CP: 89.45923;  KL_w: 0.00000;  KL_y: 1.56318;\n",
      "Total Loss=Train: 32367.22163; Val: 24354.97579\n",
      "(Epoch 318 / 400)\n",
      "Train - Log-lik: -32416.50369;  CP: 89.90046;  KL_w: 0.00000;  KL_y: 1.47701;\n",
      "Valid - Log-lik: -24054.53737;  CP: 89.11190;  KL_w: 0.00000;  KL_y: 1.52146;\n",
      "Total Loss=Train: 32507.88120; Val: 24145.17076\n",
      "(Epoch 319 / 400)\n",
      "Train - Log-lik: -32549.69178;  CP: 88.33632;  KL_w: 0.00000;  KL_y: 1.48518;\n",
      "Valid - Log-lik: -24022.27103;  CP: 86.56240;  KL_w: 0.00000;  KL_y: 1.53276;\n",
      "Total Loss=Train: 32639.51329; Val: 24110.36621\n",
      "(Epoch 320 / 400)\n",
      "Train - Log-lik: -32306.45546;  CP: 88.68918;  KL_w: 0.00000;  KL_y: 1.48861;\n",
      "Valid - Log-lik: -24096.47703;  CP: 88.95462;  KL_w: 0.00000;  KL_y: 1.51133;\n",
      "Total Loss=Train: 32396.63329; Val: 24186.94305\n",
      "====================================================================================================\n",
      "Saving checkpoint to \"experiments/FMNIST_200epochs_dp0_lr001/checkpoints/GMVAE_1_00001_25_25_10_50_3_03_0001/step_320.pt\".\n",
      "====================================================================================================\n",
      "\n",
      "(Epoch 321 / 400)\n",
      "Train - Log-lik: -32470.27883;  CP: 89.01499;  KL_w: 0.00000;  KL_y: 1.48359;\n",
      "Valid - Log-lik: -24040.87002;  CP: 88.02142;  KL_w: 0.00000;  KL_y: 1.51025;\n",
      "Total Loss=Train: 32560.77742; Val: 24130.40170\n",
      "(Epoch 322 / 400)\n",
      "Train - Log-lik: -32460.37608;  CP: 89.45281;  KL_w: 0.00000;  KL_y: 1.49282;\n",
      "Valid - Log-lik: -24180.91109;  CP: 88.81436;  KL_w: 0.00000;  KL_y: 1.52632;\n",
      "Total Loss=Train: 32551.32159; Val: 24271.25180\n",
      "(Epoch 323 / 400)\n",
      "Train - Log-lik: -32456.95154;  CP: 88.64398;  KL_w: 0.00000;  KL_y: 1.48705;\n",
      "Valid - Log-lik: -24377.59502;  CP: 87.09880;  KL_w: 0.00001;  KL_y: 1.52313;\n",
      "Total Loss=Train: 32547.08247; Val: 24466.21697\n",
      "(Epoch 324 / 400)\n",
      "Train - Log-lik: -32558.44685;  CP: 88.95813;  KL_w: 0.00000;  KL_y: 1.49917;\n",
      "Valid - Log-lik: -24230.96209;  CP: 89.03936;  KL_w: 0.00000;  KL_y: 1.53606;\n",
      "Total Loss=Train: 32648.90427; Val: 24321.53747\n",
      "(Epoch 325 / 400)\n",
      "Train - Log-lik: -32431.79818;  CP: 89.03238;  KL_w: 0.00000;  KL_y: 1.50614;\n",
      "Valid - Log-lik: -24604.83434;  CP: 88.20047;  KL_w: 0.00000;  KL_y: 1.57872;\n",
      "Total Loss=Train: 32522.33672; Val: 24694.61356\n",
      "(Epoch 326 / 400)\n",
      "Train - Log-lik: -32389.71844;  CP: 88.86404;  KL_w: 0.00000;  KL_y: 1.49685;\n",
      "Valid - Log-lik: -24541.09821;  CP: 87.98703;  KL_w: 0.00000;  KL_y: 1.52150;\n",
      "Total Loss=Train: 32480.07923; Val: 24630.60675\n",
      "(Epoch 327 / 400)\n",
      "Train - Log-lik: -32390.05848;  CP: 88.82413;  KL_w: 0.00000;  KL_y: 1.48722;\n",
      "Valid - Log-lik: -24021.10180;  CP: 87.42306;  KL_w: 0.00000;  KL_y: 1.53863;\n",
      "Total Loss=Train: 32480.36990; Val: 24110.06351\n",
      "(Epoch 328 / 400)\n",
      "Train - Log-lik: -32371.80724;  CP: 88.04257;  KL_w: 0.00000;  KL_y: 1.49391;\n",
      "Valid - Log-lik: -24709.14390;  CP: 87.25760;  KL_w: 0.00000;  KL_y: 1.51142;\n",
      "Total Loss=Train: 32461.34368; Val: 24797.91292\n",
      "(Epoch 329 / 400)\n",
      "Train - Log-lik: -32370.80216;  CP: 87.96851;  KL_w: 0.00000;  KL_y: 1.48516;\n",
      "Valid - Log-lik: -24005.03093;  CP: 87.46023;  KL_w: 0.00001;  KL_y: 1.54485;\n",
      "Total Loss=Train: 32460.25584; Val: 24094.03606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 330 / 400)\n",
      "Train - Log-lik: -32354.09412;  CP: 87.00433;  KL_w: 0.00000;  KL_y: 1.48807;\n",
      "Valid - Log-lik: -23927.77192;  CP: 86.44953;  KL_w: 0.00000;  KL_y: 1.49146;\n",
      "Total Loss=Train: 32442.58655; Val: 24015.71289\n",
      "====================================================================================================\n",
      "Saving checkpoint to \"experiments/FMNIST_200epochs_dp0_lr001/checkpoints/GMVAE_1_00001_25_25_10_50_3_03_0001/step_330.pt\".\n",
      "====================================================================================================\n",
      "\n",
      "(Epoch 331 / 400)\n",
      "Train - Log-lik: -32299.60451;  CP: 87.81912;  KL_w: 0.00000;  KL_y: 1.48712;\n",
      "Valid - Log-lik: -23803.76506;  CP: 88.13517;  KL_w: 0.00000;  KL_y: 1.56543;\n",
      "Total Loss=Train: 32388.91083; Val: 23893.46569\n",
      "(Epoch 332 / 400)\n",
      "Train - Log-lik: -32376.74803;  CP: 88.70439;  KL_w: 0.00000;  KL_y: 1.49388;\n",
      "Valid - Log-lik: -23940.80135;  CP: 87.37497;  KL_w: 0.00000;  KL_y: 1.54100;\n",
      "Total Loss=Train: 32466.94621; Val: 24029.71727\n",
      "(Epoch 333 / 400)\n",
      "Train - Log-lik: -32392.09999;  CP: 87.89958;  KL_w: 0.00000;  KL_y: 1.49402;\n",
      "Valid - Log-lik: -24097.44638;  CP: 85.86557;  KL_w: 0.00001;  KL_y: 1.51786;\n",
      "Total Loss=Train: 32481.49352; Val: 24184.82983\n",
      "(Epoch 334 / 400)\n",
      "Train - Log-lik: -32284.19032;  CP: 86.80889;  KL_w: 0.00000;  KL_y: 1.49487;\n",
      "Valid - Log-lik: -24036.01641;  CP: 86.69710;  KL_w: 0.00000;  KL_y: 1.55038;\n",
      "Total Loss=Train: 32372.49409; Val: 24124.26386\n",
      "(Epoch 335 / 400)\n",
      "Train - Log-lik: -32491.96032;  CP: 87.99842;  KL_w: 0.00000;  KL_y: 1.48754;\n",
      "Valid - Log-lik: -24013.28229;  CP: 86.41589;  KL_w: 0.00001;  KL_y: 1.55669;\n",
      "Total Loss=Train: 32581.44631; Val: 24101.25492\n",
      "(Epoch 336 / 400)\n",
      "Train - Log-lik: -32289.78926;  CP: 87.61258;  KL_w: 0.00001;  KL_y: 1.49589;\n",
      "Valid - Log-lik: -24599.84547;  CP: 86.78688;  KL_w: 0.00000;  KL_y: 1.55789;\n",
      "Total Loss=Train: 32378.89770; Val: 24688.19026\n",
      "(Epoch 337 / 400)\n",
      "Train - Log-lik: -32381.06436;  CP: 87.74106;  KL_w: 0.00000;  KL_y: 1.50306;\n",
      "Valid - Log-lik: -23987.28960;  CP: 87.20160;  KL_w: 0.00001;  KL_y: 1.56135;\n",
      "Total Loss=Train: 32470.30862; Val: 24076.05253\n",
      "(Epoch 338 / 400)\n",
      "Train - Log-lik: -32375.06292;  CP: 87.49350;  KL_w: 0.00000;  KL_y: 1.50489;\n",
      "Valid - Log-lik: -24038.87332;  CP: 85.62685;  KL_w: 0.00000;  KL_y: 1.52754;\n",
      "Total Loss=Train: 32464.06120; Val: 24126.02768\n",
      "(Epoch 339 / 400)\n",
      "Train - Log-lik: -32484.05301;  CP: 85.99845;  KL_w: 0.00000;  KL_y: 1.49632;\n",
      "Valid - Log-lik: -24453.40972;  CP: 85.32234;  KL_w: 0.00000;  KL_y: 1.55646;\n",
      "Total Loss=Train: 32571.54775; Val: 24540.28854\n",
      "(Epoch 340 / 400)\n",
      "Train - Log-lik: -32439.01918;  CP: 86.68226;  KL_w: 0.00000;  KL_y: 1.50248;\n",
      "Valid - Log-lik: -23886.20301;  CP: 85.90493;  KL_w: 0.00002;  KL_y: 1.56262;\n",
      "Total Loss=Train: 32527.20392; Val: 23973.67054\n",
      "====================================================================================================\n",
      "Saving checkpoint to \"experiments/FMNIST_200epochs_dp0_lr001/checkpoints/GMVAE_1_00001_25_25_10_50_3_03_0001/step_340.pt\".\n",
      "====================================================================================================\n",
      "\n",
      "(Epoch 341 / 400)\n",
      "Train - Log-lik: -32366.85239;  CP: 87.21334;  KL_w: 0.00001;  KL_y: 1.50443;\n",
      "Valid - Log-lik: -24421.11294;  CP: 86.11070;  KL_w: 0.00001;  KL_y: 1.55364;\n",
      "Total Loss=Train: 32455.57019; Val: 24508.77728\n",
      "(Epoch 342 / 400)\n",
      "Train - Log-lik: -32418.01601;  CP: 86.46274;  KL_w: 0.00000;  KL_y: 1.49468;\n",
      "Valid - Log-lik: -24105.75572;  CP: 85.88622;  KL_w: 0.00000;  KL_y: 1.54055;\n",
      "Total Loss=Train: 32505.97341; Val: 24193.18250\n",
      "(Epoch 343 / 400)\n",
      "Train - Log-lik: -32234.55109;  CP: 87.09013;  KL_w: 0.00000;  KL_y: 1.50425;\n",
      "Valid - Log-lik: -24272.90618;  CP: 87.28862;  KL_w: 0.00000;  KL_y: 1.55677;\n",
      "Total Loss=Train: 32323.14539; Val: 24361.75154\n",
      "(Epoch 344 / 400)\n",
      "Train - Log-lik: -32429.73318;  CP: 87.21970;  KL_w: 0.00000;  KL_y: 1.50097;\n",
      "Valid - Log-lik: -24239.53443;  CP: 85.28281;  KL_w: 0.00001;  KL_y: 1.54969;\n",
      "Total Loss=Train: 32518.45370; Val: 24326.36695\n",
      "(Epoch 345 / 400)\n",
      "Train - Log-lik: -32387.90645;  CP: 86.94075;  KL_w: 0.00000;  KL_y: 1.50448;\n",
      "Valid - Log-lik: -23951.62655;  CP: 87.92947;  KL_w: 0.00000;  KL_y: 1.53804;\n",
      "Total Loss=Train: 32476.35171; Val: 24041.09407\n",
      "(Epoch 346 / 400)\n",
      "Train - Log-lik: -32348.22870;  CP: 87.53959;  KL_w: 0.00000;  KL_y: 1.50220;\n",
      "Valid - Log-lik: -24082.14428;  CP: 85.94597;  KL_w: 0.00000;  KL_y: 1.54028;\n",
      "Total Loss=Train: 32437.27056; Val: 24169.63052\n",
      "(Epoch 347 / 400)\n",
      "Train - Log-lik: -32331.68957;  CP: 86.91462;  KL_w: 0.00000;  KL_y: 1.49754;\n",
      "Valid - Log-lik: -24022.33077;  CP: 87.05866;  KL_w: 0.00000;  KL_y: 1.58153;\n",
      "Total Loss=Train: 32420.10179; Val: 24110.97092\n",
      "(Epoch 348 / 400)\n",
      "Train - Log-lik: -32394.88199;  CP: 86.04533;  KL_w: 0.00000;  KL_y: 1.50447;\n",
      "Valid - Log-lik: -24025.37818;  CP: 86.15373;  KL_w: 0.00000;  KL_y: 1.57417;\n",
      "Total Loss=Train: 32482.43196; Val: 24113.10608\n",
      "(Epoch 349 / 400)\n",
      "Train - Log-lik: -32440.54400;  CP: 86.78824;  KL_w: 0.00000;  KL_y: 1.50480;\n",
      "Valid - Log-lik: -24137.45137;  CP: 86.40441;  KL_w: 0.00000;  KL_y: 1.56170;\n",
      "Total Loss=Train: 32528.83704; Val: 24225.41751\n",
      "(Epoch 350 / 400)\n",
      "Train - Log-lik: -32390.49692;  CP: 87.05665;  KL_w: 0.00000;  KL_y: 1.51201;\n",
      "Valid - Log-lik: -24040.21207;  CP: 85.75092;  KL_w: 0.00001;  KL_y: 1.58036;\n",
      "Total Loss=Train: 32479.06555; Val: 24127.54335\n",
      "====================================================================================================\n",
      "Saving checkpoint to \"experiments/FMNIST_200epochs_dp0_lr001/checkpoints/GMVAE_1_00001_25_25_10_50_3_03_0001/step_350.pt\".\n",
      "====================================================================================================\n",
      "\n",
      "(Epoch 351 / 400)\n",
      "Train - Log-lik: -32350.20180;  CP: 86.36372;  KL_w: 0.00000;  KL_y: 1.50904;\n",
      "Valid - Log-lik: -24662.98622;  CP: 85.80562;  KL_w: 0.00000;  KL_y: 1.50147;\n",
      "Total Loss=Train: 32438.07449; Val: 24750.29334\n",
      "(Epoch 352 / 400)\n",
      "Train - Log-lik: -32289.18305;  CP: 85.52524;  KL_w: 0.00000;  KL_y: 1.50798;\n",
      "Valid - Log-lik: -24122.66803;  CP: 84.75810;  KL_w: 0.00000;  KL_y: 1.55465;\n",
      "Total Loss=Train: 32376.21623; Val: 24208.98079\n",
      "(Epoch 353 / 400)\n",
      "Train - Log-lik: -32347.04172;  CP: 84.85074;  KL_w: 0.00000;  KL_y: 1.51014;\n",
      "Valid - Log-lik: -24019.10547;  CP: 84.66058;  KL_w: 0.00000;  KL_y: 1.51989;\n",
      "Total Loss=Train: 32433.40254; Val: 24105.28599\n",
      "(Epoch 354 / 400)\n",
      "Train - Log-lik: -32352.35582;  CP: 86.80722;  KL_w: 0.00000;  KL_y: 1.50694;\n",
      "Valid - Log-lik: -24142.96307;  CP: 86.86759;  KL_w: 0.00000;  KL_y: 1.53066;\n",
      "Total Loss=Train: 32440.67000; Val: 24231.36132\n",
      "(Epoch 355 / 400)\n",
      "Train - Log-lik: -32476.41527;  CP: 86.78995;  KL_w: 0.00000;  KL_y: 1.51631;\n",
      "Valid - Log-lik: -23775.98381;  CP: 85.48449;  KL_w: 0.00000;  KL_y: 1.57319;\n",
      "Total Loss=Train: 32564.72171; Val: 23863.04156\n",
      "(Epoch 356 / 400)\n",
      "Train - Log-lik: -32434.76535;  CP: 86.88724;  KL_w: 0.00000;  KL_y: 1.51243;\n",
      "Valid - Log-lik: -24249.01838;  CP: 86.15329;  KL_w: 0.00000;  KL_y: 1.58747;\n",
      "Total Loss=Train: 32523.16503; Val: 24336.75919\n",
      "(Epoch 357 / 400)\n",
      "Train - Log-lik: -32378.44277;  CP: 87.13834;  KL_w: 0.00000;  KL_y: 1.51046;\n",
      "Valid - Log-lik: -24178.47455;  CP: 86.62325;  KL_w: 0.00000;  KL_y: 1.55549;\n",
      "Total Loss=Train: 32467.09156; Val: 24266.65326\n",
      "(Epoch 358 / 400)\n",
      "Train - Log-lik: -32478.57811;  CP: 86.39071;  KL_w: 0.00001;  KL_y: 1.52702;\n",
      "Valid - Log-lik: -24048.41804;  CP: 84.16750;  KL_w: 0.00000;  KL_y: 1.59578;\n",
      "Total Loss=Train: 32566.49571; Val: 24134.18133\n",
      "(Epoch 359 / 400)\n",
      "Train - Log-lik: -32357.45946;  CP: 86.95996;  KL_w: 0.00000;  KL_y: 1.52432;\n",
      "Valid - Log-lik: -24112.69879;  CP: 85.98187;  KL_w: 0.00000;  KL_y: 1.57900;\n",
      "Total Loss=Train: 32445.94359; Val: 24200.25964\n",
      "(Epoch 360 / 400)\n",
      "Train - Log-lik: -32364.99767;  CP: 86.05463;  KL_w: 0.00000;  KL_y: 1.52255;\n",
      "Valid - Log-lik: -24328.90811;  CP: 86.47729;  KL_w: 0.00000;  KL_y: 1.56612;\n",
      "Total Loss=Train: 32452.57479; Val: 24416.95154\n",
      "====================================================================================================\n",
      "Saving checkpoint to \"experiments/FMNIST_200epochs_dp0_lr001/checkpoints/GMVAE_1_00001_25_25_10_50_3_03_0001/step_360.pt\".\n",
      "====================================================================================================\n",
      "\n",
      "(Epoch 361 / 400)\n",
      "Train - Log-lik: -32487.77026;  CP: 85.64780;  KL_w: 0.00000;  KL_y: 1.52437;\n",
      "Valid - Log-lik: -23941.70666;  CP: 84.49242;  KL_w: 0.00000;  KL_y: 1.59257;\n",
      "Total Loss=Train: 32574.94256; Val: 24027.79166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 362 / 400)\n",
      "Train - Log-lik: -32333.28291;  CP: 85.77353;  KL_w: 0.00000;  KL_y: 1.52821;\n",
      "Valid - Log-lik: -24014.86626;  CP: 85.88484;  KL_w: 0.00000;  KL_y: 1.55409;\n",
      "Total Loss=Train: 32420.58456; Val: 24102.30516\n",
      "(Epoch 363 / 400)\n",
      "Train - Log-lik: -32348.29726;  CP: 85.58263;  KL_w: 0.00000;  KL_y: 1.51928;\n",
      "Valid - Log-lik: -23915.66323;  CP: 85.06009;  KL_w: 0.00000;  KL_y: 1.56175;\n",
      "Total Loss=Train: 32435.39920; Val: 24002.28508\n",
      "(Epoch 364 / 400)\n",
      "Train - Log-lik: -32349.65020;  CP: 86.05990;  KL_w: 0.00000;  KL_y: 1.51426;\n",
      "Valid - Log-lik: -24402.16627;  CP: 85.47127;  KL_w: 0.00001;  KL_y: 1.57654;\n",
      "Total Loss=Train: 32437.22450; Val: 24489.21413\n",
      "(Epoch 365 / 400)\n",
      "Train - Log-lik: -32479.81899;  CP: 85.48958;  KL_w: 0.00000;  KL_y: 1.52076;\n",
      "Valid - Log-lik: -24212.63137;  CP: 84.49838;  KL_w: 0.00000;  KL_y: 1.54782;\n",
      "Total Loss=Train: 32566.82940; Val: 24298.67753\n",
      "(Epoch 366 / 400)\n",
      "Train - Log-lik: -32434.60793;  CP: 85.39272;  KL_w: 0.00000;  KL_y: 1.52037;\n",
      "Valid - Log-lik: -24002.95422;  CP: 85.95802;  KL_w: 0.00000;  KL_y: 1.55634;\n",
      "Total Loss=Train: 32521.52116; Val: 24090.46857\n",
      "(Epoch 367 / 400)\n",
      "Train - Log-lik: -32325.82074;  CP: 86.38716;  KL_w: 0.00000;  KL_y: 1.52196;\n",
      "Valid - Log-lik: -23881.32810;  CP: 85.43621;  KL_w: 0.00000;  KL_y: 1.53993;\n",
      "Total Loss=Train: 32413.72979; Val: 23968.30430\n",
      "(Epoch 368 / 400)\n",
      "Train - Log-lik: -32301.49142;  CP: 85.19154;  KL_w: 0.00000;  KL_y: 1.52742;\n",
      "Valid - Log-lik: -24421.75521;  CP: 84.92646;  KL_w: 0.00000;  KL_y: 1.51778;\n",
      "Total Loss=Train: 32388.21028; Val: 24508.19948\n",
      "(Epoch 369 / 400)\n",
      "Train - Log-lik: -32392.97720;  CP: 84.48994;  KL_w: 0.00000;  KL_y: 1.52547;\n",
      "Valid - Log-lik: -24006.93764;  CP: 83.22792;  KL_w: 0.00001;  KL_y: 1.55694;\n",
      "Total Loss=Train: 32478.99261; Val: 24091.72255\n",
      "(Epoch 370 / 400)\n",
      "Train - Log-lik: -32281.58915;  CP: 84.45610;  KL_w: 0.00000;  KL_y: 1.52506;\n",
      "Valid - Log-lik: -24490.69694;  CP: 83.87069;  KL_w: 0.00000;  KL_y: 1.56709;\n",
      "Total Loss=Train: 32367.57027; Val: 24576.13473\n",
      "====================================================================================================\n",
      "Saving checkpoint to \"experiments/FMNIST_200epochs_dp0_lr001/checkpoints/GMVAE_1_00001_25_25_10_50_3_03_0001/step_370.pt\".\n",
      "====================================================================================================\n",
      "\n",
      "(Epoch 371 / 400)\n",
      "Train - Log-lik: -32362.73119;  CP: 85.01471;  KL_w: 0.00000;  KL_y: 1.52457;\n",
      "Valid - Log-lik: -24756.71084;  CP: 84.44329;  KL_w: 0.00000;  KL_y: 1.59761;\n",
      "Total Loss=Train: 32449.27044; Val: 24842.75176\n",
      "(Epoch 372 / 400)\n",
      "Train - Log-lik: -32486.79533;  CP: 84.13991;  KL_w: 0.00000;  KL_y: 1.53143;\n",
      "Valid - Log-lik: -23799.34946;  CP: 83.55866;  KL_w: 0.00000;  KL_y: 1.57659;\n",
      "Total Loss=Train: 32572.46670; Val: 23884.48468\n",
      "(Epoch 373 / 400)\n",
      "Train - Log-lik: -32453.66996;  CP: 84.83186;  KL_w: 0.00000;  KL_y: 1.53649;\n",
      "Valid - Log-lik: -24281.13767;  CP: 85.26372;  KL_w: 0.00000;  KL_y: 1.59582;\n",
      "Total Loss=Train: 32540.03847; Val: 24367.99723\n",
      "(Epoch 374 / 400)\n",
      "Train - Log-lik: -32356.00026;  CP: 84.64708;  KL_w: 0.00000;  KL_y: 1.53668;\n",
      "Valid - Log-lik: -23967.78385;  CP: 83.31898;  KL_w: 0.00000;  KL_y: 1.57231;\n",
      "Total Loss=Train: 32442.18395; Val: 24052.67515\n",
      "(Epoch 375 / 400)\n",
      "Train - Log-lik: -32354.16179;  CP: 84.14086;  KL_w: 0.00000;  KL_y: 1.54075;\n",
      "Valid - Log-lik: -23817.36472;  CP: 83.72312;  KL_w: 0.00000;  KL_y: 1.58434;\n",
      "Total Loss=Train: 32439.84339; Val: 23902.67213\n",
      "(Epoch 376 / 400)\n",
      "Train - Log-lik: -32234.39735;  CP: 84.63489;  KL_w: 0.00000;  KL_y: 1.54859;\n",
      "Valid - Log-lik: -23851.26222;  CP: 83.68249;  KL_w: 0.00000;  KL_y: 1.60973;\n",
      "Total Loss=Train: 32320.58069; Val: 23936.55447\n",
      "(Epoch 377 / 400)\n",
      "Train - Log-lik: -32353.04576;  CP: 84.81140;  KL_w: 0.00001;  KL_y: 1.54796;\n",
      "Valid - Log-lik: -24018.19111;  CP: 83.86050;  KL_w: 0.00000;  KL_y: 1.58303;\n",
      "Total Loss=Train: 32439.40515; Val: 24103.63462\n",
      "(Epoch 378 / 400)\n",
      "Train - Log-lik: -32377.10235;  CP: 84.68566;  KL_w: 0.00000;  KL_y: 1.55326;\n",
      "Valid - Log-lik: -24027.26836;  CP: 83.29075;  KL_w: 0.00000;  KL_y: 1.58808;\n",
      "Total Loss=Train: 32463.34131; Val: 24112.14723\n",
      "(Epoch 379 / 400)\n",
      "Train - Log-lik: -32384.63366;  CP: 84.14809;  KL_w: 0.00000;  KL_y: 1.54614;\n",
      "Valid - Log-lik: -24129.05360;  CP: 83.08118;  KL_w: 0.00000;  KL_y: 1.56691;\n",
      "Total Loss=Train: 32470.32795; Val: 24213.70167\n",
      "(Epoch 380 / 400)\n",
      "Train - Log-lik: -32329.17786;  CP: 82.10713;  KL_w: 0.00000;  KL_y: 1.54513;\n",
      "Valid - Log-lik: -24063.30058;  CP: 81.45817;  KL_w: 0.00000;  KL_y: 1.52009;\n",
      "Total Loss=Train: 32412.83011; Val: 24146.27886\n",
      "====================================================================================================\n",
      "Saving checkpoint to \"experiments/FMNIST_200epochs_dp0_lr001/checkpoints/GMVAE_1_00001_25_25_10_50_3_03_0001/step_380.pt\".\n",
      "====================================================================================================\n",
      "\n",
      "(Epoch 381 / 400)\n",
      "Train - Log-lik: -32282.90880;  CP: 83.21795;  KL_w: 0.00000;  KL_y: 1.54368;\n",
      "Valid - Log-lik: -23940.38979;  CP: 82.51498;  KL_w: 0.00000;  KL_y: 1.58762;\n",
      "Total Loss=Train: 32367.67048; Val: 24024.49241\n",
      "(Epoch 382 / 400)\n",
      "Train - Log-lik: -32408.76504;  CP: 83.16789;  KL_w: 0.00000;  KL_y: 1.54391;\n",
      "Valid - Log-lik: -24531.00003;  CP: 83.08824;  KL_w: 0.00000;  KL_y: 1.57808;\n",
      "Total Loss=Train: 32493.47682; Val: 24615.66635\n",
      "(Epoch 383 / 400)\n",
      "Train - Log-lik: -32365.65066;  CP: 83.16558;  KL_w: 0.00000;  KL_y: 1.56300;\n",
      "Valid - Log-lik: -24146.32393;  CP: 82.15150;  KL_w: 0.00000;  KL_y: 1.59007;\n",
      "Total Loss=Train: 32450.37928; Val: 24230.06552\n",
      "(Epoch 384 / 400)\n",
      "Train - Log-lik: -32235.94821;  CP: 84.19136;  KL_w: 0.00000;  KL_y: 1.55768;\n",
      "Valid - Log-lik: -23963.60232;  CP: 83.52751;  KL_w: 0.00000;  KL_y: 1.61356;\n",
      "Total Loss=Train: 32321.69727; Val: 24048.74347\n",
      "(Epoch 385 / 400)\n",
      "Train - Log-lik: -32311.39257;  CP: 84.31640;  KL_w: 0.00000;  KL_y: 1.56963;\n",
      "Valid - Log-lik: -23992.74999;  CP: 83.29837;  KL_w: 0.00000;  KL_y: 1.61896;\n",
      "Total Loss=Train: 32397.27851; Val: 24077.66727\n",
      "(Epoch 386 / 400)\n",
      "Train - Log-lik: -32185.64661;  CP: 84.43307;  KL_w: 0.00001;  KL_y: 1.57024;\n",
      "Valid - Log-lik: -23893.09392;  CP: 83.53852;  KL_w: 0.00000;  KL_y: 1.65212;\n",
      "Total Loss=Train: 32271.64988; Val: 23978.28460\n",
      "(Epoch 387 / 400)\n",
      "Train - Log-lik: -32232.15094;  CP: 83.56632;  KL_w: 0.00000;  KL_y: 1.57618;\n",
      "Valid - Log-lik: -23913.58454;  CP: 83.49887;  KL_w: 0.00000;  KL_y: 1.65523;\n",
      "Total Loss=Train: 32317.29339; Val: 23998.73863\n",
      "(Epoch 388 / 400)\n",
      "Train - Log-lik: -32359.65510;  CP: 83.95701;  KL_w: 0.00000;  KL_y: 1.58693;\n",
      "Valid - Log-lik: -24323.64099;  CP: 84.17944;  KL_w: 0.00000;  KL_y: 1.60605;\n",
      "Total Loss=Train: 32445.19897; Val: 24409.42650\n",
      "(Epoch 389 / 400)\n",
      "Train - Log-lik: -32352.17953;  CP: 83.70126;  KL_w: 0.00000;  KL_y: 1.59330;\n",
      "Valid - Log-lik: -24136.84099;  CP: 83.44042;  KL_w: 0.00000;  KL_y: 1.67102;\n",
      "Total Loss=Train: 32437.47421; Val: 24221.95241\n",
      "(Epoch 390 / 400)\n",
      "Train - Log-lik: -32418.84353;  CP: 84.62451;  KL_w: 0.00000;  KL_y: 1.60429;\n",
      "Valid - Log-lik: -24193.83666;  CP: 82.93435;  KL_w: 0.00000;  KL_y: 1.63149;\n",
      "Total Loss=Train: 32505.07241; Val: 24278.40252\n",
      "====================================================================================================\n",
      "Saving checkpoint to \"experiments/FMNIST_200epochs_dp0_lr001/checkpoints/GMVAE_1_00001_25_25_10_50_3_03_0001/step_390.pt\".\n",
      "====================================================================================================\n",
      "\n",
      "(Epoch 391 / 400)\n",
      "Train - Log-lik: -32357.62137;  CP: 83.88066;  KL_w: 0.00000;  KL_y: 1.59831;\n",
      "Valid - Log-lik: -24081.93151;  CP: 83.03758;  KL_w: 0.00000;  KL_y: 1.65589;\n",
      "Total Loss=Train: 32443.10036; Val: 24166.62493\n",
      "(Epoch 392 / 400)\n",
      "Train - Log-lik: -32383.09417;  CP: 84.44441;  KL_w: 0.00000;  KL_y: 1.60508;\n",
      "Valid - Log-lik: -23985.76061;  CP: 83.10476;  KL_w: 0.00000;  KL_y: 1.63941;\n",
      "Total Loss=Train: 32469.14372; Val: 24070.50480\n",
      "(Epoch 393 / 400)\n",
      "Train - Log-lik: -32215.53482;  CP: 84.58039;  KL_w: 0.00000;  KL_y: 1.60748;\n",
      "Valid - Log-lik: -23916.13892;  CP: 83.43187;  KL_w: 0.00000;  KL_y: 1.65815;\n",
      "Total Loss=Train: 32301.72257; Val: 24001.22892\n",
      "(Epoch 394 / 400)\n",
      "Train - Log-lik: -32244.03366;  CP: 84.05000;  KL_w: 0.00000;  KL_y: 1.60811;\n",
      "Valid - Log-lik: -24093.69157;  CP: 83.69529;  KL_w: 0.00000;  KL_y: 1.62152;\n",
      "Total Loss=Train: 32329.69169; Val: 24179.00841\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 395 / 400)\n",
      "Train - Log-lik: -32314.88192;  CP: 84.25094;  KL_w: 0.00000;  KL_y: 1.61111;\n",
      "Valid - Log-lik: -24313.41489;  CP: 83.21826;  KL_w: 0.00000;  KL_y: 1.65140;\n",
      "Total Loss=Train: 32400.74393; Val: 24398.28453\n",
      "(Epoch 396 / 400)\n",
      "Train - Log-lik: -32424.59654;  CP: 83.35148;  KL_w: 0.00000;  KL_y: 1.61762;\n",
      "Valid - Log-lik: -24182.80211;  CP: 82.96985;  KL_w: 0.00000;  KL_y: 1.66553;\n",
      "Total Loss=Train: 32509.56564; Val: 24267.43749\n",
      "(Epoch 397 / 400)\n",
      "Train - Log-lik: -32399.57219;  CP: 84.69963;  KL_w: 0.00000;  KL_y: 1.61248;\n",
      "Valid - Log-lik: -24171.29461;  CP: 82.81518;  KL_w: 0.00000;  KL_y: 1.65431;\n",
      "Total Loss=Train: 32485.88423; Val: 24255.76410\n",
      "(Epoch 398 / 400)\n",
      "Train - Log-lik: -32246.78798;  CP: 83.39038;  KL_w: 0.00001;  KL_y: 1.61902;\n",
      "Valid - Log-lik: -24062.71658;  CP: 82.89705;  KL_w: 0.00000;  KL_y: 1.67593;\n",
      "Total Loss=Train: 32331.79743; Val: 24147.28953\n",
      "(Epoch 399 / 400)\n",
      "Train - Log-lik: -32380.68969;  CP: 83.66306;  KL_w: 0.00000;  KL_y: 1.61781;\n",
      "Valid - Log-lik: -23764.93147;  CP: 83.06496;  KL_w: 0.00000;  KL_y: 1.64400;\n",
      "Total Loss=Train: 32465.97047; Val: 23849.64047\n",
      "(Epoch 400 / 400)\n",
      "Train - Log-lik: -32311.92614;  CP: 83.06660;  KL_w: 0.00000;  KL_y: 1.61489;\n",
      "Valid - Log-lik: -23810.71122;  CP: 82.97347;  KL_w: 0.00000;  KL_y: 1.66433;\n",
      "Total Loss=Train: 32396.60753; Val: 23895.34906\n",
      "====================================================================================================\n",
      "Saving checkpoint to \"experiments/FMNIST_200epochs_dp0_lr001/checkpoints/GMVAE_1_00001_25_25_10_50_3_03_0001/step_400.pt\".\n",
      "====================================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "GMVAE_model.train(trainloader_GMVAE, validloader_GMVAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-4. Integration of GMVAE and fine-tuning the NoRClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have pre-trained the classification neural network and trained the GMVAE with the suitable hidden vectors obtained from the pre-trained classifier, it is time to integrate both models in one and perform the fine-tuning step. For that, we define the new module class NoRCLassifier, whose forward function has a parameter with the GMVAE model. Between layers 1 and 2, we call the reconstruction function of the GMVAE so we obtain a new noisy version of the hidden vectors through the application of the inference and generation networks from the GMVAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoRClassifier(nn.Module):\n",
    "    def __init__(self, freeze_lower=False, dropout_lower=False, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 700)\n",
    "        self.fc2 = nn.Linear(700, 600)\n",
    "        self.fc3 = nn.Linear(600, 512)\n",
    "        self.fc4 = nn.Linear(512, 256)\n",
    "        self.fc5 = nn.Linear(256, 128)\n",
    "        self.fc6 = nn.Linear(128, 64)\n",
    "        self.fc7 = nn.Linear(64, 32)\n",
    "        self.fc8 = nn.Linear(32, 16)\n",
    "        self.fc9 = nn.Linear(16, 10)\n",
    "\n",
    "        self.dropout_lower = dropout_lower\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Freeze bert layers\n",
    "        if freeze_lower:\n",
    "            for p in self.fc1.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "    def forward(self, x, GMVAE=None):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        if self.dropout_lower:\n",
    "            x = self.dropout(F.relu(self.fc1(x)))\n",
    "        else:\n",
    "            x = F.relu(self.fc1(x))\n",
    "\n",
    "        if GMVAE != None:\n",
    "            if GMVAE.cuda:\n",
    "                x = torch.Tensor(GMVAE.batch_reconstruction(x.cuda())).cuda()\n",
    "            else:\n",
    "                x = torch.Tensor(GMVAE.batch_reconstruction(x))\n",
    "\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = F.relu(self.fc6(x))\n",
    "        x = F.relu(self.fc7(x))\n",
    "        x = F.relu(self.fc8(x))\n",
    "        x = F.log_softmax(self.fc9(x), dim=1)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define some new parameters in case we want to modify the number epochs from the pre-training, include some clarification tag in the model being ejecuted or freeze the first layer in the NN architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "version = \"lower_dp0\" \n",
    "freeze_lower = True \n",
    "dropoutLower = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call the GMVAE trained before in an evaluation approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Building computation graph...\n",
      "====================================================================================================\n",
      "Saving config to \"experiments/FMNIST_200epochs_dp0_lr001/checkpoints/GMVAE_1_00001_25_25_10_50_3_03_0001/config.pt\".\n",
      "====================================================================================================\n",
      "\n",
      "Loading model from checkpoint in step 400...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GenerativeNet(\n",
       "  (activation): Tanh()\n",
       "  (qz_wy_mean_layers): ModuleList(\n",
       "    (0): Linear(in_features=25, out_features=50, bias=True)\n",
       "    (1): Tanh()\n",
       "  )\n",
       "  (qz_wy_var_layers): ModuleList(\n",
       "    (0): Linear(in_features=25, out_features=50, bias=True)\n",
       "    (1): Tanh()\n",
       "  )\n",
       "  (px_z_mean_layers): ModuleList(\n",
       "    (0): Linear(in_features=25, out_features=50, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Dropout(p=0.3, inplace=False)\n",
       "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
       "    (4): Tanh()\n",
       "    (5): Dropout(p=0.3, inplace=False)\n",
       "    (6): Linear(in_features=50, out_features=700, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('\\n Building computation graph...')\n",
    "GMVAE_model = GMVAE(config)\n",
    "\n",
    "GMVAE_model.restore_model()\n",
    "\n",
    "GMVAE_model.inference.eval()\n",
    "GMVAE_model.generative.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build the NoRClassifier model, loading the parameters from the best epoch in the pre-trianing stage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NoRClassifier(freeze_lower=freeze_lower, dropout_lower=dropoutLower, dropout=dropout)\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "model.load_state_dict(torch.load('saved/{}/model_pretrain{}.pt'.format(load_path, dropout)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.997701 \tValidation Loss: 0.962763\n",
      "validation loss decreased(inf -->0.962763). Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 0.876647 \tValidation Loss: 0.925469\n",
      "validation loss decreased(0.962763 -->0.925469). Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 0.839163 \tValidation Loss: 0.913746\n",
      "validation loss decreased(0.925469 -->0.913746). Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 0.811111 \tValidation Loss: 0.852071\n",
      "validation loss decreased(0.913746 -->0.852071). Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 0.794346 \tValidation Loss: 0.873905\n",
      "Epoch: 6 \tTraining Loss: 0.778565 \tValidation Loss: 0.827654\n",
      "validation loss decreased(0.852071 -->0.827654). Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 0.762458 \tValidation Loss: 0.801411\n",
      "validation loss decreased(0.827654 -->0.801411). Saving model ...\n",
      "Epoch: 8 \tTraining Loss: 0.756068 \tValidation Loss: 0.816322\n",
      "Epoch: 9 \tTraining Loss: 0.743602 \tValidation Loss: 0.773905\n",
      "validation loss decreased(0.801411 -->0.773905). Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 0.732850 \tValidation Loss: 0.781645\n",
      "Epoch: 11 \tTraining Loss: 0.725598 \tValidation Loss: 0.902593\n",
      "Epoch: 12 \tTraining Loss: 0.721455 \tValidation Loss: 0.762411\n",
      "validation loss decreased(0.773905 -->0.762411). Saving model ...\n",
      "Epoch: 13 \tTraining Loss: 0.712363 \tValidation Loss: 0.755415\n",
      "validation loss decreased(0.762411 -->0.755415). Saving model ...\n",
      "Epoch: 14 \tTraining Loss: 0.707228 \tValidation Loss: 0.771554\n",
      "Epoch: 15 \tTraining Loss: 0.702521 \tValidation Loss: 0.841493\n",
      "Epoch: 16 \tTraining Loss: 0.703304 \tValidation Loss: 0.749024\n",
      "validation loss decreased(0.755415 -->0.749024). Saving model ...\n",
      "Epoch: 17 \tTraining Loss: 0.690171 \tValidation Loss: 0.749297\n",
      "Epoch: 18 \tTraining Loss: 0.695112 \tValidation Loss: 0.743209\n",
      "validation loss decreased(0.749024 -->0.743209). Saving model ...\n",
      "Epoch: 19 \tTraining Loss: 0.686731 \tValidation Loss: 0.748033\n",
      "Epoch: 20 \tTraining Loss: 0.681945 \tValidation Loss: 0.829852\n",
      "Epoch: 21 \tTraining Loss: 0.681817 \tValidation Loss: 0.862710\n",
      "Epoch: 22 \tTraining Loss: 0.678826 \tValidation Loss: 0.735661\n",
      "validation loss decreased(0.743209 -->0.735661). Saving model ...\n",
      "Epoch: 23 \tTraining Loss: 0.672979 \tValidation Loss: 0.761628\n",
      "Epoch: 24 \tTraining Loss: 0.671992 \tValidation Loss: 0.756953\n",
      "Epoch: 25 \tTraining Loss: 0.670832 \tValidation Loss: 0.740868\n",
      "Epoch: 26 \tTraining Loss: 0.662559 \tValidation Loss: 0.728042\n",
      "validation loss decreased(0.735661 -->0.728042). Saving model ...\n",
      "Epoch: 27 \tTraining Loss: 0.669251 \tValidation Loss: 0.876527\n",
      "Epoch: 28 \tTraining Loss: 0.662798 \tValidation Loss: 0.745252\n",
      "Epoch: 29 \tTraining Loss: 0.661774 \tValidation Loss: 0.784975\n",
      "Epoch: 30 \tTraining Loss: 0.658452 \tValidation Loss: 0.738426\n",
      "Epoch: 31 \tTraining Loss: 0.656011 \tValidation Loss: 0.725836\n",
      "validation loss decreased(0.728042 -->0.725836). Saving model ...\n",
      "Epoch: 32 \tTraining Loss: 0.653133 \tValidation Loss: 0.725504\n",
      "validation loss decreased(0.725836 -->0.725504). Saving model ...\n",
      "Epoch: 33 \tTraining Loss: 0.653439 \tValidation Loss: 0.770337\n",
      "Epoch: 34 \tTraining Loss: 0.652231 \tValidation Loss: 0.816331\n",
      "Epoch: 35 \tTraining Loss: 0.642565 \tValidation Loss: 0.739274\n",
      "Epoch: 36 \tTraining Loss: 0.641910 \tValidation Loss: 0.739271\n",
      "Epoch: 37 \tTraining Loss: 0.640577 \tValidation Loss: 0.708294\n",
      "validation loss decreased(0.725504 -->0.708294). Saving model ...\n",
      "Epoch: 38 \tTraining Loss: 0.646459 \tValidation Loss: 0.771613\n",
      "Epoch: 39 \tTraining Loss: 0.640838 \tValidation Loss: 0.728882\n",
      "Epoch: 40 \tTraining Loss: 0.636563 \tValidation Loss: 0.718549\n",
      "Epoch: 41 \tTraining Loss: 0.636034 \tValidation Loss: 0.713216\n",
      "Epoch: 42 \tTraining Loss: 0.638417 \tValidation Loss: 0.719612\n",
      "Epoch: 43 \tTraining Loss: 0.628238 \tValidation Loss: 0.708743\n",
      "Epoch: 44 \tTraining Loss: 0.627379 \tValidation Loss: 0.719880\n",
      "Epoch: 45 \tTraining Loss: 0.630334 \tValidation Loss: 0.697765\n",
      "validation loss decreased(0.708294 -->0.697765). Saving model ...\n",
      "Epoch: 46 \tTraining Loss: 0.630383 \tValidation Loss: 0.702078\n",
      "Epoch: 47 \tTraining Loss: 0.627028 \tValidation Loss: 0.722255\n",
      "Epoch: 48 \tTraining Loss: 0.621438 \tValidation Loss: 0.950693\n",
      "Epoch: 49 \tTraining Loss: 0.625155 \tValidation Loss: 0.686896\n",
      "validation loss decreased(0.697765 -->0.686896). Saving model ...\n",
      "Epoch: 50 \tTraining Loss: 0.617139 \tValidation Loss: 0.790458\n",
      "Epoch: 51 \tTraining Loss: 0.619863 \tValidation Loss: 0.676029\n",
      "validation loss decreased(0.686896 -->0.676029). Saving model ...\n",
      "Epoch: 52 \tTraining Loss: 0.612206 \tValidation Loss: 0.687850\n",
      "Epoch: 53 \tTraining Loss: 0.619153 \tValidation Loss: 0.684351\n",
      "Epoch: 54 \tTraining Loss: 0.613161 \tValidation Loss: 0.700778\n",
      "Epoch: 55 \tTraining Loss: 0.616490 \tValidation Loss: 0.694833\n",
      "Epoch: 56 \tTraining Loss: 0.613649 \tValidation Loss: 0.708372\n",
      "Epoch: 57 \tTraining Loss: 0.611687 \tValidation Loss: 0.683466\n",
      "Epoch: 58 \tTraining Loss: 0.602374 \tValidation Loss: 0.827954\n",
      "Epoch: 59 \tTraining Loss: 0.608042 \tValidation Loss: 0.871179\n",
      "Epoch: 60 \tTraining Loss: 0.606806 \tValidation Loss: 0.707428\n",
      "Epoch: 61 \tTraining Loss: 0.603988 \tValidation Loss: 0.688370\n",
      "Epoch: 62 \tTraining Loss: 0.601548 \tValidation Loss: 0.667572\n",
      "validation loss decreased(0.676029 -->0.667572). Saving model ...\n",
      "Epoch: 63 \tTraining Loss: 0.603821 \tValidation Loss: 0.698423\n",
      "Epoch: 64 \tTraining Loss: 0.595128 \tValidation Loss: 0.668446\n",
      "Epoch: 65 \tTraining Loss: 0.597720 \tValidation Loss: 0.676459\n",
      "Epoch: 66 \tTraining Loss: 0.595577 \tValidation Loss: 0.757321\n",
      "Epoch: 67 \tTraining Loss: 0.595513 \tValidation Loss: 0.674103\n",
      "Epoch: 68 \tTraining Loss: 0.598750 \tValidation Loss: 0.701263\n",
      "Epoch: 69 \tTraining Loss: 0.591152 \tValidation Loss: 0.679003\n",
      "Epoch: 70 \tTraining Loss: 0.594155 \tValidation Loss: 0.690109\n",
      "Epoch: 71 \tTraining Loss: 0.594703 \tValidation Loss: 0.677439\n",
      "Epoch: 72 \tTraining Loss: 0.591764 \tValidation Loss: 0.665918\n",
      "validation loss decreased(0.667572 -->0.665918). Saving model ...\n",
      "Epoch: 73 \tTraining Loss: 0.598152 \tValidation Loss: 0.657229\n",
      "validation loss decreased(0.665918 -->0.657229). Saving model ...\n",
      "Epoch: 74 \tTraining Loss: 0.591947 \tValidation Loss: 0.669643\n",
      "Epoch: 75 \tTraining Loss: 0.585332 \tValidation Loss: 0.702704\n",
      "Epoch: 76 \tTraining Loss: 0.585250 \tValidation Loss: 0.665484\n",
      "Epoch: 77 \tTraining Loss: 0.586018 \tValidation Loss: 0.672444\n",
      "Epoch: 78 \tTraining Loss: 0.589552 \tValidation Loss: 0.683607\n",
      "Epoch: 79 \tTraining Loss: 0.586036 \tValidation Loss: 0.677941\n",
      "Epoch: 80 \tTraining Loss: 0.583356 \tValidation Loss: 0.670048\n",
      "Epoch: 81 \tTraining Loss: 0.578453 \tValidation Loss: 0.702501\n",
      "Epoch: 82 \tTraining Loss: 0.583171 \tValidation Loss: 0.733273\n",
      "Epoch: 83 \tTraining Loss: 0.576806 \tValidation Loss: 0.691485\n",
      "Epoch: 84 \tTraining Loss: 0.588069 \tValidation Loss: 0.685379\n",
      "Epoch: 85 \tTraining Loss: 0.577838 \tValidation Loss: 0.720227\n",
      "Epoch: 86 \tTraining Loss: 0.581176 \tValidation Loss: 0.659190\n",
      "Epoch: 87 \tTraining Loss: 0.578051 \tValidation Loss: 0.686350\n",
      "Epoch: 88 \tTraining Loss: 0.580151 \tValidation Loss: 0.669878\n",
      "Epoch: 89 \tTraining Loss: 0.575443 \tValidation Loss: 0.671869\n",
      "Epoch: 90 \tTraining Loss: 0.572516 \tValidation Loss: 0.663313\n",
      "Epoch: 91 \tTraining Loss: 0.575746 \tValidation Loss: 0.771813\n",
      "Epoch: 92 \tTraining Loss: 0.572019 \tValidation Loss: 0.657049\n",
      "validation loss decreased(0.657229 -->0.657049). Saving model ...\n",
      "Epoch: 93 \tTraining Loss: 0.572548 \tValidation Loss: 0.709274\n",
      "Epoch: 94 \tTraining Loss: 0.567963 \tValidation Loss: 0.662843\n",
      "Epoch: 95 \tTraining Loss: 0.570671 \tValidation Loss: 0.647168\n",
      "validation loss decreased(0.657049 -->0.647168). Saving model ...\n",
      "Epoch: 96 \tTraining Loss: 0.568390 \tValidation Loss: 0.657631\n",
      "Epoch: 97 \tTraining Loss: 0.567136 \tValidation Loss: 0.702838\n",
      "Epoch: 98 \tTraining Loss: 0.570383 \tValidation Loss: 0.748899\n",
      "Epoch: 99 \tTraining Loss: 0.564377 \tValidation Loss: 0.641331\n",
      "validation loss decreased(0.647168 -->0.641331). Saving model ...\n",
      "Epoch: 100 \tTraining Loss: 0.566638 \tValidation Loss: 0.655654\n",
      "Epoch: 101 \tTraining Loss: 0.566311 \tValidation Loss: 0.679892\n",
      "Epoch: 102 \tTraining Loss: 0.562594 \tValidation Loss: 0.738601\n",
      "Epoch: 103 \tTraining Loss: 0.566267 \tValidation Loss: 0.719843\n",
      "Epoch: 104 \tTraining Loss: 0.557376 \tValidation Loss: 0.765073\n",
      "Epoch: 105 \tTraining Loss: 0.566182 \tValidation Loss: 0.658022\n",
      "Epoch: 106 \tTraining Loss: 0.562724 \tValidation Loss: 0.669584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 107 \tTraining Loss: 0.563862 \tValidation Loss: 0.655319\n",
      "Epoch: 108 \tTraining Loss: 0.554509 \tValidation Loss: 0.654907\n",
      "Epoch: 109 \tTraining Loss: 0.556321 \tValidation Loss: 0.771678\n",
      "Epoch: 110 \tTraining Loss: 0.560140 \tValidation Loss: 0.733548\n",
      "Epoch: 111 \tTraining Loss: 0.555700 \tValidation Loss: 0.655994\n",
      "Epoch: 112 \tTraining Loss: 0.555758 \tValidation Loss: 0.708918\n",
      "Epoch: 113 \tTraining Loss: 0.554713 \tValidation Loss: 0.667323\n",
      "Epoch: 114 \tTraining Loss: 0.555036 \tValidation Loss: 0.754547\n",
      "Epoch: 115 \tTraining Loss: 0.557369 \tValidation Loss: 0.655965\n",
      "Epoch: 116 \tTraining Loss: 0.552373 \tValidation Loss: 0.651295\n",
      "Epoch: 117 \tTraining Loss: 0.552417 \tValidation Loss: 0.649620\n",
      "Epoch: 118 \tTraining Loss: 0.551810 \tValidation Loss: 0.642684\n",
      "Epoch: 119 \tTraining Loss: 0.552809 \tValidation Loss: 0.657275\n",
      "Epoch: 120 \tTraining Loss: 0.550748 \tValidation Loss: 0.675341\n",
      "Epoch: 121 \tTraining Loss: 0.546530 \tValidation Loss: 0.735814\n",
      "Epoch: 122 \tTraining Loss: 0.551463 \tValidation Loss: 0.678127\n",
      "Epoch: 123 \tTraining Loss: 0.549360 \tValidation Loss: 0.718497\n",
      "Epoch: 124 \tTraining Loss: 0.547416 \tValidation Loss: 0.644254\n",
      "Epoch: 125 \tTraining Loss: 0.549091 \tValidation Loss: 0.653742\n",
      "Epoch: 126 \tTraining Loss: 0.546536 \tValidation Loss: 0.637984\n",
      "validation loss decreased(0.641331 -->0.637984). Saving model ...\n",
      "Epoch: 127 \tTraining Loss: 0.547233 \tValidation Loss: 0.629867\n",
      "validation loss decreased(0.637984 -->0.629867). Saving model ...\n",
      "Epoch: 128 \tTraining Loss: 0.542701 \tValidation Loss: 0.662097\n",
      "Epoch: 129 \tTraining Loss: 0.544864 \tValidation Loss: 0.658614\n",
      "Epoch: 130 \tTraining Loss: 0.543432 \tValidation Loss: 0.661444\n",
      "Epoch: 131 \tTraining Loss: 0.539981 \tValidation Loss: 0.670284\n",
      "Epoch: 132 \tTraining Loss: 0.540957 \tValidation Loss: 0.659106\n",
      "Epoch: 133 \tTraining Loss: 0.545684 \tValidation Loss: 0.682402\n",
      "Epoch: 134 \tTraining Loss: 0.545856 \tValidation Loss: 0.811217\n",
      "Epoch: 135 \tTraining Loss: 0.544235 \tValidation Loss: 0.683713\n",
      "Epoch: 136 \tTraining Loss: 0.542635 \tValidation Loss: 0.667414\n",
      "Epoch: 137 \tTraining Loss: 0.534522 \tValidation Loss: 0.678329\n",
      "Epoch: 138 \tTraining Loss: 0.539787 \tValidation Loss: 0.695738\n",
      "Epoch: 139 \tTraining Loss: 0.535237 \tValidation Loss: 0.655315\n",
      "Epoch: 140 \tTraining Loss: 0.539374 \tValidation Loss: 0.662514\n",
      "Epoch: 141 \tTraining Loss: 0.539997 \tValidation Loss: 0.849165\n",
      "Epoch: 142 \tTraining Loss: 0.538838 \tValidation Loss: 0.668929\n",
      "Epoch: 143 \tTraining Loss: 0.533968 \tValidation Loss: 0.664967\n",
      "Epoch: 144 \tTraining Loss: 0.537433 \tValidation Loss: 0.665716\n",
      "Epoch: 145 \tTraining Loss: 0.537282 \tValidation Loss: 0.743122\n",
      "Epoch: 146 \tTraining Loss: 0.538422 \tValidation Loss: 0.772833\n",
      "Epoch: 147 \tTraining Loss: 0.534359 \tValidation Loss: 0.645600\n",
      "Epoch: 148 \tTraining Loss: 0.533936 \tValidation Loss: 0.633954\n",
      "Epoch: 149 \tTraining Loss: 0.532975 \tValidation Loss: 0.655370\n",
      "Epoch: 150 \tTraining Loss: 0.529435 \tValidation Loss: 0.669865\n",
      "Epoch: 151 \tTraining Loss: 0.531333 \tValidation Loss: 0.668986\n",
      "Epoch: 152 \tTraining Loss: 0.527311 \tValidation Loss: 0.641701\n",
      "Epoch: 153 \tTraining Loss: 0.533839 \tValidation Loss: 0.699489\n",
      "Epoch: 154 \tTraining Loss: 0.531543 \tValidation Loss: 0.640525\n",
      "Epoch: 155 \tTraining Loss: 0.526968 \tValidation Loss: 0.631132\n",
      "Epoch: 156 \tTraining Loss: 0.526815 \tValidation Loss: 0.637087\n",
      "Epoch: 157 \tTraining Loss: 0.523477 \tValidation Loss: 0.668326\n",
      "Epoch: 158 \tTraining Loss: 0.523976 \tValidation Loss: 0.633292\n",
      "Epoch: 159 \tTraining Loss: 0.526353 \tValidation Loss: 0.627043\n",
      "validation loss decreased(0.629867 -->0.627043). Saving model ...\n",
      "Epoch: 160 \tTraining Loss: 0.533135 \tValidation Loss: 0.644791\n",
      "Epoch: 161 \tTraining Loss: 0.525508 \tValidation Loss: 0.664792\n",
      "Epoch: 162 \tTraining Loss: 0.525416 \tValidation Loss: 0.682462\n",
      "Epoch: 163 \tTraining Loss: 0.524750 \tValidation Loss: 0.660900\n",
      "Epoch: 164 \tTraining Loss: 0.524647 \tValidation Loss: 0.809003\n",
      "Epoch: 165 \tTraining Loss: 0.519308 \tValidation Loss: 0.652792\n",
      "Epoch: 166 \tTraining Loss: 0.522586 \tValidation Loss: 0.663611\n",
      "Epoch: 167 \tTraining Loss: 0.523345 \tValidation Loss: 0.678520\n",
      "Epoch: 168 \tTraining Loss: 0.523561 \tValidation Loss: 0.675753\n",
      "Epoch: 169 \tTraining Loss: 0.516128 \tValidation Loss: 0.671896\n",
      "Epoch: 170 \tTraining Loss: 0.524105 \tValidation Loss: 0.629006\n",
      "Epoch: 171 \tTraining Loss: 0.517777 \tValidation Loss: 0.640331\n",
      "Epoch: 172 \tTraining Loss: 0.519357 \tValidation Loss: 0.638589\n",
      "Epoch: 173 \tTraining Loss: 0.519187 \tValidation Loss: 0.684515\n",
      "Epoch: 174 \tTraining Loss: 0.517480 \tValidation Loss: 0.629710\n",
      "Epoch: 175 \tTraining Loss: 0.515938 \tValidation Loss: 0.643361\n",
      "Epoch: 176 \tTraining Loss: 0.512809 \tValidation Loss: 0.684497\n",
      "Epoch: 177 \tTraining Loss: 0.513421 \tValidation Loss: 0.661318\n",
      "Epoch: 178 \tTraining Loss: 0.517402 \tValidation Loss: 0.683082\n",
      "Epoch: 179 \tTraining Loss: 0.518779 \tValidation Loss: 0.656435\n",
      "Epoch: 180 \tTraining Loss: 0.516962 \tValidation Loss: 0.638629\n",
      "Epoch: 181 \tTraining Loss: 0.519117 \tValidation Loss: 0.736041\n",
      "Epoch: 182 \tTraining Loss: 0.514738 \tValidation Loss: 0.633645\n",
      "Epoch: 183 \tTraining Loss: 0.510025 \tValidation Loss: 0.680085\n",
      "Epoch: 184 \tTraining Loss: 0.510925 \tValidation Loss: 0.686969\n",
      "Epoch: 185 \tTraining Loss: 0.510363 \tValidation Loss: 0.647614\n",
      "Epoch: 186 \tTraining Loss: 0.507144 \tValidation Loss: 0.626759\n",
      "validation loss decreased(0.627043 -->0.626759). Saving model ...\n",
      "Epoch: 187 \tTraining Loss: 0.517173 \tValidation Loss: 0.686635\n",
      "Epoch: 188 \tTraining Loss: 0.504673 \tValidation Loss: 0.662912\n",
      "Epoch: 189 \tTraining Loss: 0.505208 \tValidation Loss: 0.631799\n",
      "Epoch: 190 \tTraining Loss: 0.509644 \tValidation Loss: 0.776021\n",
      "Epoch: 191 \tTraining Loss: 0.513025 \tValidation Loss: 0.657407\n",
      "Epoch: 192 \tTraining Loss: 0.504396 \tValidation Loss: 0.684738\n",
      "Epoch: 193 \tTraining Loss: 0.506663 \tValidation Loss: 0.711092\n",
      "Epoch: 194 \tTraining Loss: 0.508660 \tValidation Loss: 0.651639\n",
      "Epoch: 195 \tTraining Loss: 0.503175 \tValidation Loss: 0.658228\n",
      "Epoch: 196 \tTraining Loss: 0.506519 \tValidation Loss: 0.627716\n",
      "Epoch: 197 \tTraining Loss: 0.506558 \tValidation Loss: 0.665207\n",
      "Epoch: 198 \tTraining Loss: 0.502356 \tValidation Loss: 0.728535\n",
      "Epoch: 199 \tTraining Loss: 0.502973 \tValidation Loss: 0.649999\n",
      "Epoch: 200 \tTraining Loss: 0.504489 \tValidation Loss: 0.642254\n"
     ]
    }
   ],
   "source": [
    "valid_loss_min = np.Inf\n",
    "model.train()\n",
    "train_losses, valid_losses = [], []\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    valid_loss = 0\n",
    "\n",
    "    for images, labels in trainloader:\n",
    "        optimizer.zero_grad()\n",
    "        log_ps = model(images, GMVAE=GMVAE_model)\n",
    "        loss = criterion(log_ps, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()*images.size(0)\n",
    "\n",
    "    for images, labels in validloader:\n",
    "        log_ps = model(images, GMVAE=GMVAE_model)\n",
    "        loss = criterion(log_ps, labels)\n",
    "        valid_loss += loss.item()*images.size(0)\n",
    "\n",
    "    running_loss = running_loss/len(trainloader.sampler)\n",
    "    valid_loss = valid_loss/len(validloader.sampler)\n",
    "    train_losses.append(running_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(e+1, running_loss, valid_loss))\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('validation loss decreased({:.6f} -->{:.6f}). Saving model ...'.format(valid_loss_min, valid_loss))\n",
    "        torch.save(model.state_dict(), 'saved/{}/model_GMVAE_{}epochs_{}.pt'.format(load_path, epochs, version))\n",
    "        valid_loss_min = valid_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we plot the losses in the fine-tuning stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(valid_losses, label='Valid Loss', linestyle='dashed')\n",
    "plt.xlabel('Fine-tuning epochs')\n",
    "plt.ylim((0, 1.3))\n",
    "plt.legend(fontsize=16)\n",
    "plt.grid(True)\n",
    "plt.savefig('saved/{}/loss_GMVAE_{}epochs_{}.eps'.format(load_path, epochs, version))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This figure shows the slow overfitting effect, reduced thanks to the stochastic layer and the noisy version of the hidden vectors that it gets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case we want to train the baseline example, without the GMVAE and including dropout, we can modify the previous lines, and add the line *GMVAE_model = None* as well as set some dropout and define to True the dropoutLower flag."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
